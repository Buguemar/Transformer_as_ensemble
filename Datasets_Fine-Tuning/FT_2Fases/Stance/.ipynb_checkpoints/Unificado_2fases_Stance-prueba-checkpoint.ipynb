{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import os\n",
    "from sc_ft import *\n",
    "from lm_ft_prueba import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: GeForce GTX 1060 6GB\n"
     ]
    }
   ],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "epoch_macro= 1\n",
    "\n",
    "MODEL_CLASSES = {\n",
    "    \"gpt2\": (GPT2Config, GPT2LMHeadModel, GPT2Tokenizer),\n",
    "    \"openai-gpt\": (OpenAIGPTConfig, OpenAIGPTLMHeadModel, OpenAIGPTTokenizer),\n",
    "    \"bert\": (BertConfig, BertForMaskedLM, BertTokenizer),\n",
    "    \"roberta\": (RobertaConfig, RobertaForMaskedLM, RobertaTokenizer),\n",
    "}\n",
    "\n",
    "if torch.cuda.is_available():       \n",
    "    device = torch.device(\"cuda\")\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args_LM:\n",
    "    adam_epsilon=1e-8\n",
    "    block_size=-1 \n",
    "    cache_dir=None\n",
    "    config_name=None\n",
    "    do_train=True        \n",
    "    do_eval=True \n",
    "    do_lower_case=True\n",
    "    eval_all_checkpoints=True\n",
    "    evaluate_during_training=True\n",
    "    eval_data_file=None\n",
    "    fp16= False\n",
    "    fp16_opt_level=\"O1\"\n",
    "    gradient_accumulation_steps=1\n",
    "    learning_rate=5e-5\n",
    "    logging_steps=100\n",
    "    local_rank=-1\n",
    "    model_type= default=\"bert\"\n",
    "    model_name_or_path= \"bert-base-uncased\" \n",
    "    mlm=True\n",
    "    mlm_probability=0.15\n",
    "    max_grad_norm=1.0\n",
    "    max_steps=-1\n",
    "    num_train_epochs=1.0\n",
    "    n_gpu=1\n",
    "    no_cuda=False\n",
    "    overwrite_output_dir=True\n",
    "    overwrite_cache=False\n",
    "    output_dir=None\n",
    "    per_gpu_train_batch_size=4\n",
    "    per_gpu_eval_batch_size=4\n",
    "    save_steps=50\n",
    "    save_total_limit=None    \n",
    "    seed=42   \n",
    "    server_ip=\"\"\n",
    "    server_port=\"\"\n",
    "    train_data_file=None #str\n",
    "    tokenizer_name=None \n",
    "    weight_decay=0.0   \n",
    "    warmup_steps=0\n",
    "    line_by_line=False\n",
    "    should_continue=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert-base-uncased_cached_lm_128_Stance_raw_train.txt  Stance_raw_test\r\n",
      "bert-base-uncased_cached_lm_128_Stance_raw_val.txt    Stance_raw_test.txt\r\n",
      "bert-base-uncased_cached_lm_40_Stance_raw_train.txt   Stance_raw_train\r\n",
      "bert-base-uncased_cached_lm_40_Stance_raw_val.txt     Stance_raw_train.txt\r\n",
      "bert-base-uncased_cached_lm_64_Stance_raw_test.txt    Stance_raw_val\r\n",
      "bert-base-uncased_cached_lm_64_Stance_raw_train.txt   Stance_raw_val.txt\r\n",
      "bert-base-uncased_cached_lm_64_Stance_raw_val.txt     test_semeval_raw.csv\r\n",
      "bert_cached_lm_62_Stance_raw_train.txt\t\t      train_semeval_raw.csv\r\n",
      "dev_semeval_raw.csv\r\n"
     ]
    }
   ],
   "source": [
    "!ls ../../CSV_Stance/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "out= '../../../../model_save/Dos-Fases-all_Stance_prueba'\n",
    "\n",
    "my_args = Args_LM()\n",
    "my_args.output_dir = out\n",
    "my_args.model_type = 'bert'\n",
    "my_args.train_data_file = '../../CSV_Stance/Stance_raw_train.txt'\n",
    "my_args.block_size = 64\n",
    "my_args.eval_data_file = '../../CSV_Stance/Stance_raw_test.txt'\n",
    "my_args.per_gpu_train_batch_size = 32\n",
    "my_args.per_gpu_eval_batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fase_LM(from_pre, my_args, logger):\n",
    "    if from_pre:\n",
    "        model_dir_ft = 'bert-base-uncased'\n",
    "        my_args.model_name_or_path = model_dir_ft\n",
    "    else:\n",
    "        model_dir_ft = out\n",
    "        my_args.model_name_or_path = model_dir_ft\n",
    "    \n",
    "    if my_args.model_type in [\"bert\", \"roberta\"] and not my_args.mlm:\n",
    "        raise ValueError(\n",
    "            \"BERT and RoBERTa do not have LM heads but masked LM heads. They must be run using the --mlm \"\n",
    "            \"flag (masked language modeling).\"\n",
    "        )\n",
    "    if my_args.eval_data_file is None and my_args.do_eval:\n",
    "        raise ValueError(\n",
    "            \"Cannot do evaluation without an evaluation data file. Either supply a file to --eval_data_file \"\n",
    "            \"or remove the --do_eval argument.\"\n",
    "        )\n",
    "\n",
    "    if my_args.should_continue:\n",
    "        sorted_checkpoints = sorted_checkpoints_f(my_args)\n",
    "        if len(sorted_checkpoints) == 0:\n",
    "            raise ValueError(\"Used --should_continue but no checkpoint was found in --output_dir.\")\n",
    "        else:\n",
    "            my_args.model_name_or_path = sorted_checkpoints[-1]\n",
    "    if (\n",
    "        os.path.exists(my_args.output_dir)\n",
    "        and os.listdir(my_args.output_dir)\n",
    "        and my_args.do_train\n",
    "        and not my_args.overwrite_output_dir\n",
    "    ):\n",
    "        raise ValueError(\n",
    "            \"Output directory ({}) already exists and is not empty. Use --overwrite_output_dir to overcome.\".format(\n",
    "                my_args.output_dir\n",
    "            )\n",
    "        )\n",
    "\n",
    "    if my_args.server_ip and my_args.server_port:\n",
    "        import ptvsd\n",
    "        #print(\"Waiting for debugger attach\")\n",
    "        ptvsd.enable_attach(address=(my_args.server_ip, my_args.server_port), redirect_output=True)\n",
    "        ptvsd.wait_for_attach()\n",
    "    if my_args.local_rank == -1 or my_args.no_cuda:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() and not my_args.no_cuda else \"cpu\")\n",
    "        my_args.n_gpu = torch.cuda.device_count()\n",
    "    else:  # Initializes the distributed backend which will take care of sychronizing nodes/GPUs\n",
    "        torch.cuda.set_device(my_args.local_rank)\n",
    "        device = torch.device(\"cuda\", my_args.local_rank)\n",
    "        torch.distributed.init_process_group(backend=\"nccl\")\n",
    "        my_args.n_gpu = 1\n",
    "    my_args.device = device\n",
    "    logging.basicConfig(\n",
    "        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
    "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "        level=logging.INFO if my_args.local_rank in [-1, 0] else logging.WARN,\n",
    "    )\n",
    "    logger.warning(\n",
    "        \"Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s\",\n",
    "        my_args.local_rank,\n",
    "        device,\n",
    "        my_args.n_gpu,\n",
    "        bool(my_args.local_rank != -1),\n",
    "        my_args.fp16,\n",
    "    )\n",
    "    set_seed(my_args)\n",
    "    if my_args.local_rank not in [-1, 0]:\n",
    "        torch.distributed.barrier()  # Barrier to make sure only the first process in distributed training download model & vocab\n",
    "    config_class, model_class, tokenizer_class = MODEL_CLASSES[my_args.model_type]\n",
    "    #print (\"\\nLoadding model \", model_class)\n",
    "    config = config_class.from_pretrained(\n",
    "        my_args.config_name if my_args.config_name else my_args.model_name_or_path,\n",
    "        cache_dir=my_args.cache_dir if my_args.cache_dir else None,\n",
    "    )\n",
    "    tokenizer = tokenizer_class.from_pretrained(\n",
    "        my_args.tokenizer_name if my_args.tokenizer_name else my_args.model_name_or_path,\n",
    "        do_lower_case=my_args.do_lower_case,\n",
    "        cache_dir=my_args.cache_dir if my_args.cache_dir else None,\n",
    "    )\n",
    "    if my_args.block_size <= 0:\n",
    "        my_args.block_size = (\n",
    "            tokenizer.max_len_single_sentence\n",
    "        )  # Our input block size will be the max possible for the model\n",
    "    my_args.block_size = min(my_args.block_size, tokenizer.max_len_single_sentence)\n",
    "    model = model_class.from_pretrained(\n",
    "        my_args.model_name_or_path,\n",
    "        from_tf=bool(\".ckpt\" in my_args.model_name_or_path),\n",
    "        config=config,\n",
    "        cache_dir=my_args.cache_dir if my_args.cache_dir else None,\n",
    "    )\n",
    "    model.to(my_args.device)\n",
    "    if my_args.local_rank == 0:\n",
    "        torch.distributed.barrier()  # End of barrier to make sure only the first process in distributed training download model & vocab\n",
    "    logger.info(\"Training/evaluation parameters %s\", my_args)\n",
    "####################################################################################################\n",
    "    # Training\n",
    "    if my_args.do_train:\n",
    "        if my_args.local_rank not in [-1, 0]:\n",
    "            torch.distributed.barrier()\n",
    "        train_dataset = load_and_cache_examples(my_args, tokenizer, logger, evaluate=False)\n",
    "        if my_args.local_rank == 0:\n",
    "            torch.distributed.barrier()\n",
    "        global_step, tr_loss = train(my_args, train_dataset, model, tokenizer, logger)\n",
    "        logger.info(\" global_step = %s, average loss = %s\", global_step, tr_loss)\n",
    "        \n",
    "    if my_args.do_train and (my_args.local_rank == -1 or torch.distributed.get_rank() == 0):\n",
    "        if not os.path.exists(my_args.output_dir) and my_args.local_rank in [-1, 0]:\n",
    "            os.makedirs(my_args.output_dir)\n",
    "        logger.info(\"Saving model checkpoint to %s\", my_args.output_dir)\n",
    "        model_to_save = (\n",
    "            model.module if hasattr(model, \"module\") else model\n",
    "        )  # Take care of distributed/parallel training\n",
    "        model_to_save.save_pretrained(my_args.output_dir)\n",
    "        tokenizer.save_pretrained(my_args.output_dir)\n",
    "        torch.save(my_args, os.path.join(my_args.output_dir, \"training_args.bin\"))\n",
    "\n",
    "        model = model_class.from_pretrained(my_args.output_dir)\n",
    "        tokenizer = tokenizer_class.from_pretrained(my_args.output_dir, do_lower_case=my_args.do_lower_case)\n",
    "        model.to(my_args.device)\n",
    "\n",
    "    # Evaluation\n",
    "    results = {}\n",
    "    if my_args.do_eval and my_args.local_rank in [-1, 0]:\n",
    "        checkpoints = [my_args.output_dir]\n",
    "        if my_args.eval_all_checkpoints:\n",
    "            checkpoints = list(\n",
    "                os.path.dirname(c) for c in sorted(glob.glob(my_args.output_dir + \"/**/\" + WEIGHTS_NAME, recursive=True))\n",
    "            )\n",
    "            logging.getLogger(\"transformers.modeling_utils\").setLevel(logging.WARN)  # Reduce logging\n",
    "        logger.info(\"Evaluate the following checkpoints: %s\", checkpoints)\n",
    "        for checkpoint in checkpoints:\n",
    "            global_step = checkpoint.split(\"-\")[-1] if len(checkpoints) > 1 else \"\"\n",
    "            prefix = checkpoint.split(\"/\")[-1] if checkpoint.find(\"checkpoint\") != -1 else \"\"\n",
    "\n",
    "            model = model_class.from_pretrained(checkpoint)\n",
    "            model.to(my_args.device)\n",
    "            result = evaluate(my_args, model, tokenizer, logger, prefix=prefix)\n",
    "            result = dict((k + \"_{}\".format(global_step), v) for k, v in result.items())\n",
    "            results.update(result)\n",
    "            \n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fase_SC(from_pre, batch_si, d_lab, max_len):\n",
    "    if from_pre:\n",
    "        model_dir_ft = 'bert-base-uncased'\n",
    "    else:\n",
    "        model_dir_ft = '../../../../model_save/Dos-Fases-all_Stance_prueba'\n",
    "    print('Loading BERT tokenizer...')\n",
    "    tokenizer = BertTokenizer.from_pretrained(model_dir_ft, do_lower_case=True)\n",
    "    print('Loading BERT Seq Class...')\n",
    "    model_loaded = BertForSequenceClassification.from_pretrained(model_dir_ft, num_labels=4) \n",
    "    model_loaded.cuda()\n",
    "\n",
    "    print (\"Modelo cargado correctamente desde \", model_dir_ft)\n",
    "    df_read=pd.read_csv(\"../../CSV_Stance/train_semeval_raw.csv\")\n",
    "    etiq=[]\n",
    "    for et in df_read['Label'].values:\n",
    "        etiq.append(d_lab[et])\n",
    "    sentences = df_read['Tweet'].values\n",
    "    n_labels = np.array(etiq)\n",
    "    input_ids = []\n",
    "    for sent in sentences:\n",
    "        myc, myc_list= my_cleaner(sent)\n",
    "        encoded_sent = tokenizer.encode(myc,add_special_tokens = True )    \n",
    "        input_ids.append(encoded_sent)\n",
    "        \n",
    "    input_ids, attention_masks = make_padding_and_masks(max_len, tokenizer, input_ids)\n",
    "    train_all, val_all = data_batches(input_ids, attention_masks, n_labels, test_size=0.1, batch_size=batch_si, mode='train')\n",
    "    train_data, train_sampler, train_dataloader = train_all\n",
    "    validation_data, validation_sampler, validation_dataloader = val_all\n",
    "\n",
    "    optimizer = AdamW(model_loaded.parameters(), lr = 2e-5, eps = 1e-8)\n",
    "    epochs = 1\n",
    "    total_steps = len(train_dataloader) * epochs\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps = 0, num_training_steps = total_steps)\n",
    "\n",
    "    seed_val = 42\n",
    "    random.seed(seed_val)\n",
    "    np.random.seed(seed_val)\n",
    "    torch.manual_seed(seed_val)\n",
    "    torch.cuda.manual_seed_all(seed_val)\n",
    "    loss_values = []\n",
    "    model_loaded.zero_grad()    \n",
    "    \n",
    "    total_loss = 0\n",
    "    model_loaded.train()        \n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        model_loaded.train()\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)                \n",
    "        outputs = model_loaded(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
    "        loss = outputs[0]\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model_loaded.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        model_loaded.zero_grad()\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_dataloader)       \n",
    "    loss_values.append(avg_train_loss)\n",
    "    #print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    #print(\"Running Validation...\")\n",
    "    model_loaded.eval()\n",
    "    eval_loss, eval_accuracy, eval_fscore = 0, 0, 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "    for batch in validation_dataloader:\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        with torch.no_grad():              \n",
    "            outputs = model_loaded(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)        \n",
    "        logits = outputs[0]\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()     \n",
    "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "        eval_accuracy += tmp_eval_accuracy\n",
    "        tmp_eval_fscore = flat_fscore(logits, label_ids)\n",
    "        eval_fscore += tmp_eval_fscore\n",
    "        nb_eval_steps += 1\n",
    "    #print(\"  F-score macro: {0:.2f}\".format(eval_fscore/nb_eval_steps))\n",
    "    #print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
    "    #df_read=pd.read_csv(\"../CSV_Stance/dev_semeval_raw.csv\")\n",
    "    df_read=pd.read_csv(\"../../CSV_Stance/test_semeval_raw.csv\")\n",
    "    etiq=[]\n",
    "    for et in df_read['Label'].values:\n",
    "        etiq.append(d_lab[et])\n",
    "    sentences = df_read['Tweet'].values\n",
    "    n_labels = np.array(etiq)\n",
    "    input_ids = []\n",
    "    for sent in sentences:\n",
    "        myc, myc_list= my_cleaner(sent)\n",
    "        encoded_sent = tokenizer.encode(myc,add_special_tokens = True )   \n",
    "        input_ids.append(encoded_sent)\n",
    "    input_ids, attention_masks = make_padding_and_masks(max_len, tokenizer, input_ids)\n",
    "    pred_all = data_batches(input_ids, attention_masks, n_labels, test_size=0.1, batch_size=batch_si, mode='eval')\n",
    "    prediction_data, prediction_sampler, prediction_dataloader = pred_all\n",
    "    \n",
    "    model_loaded.eval()\n",
    "    predictions , true_labels = [], []\n",
    "    for batch in prediction_dataloader:\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        with torch.no_grad():\n",
    "            outputs = model_loaded(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "        logits = outputs[0]\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "        predictions.append(logits)\n",
    "        true_labels.append(label_ids)\n",
    "    f_macros = []\n",
    "    print('Calculating F-macro...')\n",
    "    flat_predictions = [item for sublist in predictions for item in sublist]\n",
    "    flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
    "    flat_true_labels = [item for sublist in true_labels for item in sublist]\n",
    "    fma = f1_score(flat_true_labels, flat_predictions, average='macro')\n",
    "    fno = f1_score(flat_true_labels, flat_predictions, average=None)\n",
    "    acc = accuracy_score(flat_true_labels, flat_predictions)\n",
    "    print('F macro: %.3f' % fma)\n",
    "    print('F macro none average: ', fno)\n",
    "    print('Accuracy: %.3f' % acc)\n",
    "    \n",
    "    state = {\n",
    "            'device': device,\n",
    "            'f_macro': fma,\n",
    "            'F_none': fno,\n",
    "            'Accuracy': acc,\n",
    "            'epoch': epoch_macro,\n",
    "            'batch_size': batch_si, \n",
    "            'max_len': max_len,\n",
    "            'optimizer': optimizer ,\n",
    "            'orden_epoch': 'primero'\n",
    "        }\n",
    "    save_model_to(out, model_loaded, tokenizer, state)\n",
    "    \n",
    "    del model_loaded\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    return fma, fno, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import resource\n",
    "\n",
    "def mem():\n",
    "    print('Memory usage         : % 2.2f MB' % round(\n",
    "        resource.getrusage(resource.RUSAGE_SELF).ru_maxrss/1024.0/1024.0,1)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/05/2020 00:21:00 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "04/05/2020 00:21:00 - INFO - transformers.configuration_utils -   loading configuration file ../../../../model_save/Dos-Fases-all_Stance_prueba/checkpoint-50/config.json\n",
      "04/05/2020 00:21:00 - INFO - transformers.configuration_utils -   Model config BertConfig {\n",
      "  \"_num_labels\": 4,\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": null,\n",
      "  \"decoder_start_token_id\": null,\n",
      "  \"do_sample\": false,\n",
      "  \"early_stopping\": false,\n",
      "  \"eos_token_id\": null,\n",
      "  \"eos_token_ids\": null,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"is_encoder_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"min_length\": 0,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"no_repeat_ngram_size\": 0,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": null,\n",
      "  \"prefix\": null,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"task_specific_params\": null,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "04/05/2020 00:21:00 - INFO - transformers.tokenization_utils -   Model name '../../../../model_save/Dos-Fases-all_Stance_prueba/checkpoint-50' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming '../../../../model_save/Dos-Fases-all_Stance_prueba/checkpoint-50' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "04/05/2020 00:21:00 - INFO - transformers.tokenization_utils -   Didn't find file ../../../../model_save/Dos-Fases-all_Stance_prueba/checkpoint-50/added_tokens.json. We won't load it.\n",
      "04/05/2020 00:21:00 - INFO - transformers.tokenization_utils -   loading file ../../../../model_save/Dos-Fases-all_Stance_prueba/checkpoint-50/vocab.txt\n",
      "04/05/2020 00:21:00 - INFO - transformers.tokenization_utils -   loading file None\n",
      "04/05/2020 00:21:00 - INFO - transformers.tokenization_utils -   loading file ../../../../model_save/Dos-Fases-all_Stance_prueba/checkpoint-50/special_tokens_map.json\n",
      "04/05/2020 00:21:00 - INFO - transformers.tokenization_utils -   loading file ../../../../model_save/Dos-Fases-all_Stance_prueba/checkpoint-50/tokenizer_config.json\n",
      "04/05/2020 00:21:00 - INFO - transformers.modeling_utils -   loading weights file ../../../../model_save/Dos-Fases-all_Stance_prueba/checkpoint-50/pytorch_model.bin\n",
      "04/05/2020 00:21:05 - INFO - __main__ -   Training/evaluation parameters <__main__.Args_LM object at 0x7f66a254f6d8>\n",
      "04/05/2020 00:21:05 - INFO - __main__ -   Loading features from cached file ../../CSV_Stance/bert_cached_lm_62_Stance_raw_train.txt\n",
      "04/05/2020 00:21:05 - INFO - __main__ -   ***** Running training *****\n",
      "04/05/2020 00:21:05 - INFO - __main__ -     Num examples = 1977\n",
      "04/05/2020 00:21:05 - INFO - __main__ -     Num Epochs = 1\n",
      "04/05/2020 00:21:05 - INFO - __main__ -     Instantaneous batch size per GPU = 32\n",
      "04/05/2020 00:21:05 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "04/05/2020 00:21:05 - INFO - __main__ -     Gradient Accumulation steps = 1\n",
      "04/05/2020 00:21:05 - INFO - __main__ -     Total optimization steps = 62\n",
      "04/05/2020 00:21:05 - INFO - __main__ -     Continuing training from checkpoint, will skip to saved global_step\n",
      "04/05/2020 00:21:05 - INFO - __main__ -     Continuing training from epoch 0\n",
      "04/05/2020 00:21:05 - INFO - __main__ -     Continuing training from global step 50\n",
      "04/05/2020 00:21:05 - INFO - __main__ -     Will skip the first 50 steps in the first epoch\n",
      "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Iteration:   0%|          | 0/62 [00:00<?, ?it/s]\u001b[A\n",
      "Iteration:  82%|████████▏ | 51/62 [00:00<00:00, 70.35it/s]\u001b[A\n",
      "Iteration:  87%|████████▋ | 54/62 [00:02<00:01,  5.86it/s]\u001b[A\n",
      "Iteration:  90%|█████████ | 56/62 [00:03<00:01,  3.56it/s]\u001b[A\n",
      "Iteration:  94%|█████████▎| 58/62 [00:04<00:01,  2.79it/s]\u001b[A\n",
      "Iteration:  95%|█████████▌| 59/62 [00:05<00:01,  2.42it/s]\u001b[A\n",
      "Iteration:  97%|█████████▋| 60/62 [00:05<00:00,  2.21it/s]\u001b[A\n",
      "Iteration:  98%|█████████▊| 61/62 [00:06<00:00,  2.09it/s]\u001b[A\n",
      "Iteration: 100%|██████████| 62/62 [00:06<00:00,  9.44it/s]\u001b[A\n",
      "Epoch: 100%|██████████| 1/1 [00:06<00:00,  6.57s/it]\n",
      "04/05/2020 00:21:11 - INFO - __main__ -    global_step = 62, average loss = 0.800356795710902\n",
      "04/05/2020 00:21:11 - INFO - __main__ -   Saving model checkpoint to ../../../../model_save/Dos-Fases-all_Stance_prueba\n",
      "04/05/2020 00:21:11 - INFO - transformers.configuration_utils -   Configuration saved in ../../../../model_save/Dos-Fases-all_Stance_prueba/config.json\n",
      "04/05/2020 00:21:15 - INFO - transformers.modeling_utils -   Model weights saved in ../../../../model_save/Dos-Fases-all_Stance_prueba/pytorch_model.bin\n",
      "04/05/2020 00:21:15 - INFO - transformers.configuration_utils -   loading configuration file ../../../../model_save/Dos-Fases-all_Stance_prueba/config.json\n",
      "04/05/2020 00:21:15 - INFO - transformers.configuration_utils -   Model config BertConfig {\n",
      "  \"_num_labels\": 4,\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": null,\n",
      "  \"decoder_start_token_id\": null,\n",
      "  \"do_sample\": false,\n",
      "  \"early_stopping\": false,\n",
      "  \"eos_token_id\": null,\n",
      "  \"eos_token_ids\": null,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"is_encoder_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"min_length\": 0,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"no_repeat_ngram_size\": 0,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": null,\n",
      "  \"prefix\": null,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"task_specific_params\": null,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "04/05/2020 00:21:15 - INFO - transformers.modeling_utils -   loading weights file ../../../../model_save/Dos-Fases-all_Stance_prueba/pytorch_model.bin\n",
      "04/05/2020 00:21:17 - INFO - transformers.tokenization_utils -   Model name '../../../../model_save/Dos-Fases-all_Stance_prueba' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming '../../../../model_save/Dos-Fases-all_Stance_prueba' is a path, a model identifier, or url to a directory containing tokenizer files.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/05/2020 00:21:17 - INFO - transformers.tokenization_utils -   Didn't find file ../../../../model_save/Dos-Fases-all_Stance_prueba/added_tokens.json. We won't load it.\n",
      "04/05/2020 00:21:17 - INFO - transformers.tokenization_utils -   loading file ../../../../model_save/Dos-Fases-all_Stance_prueba/vocab.txt\n",
      "04/05/2020 00:21:17 - INFO - transformers.tokenization_utils -   loading file None\n",
      "04/05/2020 00:21:17 - INFO - transformers.tokenization_utils -   loading file ../../../../model_save/Dos-Fases-all_Stance_prueba/special_tokens_map.json\n",
      "04/05/2020 00:21:17 - INFO - transformers.tokenization_utils -   loading file ../../../../model_save/Dos-Fases-all_Stance_prueba/tokenizer_config.json\n",
      "04/05/2020 00:21:17 - INFO - __main__ -   Evaluate the following checkpoints: ['../../../../model_save/Dos-Fases-all_Stance_prueba/checkpoint-50', '../../../../model_save/Dos-Fases-all_Stance_prueba']\n",
      "04/05/2020 00:21:17 - INFO - transformers.configuration_utils -   loading configuration file ../../../../model_save/Dos-Fases-all_Stance_prueba/checkpoint-50/config.json\n",
      "04/05/2020 00:21:17 - INFO - transformers.configuration_utils -   Model config BertConfig {\n",
      "  \"_num_labels\": 4,\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": null,\n",
      "  \"decoder_start_token_id\": null,\n",
      "  \"do_sample\": false,\n",
      "  \"early_stopping\": false,\n",
      "  \"eos_token_id\": null,\n",
      "  \"eos_token_ids\": null,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"is_encoder_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"min_length\": 0,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"no_repeat_ngram_size\": 0,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": null,\n",
      "  \"prefix\": null,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"task_specific_params\": null,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "04/05/2020 00:21:19 - INFO - __main__ -   Creating features from dataset file at ../../CSV_Stance\n",
      "04/05/2020 00:21:19 - INFO - __main__ -   Saving features into cached file ../../CSV_Stance/bert_cached_lm_62_Stance_raw_test.txt\n",
      "04/05/2020 00:21:19 - INFO - __main__ -   ***** Running evaluation checkpoint-50 *****\n",
      "04/05/2020 00:21:19 - INFO - __main__ -     Num examples = 471\n",
      "04/05/2020 00:21:19 - INFO - __main__ -     Batch size = 32\n",
      "Evaluating: 100%|██████████| 15/15 [00:02<00:00,  6.14it/s]\n",
      "04/05/2020 00:21:22 - INFO - __main__ -   ***** Eval results checkpoint-50 *****\n",
      "04/05/2020 00:21:22 - INFO - __main__ -     perplexity = tensor(93.2336)\n",
      "04/05/2020 00:21:22 - INFO - transformers.configuration_utils -   loading configuration file ../../../../model_save/Dos-Fases-all_Stance_prueba/config.json\n",
      "04/05/2020 00:21:22 - INFO - transformers.configuration_utils -   Model config BertConfig {\n",
      "  \"_num_labels\": 4,\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": null,\n",
      "  \"decoder_start_token_id\": null,\n",
      "  \"do_sample\": false,\n",
      "  \"early_stopping\": false,\n",
      "  \"eos_token_id\": null,\n",
      "  \"eos_token_ids\": null,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"is_encoder_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"min_length\": 0,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"no_repeat_ngram_size\": 0,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": null,\n",
      "  \"prefix\": null,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"task_specific_params\": null,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "04/05/2020 00:21:24 - INFO - __main__ -   Loading features from cached file ../../CSV_Stance/bert_cached_lm_62_Stance_raw_test.txt\n",
      "04/05/2020 00:21:24 - INFO - __main__ -   ***** Running evaluation  *****\n",
      "04/05/2020 00:21:24 - INFO - __main__ -     Num examples = 471\n",
      "04/05/2020 00:21:24 - INFO - __main__ -     Batch size = 32\n",
      "Evaluating: 100%|██████████| 15/15 [00:02<00:00,  6.14it/s]\n",
      "04/05/2020 00:21:26 - INFO - __main__ -   ***** Eval results  *****\n",
      "04/05/2020 00:21:26 - INFO - __main__ -     perplexity = tensor(88.3864)\n",
      "04/05/2020 00:21:27 - INFO - transformers.tokenization_utils -   Model name '../../../../model_save/Dos-Fases-all_Stance_prueba' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming '../../../../model_save/Dos-Fases-all_Stance_prueba' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "04/05/2020 00:21:27 - INFO - transformers.tokenization_utils -   Didn't find file ../../../../model_save/Dos-Fases-all_Stance_prueba/added_tokens.json. We won't load it.\n",
      "04/05/2020 00:21:27 - INFO - transformers.tokenization_utils -   loading file ../../../../model_save/Dos-Fases-all_Stance_prueba/vocab.txt\n",
      "04/05/2020 00:21:27 - INFO - transformers.tokenization_utils -   loading file None\n",
      "04/05/2020 00:21:27 - INFO - transformers.tokenization_utils -   loading file ../../../../model_save/Dos-Fases-all_Stance_prueba/special_tokens_map.json\n",
      "04/05/2020 00:21:27 - INFO - transformers.tokenization_utils -   loading file ../../../../model_save/Dos-Fases-all_Stance_prueba/tokenizer_config.json\n",
      "04/05/2020 00:21:27 - INFO - transformers.configuration_utils -   loading configuration file ../../../../model_save/Dos-Fases-all_Stance_prueba/config.json\n",
      "04/05/2020 00:21:27 - INFO - transformers.configuration_utils -   Model config BertConfig {\n",
      "  \"_num_labels\": 4,\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": null,\n",
      "  \"decoder_start_token_id\": null,\n",
      "  \"do_sample\": false,\n",
      "  \"early_stopping\": false,\n",
      "  \"eos_token_id\": null,\n",
      "  \"eos_token_ids\": null,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"is_encoder_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"min_length\": 0,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"no_repeat_ngram_size\": 0,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": null,\n",
      "  \"prefix\": null,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"task_specific_params\": null,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity epoch: 93.23355\n",
      "\n",
      "Memory usage         :  3.80 MB\n",
      "GC collected objects : 854\n",
      "Memory usage         :  3.80 MB\n",
      "GC collected objects : 0\n",
      "Memory usage         :  3.80 MB\n",
      "Loading BERT tokenizer...\n",
      "Loading BERT Seq Class...\n",
      "Modelo cargado correctamente desde  ../../../../model_save/Dos-Fases-all_Stance_prueba\n",
      "\n",
      "Padding/truncating all sentences to 50 values...\n",
      "\n",
      "Padding token: \"[PAD]\", ID: 0\n",
      "Completado.\n",
      "\n",
      "Padding/truncating all sentences to 50 values...\n",
      "\n",
      "Padding token: \"[PAD]\", ID: 0\n",
      "Completado.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/05/2020 00:22:35 - INFO - transformers.configuration_utils -   Configuration saved in ../../../../model_save/Dos-Fases-all_Stance_prueba/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating F-macro...\n",
      "F macro: 0.400\n",
      "F macro none average:  [0.62200957 0.08333333 0.85203453 0.04395604]\n",
      "Accuracy: 0.747\n",
      "Saving model to ../../../../model_save/Dos-Fases-all_Stance_prueba\n",
      "Memory usage         :  3.80 MB\n",
      "GC collected objects : 427\n",
      "Memory usage         :  3.80 MB\n",
      "GC collected objects : 0\n",
      "Memory usage         :  3.80 MB\n"
     ]
    }
   ],
   "source": [
    "Num_epochs= 1\n",
    "out= '../../../../model_save/Dos-Fases-all_Stance_prueba'\n",
    "from_pre = True\n",
    "\n",
    "f= open(out+\"/Summary_macro.txt\",\"w\")\n",
    "f.write(\"This is a summary of the running \\n\")\n",
    "f.close()\n",
    "\n",
    "while epoch_macro <= Num_epochs:\n",
    "    ####################################################\n",
    "    ################## LANGUAGE MODEL ##################\n",
    "    f=open(out+\"/Summary_macro.txt\", \"a+\")\n",
    "    f.write(\"------------------------------------\\n\")\n",
    "    f.write(\"Running Language Model epoch: \"+str(epoch_macro)+'\\n')\n",
    "    resultado_lm = fase_LM(from_pre, my_args, logger)\n",
    "    print (\"Perplexity epoch: \"+str(list(resultado_lm.values())[0].cpu().numpy())+'\\n')\n",
    "    f.write(\"Perplexity epoch: \"+str(list(resultado_lm.values())[0].cpu().numpy())+'\\n')\n",
    "    f.close()\n",
    "    ####################################################\n",
    "    ################### Free memory ####################\n",
    "    mem()\n",
    "    print('GC collected objects : %d' % gc.collect())\n",
    "    mem()\n",
    "    \n",
    "    gc.collect()    \n",
    "    \n",
    "    print('GC collected objects : %d' % gc.collect())\n",
    "    mem()\n",
    "    ####################################################\n",
    "    ################### SEQUENCE CLASS #################\n",
    "    f=open(out+\"/Summary_macro.txt\", \"a+\")\n",
    "    f.write(\"------------------------------------\\n\")\n",
    "    f.write(\"Running Sentence Classification epoch: \"+str(epoch_macro)+'\\n')\n",
    "    \n",
    "    from_pre = False\n",
    "    #qscd\n",
    "    d_lab=dict()\n",
    "    d_lab[\"questioning\"]=0\n",
    "    d_lab[\"support\"]=1\n",
    "    d_lab[\"commenting\"]=2\n",
    "    d_lab[\"denying\"]=3\n",
    "      \n",
    "    max_len = 50\n",
    "    batch = 16 \n",
    "    fma, fno, acc = fase_SC(from_pre, batch, d_lab, max_len)\n",
    "    f.write(\"F macro: \"+str(fma)+'\\n')\n",
    "    f.write(\"F macro none avergage: \"+str(fno)+'\\n')\n",
    "    f.write(\"Accuracy: \"+str(acc)+'\\n')\n",
    "    f.write(\"------------------------------------\\n\")\n",
    "    f.close()\n",
    "    \n",
    "    ####################################################\n",
    "    ################### Free memory ####################\n",
    "    mem()\n",
    "    print('GC collected objects : %d' % gc.collect())\n",
    "    mem()\n",
    "    \n",
    "    gc.collect()    \n",
    "    \n",
    "    print('GC collected objects : %d' % gc.collect())\n",
    "    mem()\n",
    "    \n",
    "    epoch_macro += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:newpy3]",
   "language": "python",
   "name": "conda-env-newpy3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
