{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import os\n",
    "from sc_ft import *\n",
    "from lm_ft import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: GeForce GTX 1060 6GB\n"
     ]
    }
   ],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "epoch_macro= 1\n",
    "\n",
    "MODEL_CLASSES = {\n",
    "    \"gpt2\": (GPT2Config, GPT2LMHeadModel, GPT2Tokenizer),\n",
    "    \"openai-gpt\": (OpenAIGPTConfig, OpenAIGPTLMHeadModel, OpenAIGPTTokenizer),\n",
    "    \"bert\": (BertConfig, BertForMaskedLM, BertTokenizer),\n",
    "    \"roberta\": (RobertaConfig, RobertaForMaskedLM, RobertaTokenizer),\n",
    "}\n",
    "\n",
    "if torch.cuda.is_available():       \n",
    "    device = torch.device(\"cuda\")\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args_LM:\n",
    "    adam_epsilon=1e-8\n",
    "    block_size=-1 \n",
    "    cache_dir=\"\"\n",
    "    config_name=\"\"\n",
    "    do_train=True        \n",
    "    do_eval=True \n",
    "    do_lower_case=True\n",
    "    eval_all_checkpoints=False\n",
    "    evaluate_during_training=False\n",
    "    eval_data_file=None\n",
    "    fp16= False\n",
    "    fp16_opt_level=\"O1\"\n",
    "    gradient_accumulation_steps=1\n",
    "    learning_rate=5e-5\n",
    "    logging_steps=100\n",
    "    local_rank=-1\n",
    "    model_type= default=\"bert\"\n",
    "    model_name_or_path= \"bert-base-cased\" \n",
    "    mlm=True\n",
    "    mlm_probability=0.15\n",
    "    max_grad_norm=1.0\n",
    "    max_steps=-1\n",
    "    num_train_epochs=1.0\n",
    "    n_gpu=1\n",
    "    no_cuda=False\n",
    "    overwrite_output_dir=True\n",
    "    overwrite_cache=False\n",
    "    output_dir=None\n",
    "    per_gpu_train_batch_size=4\n",
    "    per_gpu_eval_batch_size=4\n",
    "    save_steps=100\n",
    "    save_total_limit=None    \n",
    "    seed=42   \n",
    "    server_ip=\"\"\n",
    "    server_port=\"\"\n",
    "    train_data_file=None #str\n",
    "    tokenizer_name=\"\" \n",
    "    weight_decay=0.0   \n",
    "    warmup_steps=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert-base-uncased_cached_lm_128_Stance_raw_train.txt  Stance_raw_test\r\n",
      "bert-base-uncased_cached_lm_128_Stance_raw_val.txt    Stance_raw_test.txt\r\n",
      "bert-base-uncased_cached_lm_40_Stance_raw_train.txt   Stance_raw_train\r\n",
      "bert-base-uncased_cached_lm_40_Stance_raw_val.txt     Stance_raw_train.txt\r\n",
      "bert-base-uncased_cached_lm_64_Stance_raw_test.txt    Stance_raw_val\r\n",
      "bert-base-uncased_cached_lm_64_Stance_raw_train.txt   Stance_raw_val.txt\r\n",
      "bert-base-uncased_cached_lm_64_Stance_raw_val.txt     test_semeval_raw.csv\r\n",
      "dev_semeval_raw.csv\t\t\t\t      train_semeval_raw.csv\r\n"
     ]
    }
   ],
   "source": [
    "!ls ../../CSV_Stance/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "out= '../../../../model_save/Dos-Fases-all_Stance'\n",
    "\n",
    "my_args = Args_LM()\n",
    "my_args.output_dir = out\n",
    "my_args.model_type = 'bert'\n",
    "my_args.train_data_file = '../../CSV_Stance/Stance_raw_train.txt'\n",
    "my_args.block_size = 64\n",
    "my_args.eval_data_file = '../../CSV_Stance/Stance_raw_test.txt'\n",
    "my_args.per_gpu_train_batch_size = 16\n",
    "my_args.per_gpu_eval_batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fase_LM(from_pre, my_args, logger):\n",
    "    if from_pre:\n",
    "        model_dir_ft = 'bert-base-uncased'\n",
    "        my_args.model_name_or_path = model_dir_ft\n",
    "    else:\n",
    "        model_dir_ft = out\n",
    "        my_args.model_name_or_path = model_dir_ft\n",
    "    \n",
    "    if my_args.model_type in [\"bert\", \"roberta\"] and not my_args.mlm:\n",
    "        raise ValueError(\n",
    "            \"BERT and RoBERTa do not have LM heads but masked LM heads. They must be run using the --mlm \"\n",
    "            \"flag (masked language modeling).\"\n",
    "        )\n",
    "    if my_args.eval_data_file is None and my_args.do_eval:\n",
    "        raise ValueError(\n",
    "            \"Cannot do evaluation without an evaluation data file. Either supply a file to --eval_data_file \"\n",
    "            \"or remove the --do_eval argument.\"\n",
    "        )\n",
    "\n",
    "    if (\n",
    "        os.path.exists(my_args.output_dir)\n",
    "        and os.listdir(my_args.output_dir)\n",
    "        and my_args.do_train\n",
    "        and not my_args.overwrite_output_dir\n",
    "    ):\n",
    "        raise ValueError(\n",
    "            \"Output directory ({}) already exists and is not empty. Use --overwrite_output_dir to overcome.\".format(\n",
    "                my_args.output_dir\n",
    "            )\n",
    "        )\n",
    "\n",
    "    if my_args.server_ip and my_args.server_port:\n",
    "        import ptvsd\n",
    "        #print(\"Waiting for debugger attach\")\n",
    "        ptvsd.enable_attach(address=(my_args.server_ip, my_args.server_port), redirect_output=True)\n",
    "        ptvsd.wait_for_attach()\n",
    "    if my_args.local_rank == -1 or my_args.no_cuda:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() and not my_args.no_cuda else \"cpu\")\n",
    "        my_args.n_gpu = torch.cuda.device_count()\n",
    "    else:  # Initializes the distributed backend which will take care of sychronizing nodes/GPUs\n",
    "        torch.cuda.set_device(my_args.local_rank)\n",
    "        device = torch.device(\"cuda\", my_args.local_rank)\n",
    "        torch.distributed.init_process_group(backend=\"nccl\")\n",
    "        my_args.n_gpu = 1\n",
    "    my_args.device = device\n",
    "    logging.basicConfig(\n",
    "        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
    "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "        level=logging.INFO if my_args.local_rank in [-1, 0] else logging.WARN,\n",
    "    )\n",
    "    logger.warning(\n",
    "        \"Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s\",\n",
    "        my_args.local_rank,\n",
    "        device,\n",
    "        my_args.n_gpu,\n",
    "        bool(my_args.local_rank != -1),\n",
    "        my_args.fp16,\n",
    "    )\n",
    "    set_seed(my_args)\n",
    "    if my_args.local_rank not in [-1, 0]:\n",
    "        torch.distributed.barrier()  # Barrier to make sure only the first process in distributed training download model & vocab\n",
    "    config_class, model_class, tokenizer_class = MODEL_CLASSES[my_args.model_type]\n",
    "    #print (\"\\nLoadding model \", model_class)\n",
    "    config = config_class.from_pretrained(\n",
    "        my_args.config_name if my_args.config_name else my_args.model_name_or_path,\n",
    "        cache_dir=my_args.cache_dir if my_args.cache_dir else None,\n",
    "    )\n",
    "    tokenizer = tokenizer_class.from_pretrained(\n",
    "        my_args.tokenizer_name if my_args.tokenizer_name else my_args.model_name_or_path,\n",
    "        do_lower_case=my_args.do_lower_case,\n",
    "        cache_dir=my_args.cache_dir if my_args.cache_dir else None,\n",
    "    )\n",
    "    if my_args.block_size <= 0:\n",
    "        my_args.block_size = (\n",
    "            tokenizer.max_len_single_sentence\n",
    "        )  # Our input block size will be the max possible for the model\n",
    "    my_args.block_size = min(my_args.block_size, tokenizer.max_len_single_sentence)\n",
    "    model = model_class.from_pretrained(\n",
    "        my_args.model_name_or_path,\n",
    "        from_tf=bool(\".ckpt\" in my_args.model_name_or_path),\n",
    "        config=config,\n",
    "        cache_dir=my_args.cache_dir if my_args.cache_dir else None,\n",
    "    )\n",
    "    model.to(my_args.device)\n",
    "    if my_args.local_rank == 0:\n",
    "        torch.distributed.barrier()  # End of barrier to make sure only the first process in distributed training download model & vocab\n",
    "    logger.info(\"Training/evaluation parameters %s\", my_args)\n",
    "####################################################################################################\n",
    "    # Training\n",
    "    if my_args.do_train:\n",
    "        if my_args.local_rank not in [-1, 0]:\n",
    "            torch.distributed.barrier()\n",
    "        train_dataset = load_and_cache_examples(my_args, tokenizer, logger, evaluate=False)\n",
    "        if my_args.local_rank == 0:\n",
    "            torch.distributed.barrier()\n",
    "        global_step, tr_loss = train(my_args, train_dataset, model, tokenizer, logger)\n",
    "        logger.info(\" global_step = %s, average loss = %s\", global_step, tr_loss)\n",
    "        \n",
    "    if my_args.do_train and (my_args.local_rank == -1 or torch.distributed.get_rank() == 0):\n",
    "        if not os.path.exists(my_args.output_dir) and my_args.local_rank in [-1, 0]:\n",
    "            os.makedirs(my_args.output_dir)\n",
    "        logger.info(\"Saving model checkpoint to %s\", my_args.output_dir)\n",
    "        model_to_save = (\n",
    "            model.module if hasattr(model, \"module\") else model\n",
    "        )  # Take care of distributed/parallel training\n",
    "        model_to_save.save_pretrained(my_args.output_dir)\n",
    "        tokenizer.save_pretrained(my_args.output_dir)\n",
    "        torch.save(my_args, os.path.join(my_args.output_dir, \"training_args.bin\"))\n",
    "\n",
    "        model = model_class.from_pretrained(my_args.output_dir)\n",
    "        tokenizer = tokenizer_class.from_pretrained(my_args.output_dir, do_lower_case=my_args.do_lower_case)\n",
    "        model.to(my_args.device)\n",
    "\n",
    "    # Evaluation\n",
    "    results = {}\n",
    "    if my_args.do_eval and my_args.local_rank in [-1, 0]:\n",
    "        checkpoints = [my_args.output_dir]\n",
    "        if my_args.eval_all_checkpoints:\n",
    "            checkpoints = list(\n",
    "                os.path.dirname(c) for c in sorted(glob.glob(my_args.output_dir + \"/**/\" + WEIGHTS_NAME, recursive=True))\n",
    "            )\n",
    "            logging.getLogger(\"transformers.modeling_utils\").setLevel(logging.WARN)  # Reduce logging\n",
    "        logger.info(\"Evaluate the following checkpoints: %s\", checkpoints)\n",
    "        for checkpoint in checkpoints:\n",
    "            global_step = checkpoint.split(\"-\")[-1] if len(checkpoints) > 1 else \"\"\n",
    "            prefix = checkpoint.split(\"/\")[-1] if checkpoint.find(\"checkpoint\") != -1 else \"\"\n",
    "\n",
    "            model = model_class.from_pretrained(checkpoint)\n",
    "            model.to(my_args.device)\n",
    "            result = evaluate(my_args, model, tokenizer, logger, prefix=prefix)\n",
    "            result = dict((k + \"_{}\".format(global_step), v) for k, v in result.items())\n",
    "            results.update(result)\n",
    "            \n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fase_SC(from_pre, batch_si, d_lab, max_len):\n",
    "    if from_pre:\n",
    "        model_dir_ft = 'bert-base-uncased'\n",
    "    else:\n",
    "        model_dir_ft = '../../../../model_save/Dos-Fases-all_Stance'\n",
    "    print('Loading BERT tokenizer...')\n",
    "    tokenizer = BertTokenizer.from_pretrained(model_dir_ft, do_lower_case=True)\n",
    "    print('Loading BERT Seq Class...')\n",
    "    model_loaded = BertForSequenceClassification.from_pretrained(model_dir_ft, num_labels=4) \n",
    "    model_loaded.cuda()\n",
    "\n",
    "    print (\"Modelo cargado correctamente desde \", model_dir_ft)\n",
    "    df_read=pd.read_csv(\"../../CSV_Stance/train_semeval_raw.csv\")\n",
    "    etiq=[]\n",
    "    for et in df_read['Label'].values:\n",
    "        etiq.append(d_lab[et])\n",
    "    sentences = df_read['Tweet'].values\n",
    "    n_labels = np.array(etiq)\n",
    "    input_ids = []\n",
    "    for sent in sentences:\n",
    "        myc, myc_list= my_cleaner(sent)\n",
    "        encoded_sent = tokenizer.encode(myc,add_special_tokens = True )    \n",
    "        input_ids.append(encoded_sent)\n",
    "        \n",
    "    input_ids, attention_masks = make_padding_and_masks(max_len, tokenizer, input_ids)\n",
    "    train_all, val_all = data_batches(input_ids, attention_masks, n_labels, test_size=0.1, batch_size=batch_si, mode='train')\n",
    "    train_data, train_sampler, train_dataloader = train_all\n",
    "    validation_data, validation_sampler, validation_dataloader = val_all\n",
    "\n",
    "    optimizer = AdamW(model_loaded.parameters(), lr = 2e-5, eps = 1e-8)\n",
    "    epochs = 1\n",
    "    total_steps = len(train_dataloader) * epochs\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps = 0, num_training_steps = total_steps)\n",
    "\n",
    "    seed_val = 42\n",
    "    random.seed(seed_val)\n",
    "    np.random.seed(seed_val)\n",
    "    torch.manual_seed(seed_val)\n",
    "    torch.cuda.manual_seed_all(seed_val)\n",
    "    loss_values = []\n",
    "    model_loaded.zero_grad()    \n",
    "    \n",
    "    total_loss = 0\n",
    "    model_loaded.train()        \n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        model_loaded.train()\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)                \n",
    "        outputs = model_loaded(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
    "        loss = outputs[0]\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model_loaded.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        model_loaded.zero_grad()\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_dataloader)       \n",
    "    loss_values.append(avg_train_loss)\n",
    "    #print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    #print(\"Running Validation...\")\n",
    "    model_loaded.eval()\n",
    "    eval_loss, eval_accuracy, eval_fscore = 0, 0, 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "    for batch in validation_dataloader:\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        with torch.no_grad():              \n",
    "            outputs = model_loaded(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)        \n",
    "        logits = outputs[0]\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()     \n",
    "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "        eval_accuracy += tmp_eval_accuracy\n",
    "        tmp_eval_fscore = flat_fscore(logits, label_ids)\n",
    "        eval_fscore += tmp_eval_fscore\n",
    "        nb_eval_steps += 1\n",
    "    #print(\"  F-score macro: {0:.2f}\".format(eval_fscore/nb_eval_steps))\n",
    "    #print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
    "    #df_read=pd.read_csv(\"../CSV_Stance/dev_semeval_raw.csv\")\n",
    "    df_read=pd.read_csv(\"../../CSV_Stance/test_semeval_raw.csv\")\n",
    "    etiq=[]\n",
    "    for et in df_read['Label'].values:\n",
    "        etiq.append(d_lab[et])\n",
    "    sentences = df_read['Tweet'].values\n",
    "    n_labels = np.array(etiq)\n",
    "    input_ids = []\n",
    "    for sent in sentences:\n",
    "        myc, myc_list= my_cleaner(sent)\n",
    "        encoded_sent = tokenizer.encode(myc,add_special_tokens = True )   \n",
    "        input_ids.append(encoded_sent)\n",
    "    input_ids, attention_masks = make_padding_and_masks(max_len, tokenizer, input_ids)\n",
    "    pred_all = data_batches(input_ids, attention_masks, n_labels, test_size=0.1, batch_size=batch_si, mode='eval')\n",
    "    prediction_data, prediction_sampler, prediction_dataloader = pred_all\n",
    "    \n",
    "    model_loaded.eval()\n",
    "    predictions , true_labels = [], []\n",
    "    for batch in prediction_dataloader:\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        with torch.no_grad():\n",
    "            outputs = model_loaded(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "        logits = outputs[0]\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "        predictions.append(logits)\n",
    "        true_labels.append(label_ids)\n",
    "    f_macros = []\n",
    "    print('Calculating F-macro...')\n",
    "    flat_predictions = [item for sublist in predictions for item in sublist]\n",
    "    flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
    "    flat_true_labels = [item for sublist in true_labels for item in sublist]\n",
    "    fma = f1_score(flat_true_labels, flat_predictions, average='macro')\n",
    "    fno = f1_score(flat_true_labels, flat_predictions, average=None)\n",
    "    acc = accuracy_score(flat_true_labels, flat_predictions)\n",
    "    print('F macro: %.3f' % fma)\n",
    "    print('F macro none average: ', fno)\n",
    "    print('Accuracy: %.3f' % acc)\n",
    "    \n",
    "    state = {\n",
    "            'device': device,\n",
    "            'f_macro': fma,\n",
    "            'F_none': fno,\n",
    "            'Accuracy': acc,\n",
    "            'epoch': epoch_macro,\n",
    "            'batch_size': batch_si, \n",
    "            'max_len': max_len,\n",
    "            'optimizer': optimizer ,\n",
    "            'orden_epoch': 'primero'\n",
    "        }\n",
    "    save_model_to(out, model_loaded, tokenizer, state)\n",
    "    \n",
    "    del model_loaded\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    return fma, fno, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import resource\n",
    "\n",
    "def mem():\n",
    "    print('Memory usage         : % 2.2f MB' % round(\n",
    "        resource.getrusage(resource.RUSAGE_SELF).ru_maxrss/1024.0/1024.0,1)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/05/2020 01:35:22 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "03/05/2020 01:35:23 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /home/mbugueno/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.8f56353af4a709bf5ff0fbc915d8f5b42bfff892cbb6ac98c3c45f481a03c685\n",
      "03/05/2020 01:35:23 - INFO - transformers.configuration_utils -   Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": null,\n",
      "  \"do_sample\": false,\n",
      "  \"eos_token_ids\": null,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": null,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "03/05/2020 01:35:24 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/mbugueno/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "03/05/2020 01:35:24 - INFO - transformers.modeling_utils -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /home/mbugueno/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
      "03/05/2020 01:35:27 - INFO - transformers.modeling_utils -   Weights of BertForMaskedLM not initialized from pretrained model: ['cls.predictions.decoder.bias']\n",
      "03/05/2020 01:35:27 - INFO - transformers.modeling_utils -   Weights from pretrained model not used in BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "03/05/2020 01:35:28 - INFO - __main__ -   Training/evaluation parameters <__main__.Args_LM object at 0x7f274a67cd30>\n",
      "03/05/2020 01:35:28 - INFO - __main__ -   ***** Running training *****\n",
      "03/05/2020 01:35:28 - INFO - __main__ -     Num examples = 1916\n",
      "03/05/2020 01:35:28 - INFO - __main__ -     Num Epochs = 1\n",
      "03/05/2020 01:35:28 - INFO - __main__ -     Instantaneous batch size per GPU = 16\n",
      "03/05/2020 01:35:28 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "03/05/2020 01:35:28 - INFO - __main__ -     Gradient Accumulation steps = 1\n",
      "03/05/2020 01:35:28 - INFO - __main__ -     Total optimization steps = 120\n",
      "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Iteration:   0%|          | 0/120 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "directory ../../CSV_Stance\n",
      "FILENAME Stance_raw_train.txt\n",
      "args.model_name_or_path bert-base-uncased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration:   1%|          | 1/120 [00:00<01:02,  1.89it/s]\u001b[A\n",
      "Iteration:   2%|▏         | 2/120 [00:00<00:56,  2.09it/s]\u001b[A\n",
      "Iteration:   2%|▎         | 3/120 [00:01<00:51,  2.27it/s]\u001b[A\n",
      "Iteration:   3%|▎         | 4/120 [00:01<00:48,  2.42it/s]\u001b[A\n",
      "Iteration:   4%|▍         | 5/120 [00:01<00:45,  2.52it/s]\u001b[A\n",
      "Iteration:   5%|▌         | 6/120 [00:02<00:43,  2.61it/s]\u001b[A\n",
      "Iteration:   6%|▌         | 7/120 [00:02<00:42,  2.68it/s]\u001b[A\n",
      "Iteration:   7%|▋         | 8/120 [00:03<00:41,  2.72it/s]\u001b[A\n",
      "Iteration:   8%|▊         | 9/120 [00:03<00:40,  2.75it/s]\u001b[A\n",
      "Iteration:   8%|▊         | 10/120 [00:03<00:39,  2.78it/s]\u001b[A\n",
      "Iteration:   9%|▉         | 11/120 [00:04<00:38,  2.80it/s]\u001b[A\n",
      "Iteration:  10%|█         | 12/120 [00:04<00:38,  2.81it/s]\u001b[A\n",
      "Iteration:  11%|█         | 13/120 [00:04<00:38,  2.81it/s]\u001b[A\n",
      "Iteration:  12%|█▏        | 14/120 [00:05<00:37,  2.81it/s]\u001b[A\n",
      "Iteration:  12%|█▎        | 15/120 [00:05<00:37,  2.81it/s]\u001b[A\n",
      "Iteration:  13%|█▎        | 16/120 [00:05<00:36,  2.81it/s]\u001b[A\n",
      "Iteration:  14%|█▍        | 17/120 [00:06<00:36,  2.81it/s]\u001b[A\n",
      "Iteration:  15%|█▌        | 18/120 [00:06<00:36,  2.81it/s]\u001b[A\n",
      "Iteration:  16%|█▌        | 19/120 [00:06<00:35,  2.81it/s]\u001b[A\n",
      "Iteration:  17%|█▋        | 20/120 [00:07<00:35,  2.82it/s]\u001b[A\n",
      "Iteration:  18%|█▊        | 21/120 [00:07<00:35,  2.81it/s]\u001b[A\n",
      "Iteration:  18%|█▊        | 22/120 [00:07<00:34,  2.81it/s]\u001b[A\n",
      "Iteration:  19%|█▉        | 23/120 [00:08<00:34,  2.81it/s]\u001b[A\n",
      "Iteration:  20%|██        | 24/120 [00:08<00:34,  2.81it/s]\u001b[A\n",
      "Iteration:  21%|██        | 25/120 [00:09<00:33,  2.81it/s]\u001b[A\n",
      "Iteration:  22%|██▏       | 26/120 [00:09<00:33,  2.81it/s]\u001b[A\n",
      "Iteration:  22%|██▎       | 27/120 [00:09<00:33,  2.82it/s]\u001b[A\n",
      "Iteration:  23%|██▎       | 28/120 [00:10<00:32,  2.82it/s]\u001b[A\n",
      "Iteration:  24%|██▍       | 29/120 [00:10<00:32,  2.82it/s]\u001b[A\n",
      "Iteration:  25%|██▌       | 30/120 [00:10<00:32,  2.80it/s]\u001b[A\n",
      "Iteration:  26%|██▌       | 31/120 [00:11<00:31,  2.81it/s]\u001b[A\n",
      "Iteration:  27%|██▋       | 32/120 [00:11<00:31,  2.81it/s]\u001b[A\n",
      "Iteration:  28%|██▊       | 33/120 [00:11<00:30,  2.82it/s]\u001b[A\n",
      "Iteration:  28%|██▊       | 34/120 [00:12<00:30,  2.81it/s]\u001b[A\n",
      "Iteration:  29%|██▉       | 35/120 [00:12<00:30,  2.82it/s]\u001b[A\n",
      "Iteration:  30%|███       | 36/120 [00:12<00:29,  2.83it/s]\u001b[A\n",
      "Iteration:  31%|███       | 37/120 [00:13<00:29,  2.84it/s]\u001b[A\n",
      "Iteration:  32%|███▏      | 38/120 [00:13<00:28,  2.84it/s]\u001b[A\n",
      "Iteration:  32%|███▎      | 39/120 [00:13<00:28,  2.85it/s]\u001b[A\n",
      "Iteration:  33%|███▎      | 40/120 [00:14<00:28,  2.84it/s]\u001b[A\n",
      "Iteration:  34%|███▍      | 41/120 [00:14<00:27,  2.84it/s]\u001b[A\n",
      "Iteration:  35%|███▌      | 42/120 [00:15<00:27,  2.84it/s]\u001b[A\n",
      "Iteration:  36%|███▌      | 43/120 [00:15<00:27,  2.84it/s]\u001b[A\n",
      "Iteration:  37%|███▋      | 44/120 [00:15<00:26,  2.84it/s]\u001b[A\n",
      "Iteration:  38%|███▊      | 45/120 [00:16<00:26,  2.84it/s]\u001b[A\n",
      "Iteration:  38%|███▊      | 46/120 [00:16<00:26,  2.84it/s]\u001b[A\n",
      "Iteration:  39%|███▉      | 47/120 [00:16<00:25,  2.83it/s]\u001b[A\n",
      "Iteration:  40%|████      | 48/120 [00:17<00:25,  2.83it/s]\u001b[A\n",
      "Iteration:  41%|████      | 49/120 [00:17<00:25,  2.84it/s]\u001b[A\n",
      "Iteration:  42%|████▏     | 50/120 [00:17<00:24,  2.84it/s]\u001b[A\n",
      "Iteration:  42%|████▎     | 51/120 [00:18<00:24,  2.83it/s]\u001b[A\n",
      "Iteration:  43%|████▎     | 52/120 [00:18<00:24,  2.83it/s]\u001b[A\n",
      "Iteration:  44%|████▍     | 53/120 [00:18<00:23,  2.83it/s]\u001b[A\n",
      "Iteration:  45%|████▌     | 54/120 [00:19<00:23,  2.80it/s]\u001b[A\n",
      "Iteration:  46%|████▌     | 55/120 [00:19<00:23,  2.81it/s]\u001b[A\n",
      "Iteration:  47%|████▋     | 56/120 [00:19<00:22,  2.82it/s]\u001b[A\n",
      "Iteration:  48%|████▊     | 57/120 [00:20<00:22,  2.82it/s]\u001b[A\n",
      "Iteration:  48%|████▊     | 58/120 [00:20<00:22,  2.75it/s]\u001b[A\n",
      "Iteration:  49%|████▉     | 59/120 [00:21<00:22,  2.77it/s]\u001b[A\n",
      "Iteration:  50%|█████     | 60/120 [00:21<00:22,  2.68it/s]\u001b[A\n",
      "Iteration:  51%|█████     | 61/120 [00:21<00:22,  2.61it/s]\u001b[A\n",
      "Iteration:  52%|█████▏    | 62/120 [00:22<00:21,  2.68it/s]\u001b[A\n",
      "Iteration:  52%|█████▎    | 63/120 [00:22<00:20,  2.73it/s]\u001b[A\n",
      "Iteration:  53%|█████▎    | 64/120 [00:23<00:21,  2.62it/s]\u001b[A\n",
      "Iteration:  54%|█████▍    | 65/120 [00:23<00:21,  2.55it/s]\u001b[A\n",
      "Iteration:  55%|█████▌    | 66/120 [00:23<00:20,  2.61it/s]\u001b[A\n",
      "Iteration:  56%|█████▌    | 67/120 [00:24<00:20,  2.64it/s]\u001b[A\n",
      "Iteration:  57%|█████▋    | 68/120 [00:24<00:19,  2.69it/s]\u001b[A\n",
      "Iteration:  57%|█████▊    | 69/120 [00:24<00:18,  2.74it/s]\u001b[A\n",
      "Iteration:  58%|█████▊    | 70/120 [00:25<00:18,  2.72it/s]\u001b[A\n",
      "Iteration:  59%|█████▉    | 71/120 [00:25<00:18,  2.61it/s]\u001b[A\n",
      "Iteration:  60%|██████    | 72/120 [00:26<00:18,  2.54it/s]\u001b[A\n",
      "Iteration:  61%|██████    | 73/120 [00:26<00:18,  2.60it/s]\u001b[A\n",
      "Iteration:  62%|██████▏   | 74/120 [00:26<00:17,  2.56it/s]\u001b[A\n",
      "Iteration:  62%|██████▎   | 75/120 [00:27<00:18,  2.50it/s]\u001b[A\n",
      "Iteration:  63%|██████▎   | 76/120 [00:27<00:17,  2.57it/s]\u001b[A\n",
      "Iteration:  64%|██████▍   | 77/120 [00:28<00:16,  2.58it/s]\u001b[A\n",
      "Iteration:  65%|██████▌   | 78/120 [00:28<00:16,  2.62it/s]\u001b[A\n",
      "Iteration:  66%|██████▌   | 79/120 [00:28<00:15,  2.66it/s]\u001b[A\n",
      "Iteration:  67%|██████▋   | 80/120 [00:29<00:14,  2.71it/s]\u001b[A\n",
      "Iteration:  68%|██████▊   | 81/120 [00:29<00:14,  2.76it/s]\u001b[A\n",
      "Iteration:  68%|██████▊   | 82/120 [00:29<00:13,  2.75it/s]\u001b[A\n",
      "Iteration:  69%|██████▉   | 83/120 [00:30<00:14,  2.63it/s]\u001b[A\n",
      "Iteration:  70%|███████   | 84/120 [00:30<00:13,  2.64it/s]\u001b[A\n",
      "Iteration:  71%|███████   | 85/120 [00:30<00:13,  2.67it/s]\u001b[A\n",
      "Iteration:  72%|███████▏  | 86/120 [00:31<00:13,  2.61it/s]\u001b[A\n",
      "Iteration:  72%|███████▎  | 87/120 [00:31<00:12,  2.68it/s]\u001b[A\n",
      "Iteration:  73%|███████▎  | 88/120 [00:32<00:11,  2.70it/s]\u001b[A\n",
      "Iteration:  74%|███████▍  | 89/120 [00:32<00:11,  2.74it/s]\u001b[A\n",
      "Iteration:  75%|███████▌  | 90/120 [00:32<00:10,  2.78it/s]\u001b[A\n",
      "Iteration:  76%|███████▌  | 91/120 [00:33<00:10,  2.81it/s]\u001b[A\n",
      "Iteration:  77%|███████▋  | 92/120 [00:33<00:09,  2.83it/s]\u001b[A\n",
      "Iteration:  78%|███████▊  | 93/120 [00:33<00:09,  2.85it/s]\u001b[A\n",
      "Iteration:  78%|███████▊  | 94/120 [00:34<00:09,  2.86it/s]\u001b[A\n",
      "Iteration:  79%|███████▉  | 95/120 [00:34<00:08,  2.87it/s]\u001b[A\n",
      "Iteration:  80%|████████  | 96/120 [00:34<00:08,  2.87it/s]\u001b[A\n",
      "Iteration:  81%|████████  | 97/120 [00:35<00:07,  2.88it/s]\u001b[A\n",
      "Iteration:  82%|████████▏ | 98/120 [00:35<00:07,  2.88it/s]\u001b[A\n",
      "Iteration:  82%|████████▎ | 99/120 [00:35<00:07,  2.88it/s]\u001b[A/home/casapanshop/anaconda2/envs/newpy3/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:224: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "03/05/2020 01:36:04 - INFO - transformers.configuration_utils -   Configuration saved in ../../../../model_save/Dos-Fases-all_Stance/checkpoint-100/config.json\n",
      "03/05/2020 01:36:07 - INFO - transformers.modeling_utils -   Model weights saved in ../../../../model_save/Dos-Fases-all_Stance/checkpoint-100/pytorch_model.bin\n",
      "03/05/2020 01:36:07 - INFO - __main__ -   Saving model checkpoint to ../../../../model_save/Dos-Fases-all_Stance/checkpoint-100\n",
      "03/05/2020 01:36:13 - INFO - __main__ -   Saving optimizer and scheduler states to ../../../../model_save/Dos-Fases-all_Stance/checkpoint-100\n",
      "\n",
      "Iteration:  83%|████████▎ | 100/120 [00:44<00:58,  2.91s/it]\u001b[A\n",
      "Iteration:  84%|████████▍ | 101/120 [00:45<00:40,  2.15s/it]\u001b[A\n",
      "Iteration:  85%|████████▌ | 102/120 [00:45<00:29,  1.61s/it]\u001b[A\n",
      "Iteration:  86%|████████▌ | 103/120 [00:45<00:20,  1.23s/it]\u001b[A\n",
      "Iteration:  87%|████████▋ | 104/120 [00:46<00:15,  1.03it/s]\u001b[A\n",
      "Iteration:  88%|████████▊ | 105/120 [00:46<00:11,  1.28it/s]\u001b[A\n",
      "Iteration:  88%|████████▊ | 106/120 [00:46<00:09,  1.54it/s]\u001b[A\n",
      "Iteration:  89%|████████▉ | 107/120 [00:47<00:07,  1.79it/s]\u001b[A\n",
      "Iteration:  90%|█████████ | 108/120 [00:47<00:05,  2.02it/s]\u001b[A\n",
      "Iteration:  91%|█████████ | 109/120 [00:47<00:04,  2.22it/s]\u001b[A\n",
      "Iteration:  92%|█████████▏| 110/120 [00:48<00:04,  2.38it/s]\u001b[A\n",
      "Iteration:  92%|█████████▎| 111/120 [00:48<00:03,  2.51it/s]\u001b[A\n",
      "Iteration:  93%|█████████▎| 112/120 [00:49<00:03,  2.61it/s]\u001b[A\n",
      "Iteration:  94%|█████████▍| 113/120 [00:49<00:02,  2.69it/s]\u001b[A\n",
      "Iteration:  95%|█████████▌| 114/120 [00:49<00:02,  2.74it/s]\u001b[A\n",
      "Iteration:  96%|█████████▌| 115/120 [00:50<00:01,  2.78it/s]\u001b[A\n",
      "Iteration:  97%|█████████▋| 116/120 [00:50<00:01,  2.81it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:  98%|█████████▊| 117/120 [00:50<00:01,  2.84it/s]\u001b[A\n",
      "Iteration:  98%|█████████▊| 118/120 [00:51<00:00,  2.85it/s]\u001b[A\n",
      "Iteration:  99%|█████████▉| 119/120 [00:51<00:00,  2.85it/s]\u001b[A\n",
      "Iteration: 100%|██████████| 120/120 [00:51<00:00,  2.32it/s]\u001b[A\n",
      "Epoch: 100%|██████████| 1/1 [00:51<00:00, 51.74s/it]\n",
      "03/05/2020 01:36:20 - INFO - __main__ -    global_step = 120, average loss = 3.5634536941846213\n",
      "03/05/2020 01:36:20 - INFO - __main__ -   Saving model checkpoint to ../../../../model_save/Dos-Fases-all_Stance\n",
      "03/05/2020 01:36:20 - INFO - transformers.configuration_utils -   Configuration saved in ../../../../model_save/Dos-Fases-all_Stance/config.json\n",
      "03/05/2020 01:36:22 - INFO - transformers.modeling_utils -   Model weights saved in ../../../../model_save/Dos-Fases-all_Stance/pytorch_model.bin\n",
      "03/05/2020 01:36:22 - INFO - transformers.configuration_utils -   loading configuration file ../../../../model_save/Dos-Fases-all_Stance/config.json\n",
      "03/05/2020 01:36:22 - INFO - transformers.configuration_utils -   Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": null,\n",
      "  \"do_sample\": false,\n",
      "  \"eos_token_ids\": null,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": null,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "03/05/2020 01:36:22 - INFO - transformers.modeling_utils -   loading weights file ../../../../model_save/Dos-Fases-all_Stance/pytorch_model.bin\n",
      "03/05/2020 01:36:24 - INFO - transformers.tokenization_utils -   Model name '../../../../model_save/Dos-Fases-all_Stance' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming '../../../../model_save/Dos-Fases-all_Stance' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "03/05/2020 01:36:24 - INFO - transformers.tokenization_utils -   Didn't find file ../../../../model_save/Dos-Fases-all_Stance/added_tokens.json. We won't load it.\n",
      "03/05/2020 01:36:24 - INFO - transformers.tokenization_utils -   loading file ../../../../model_save/Dos-Fases-all_Stance/vocab.txt\n",
      "03/05/2020 01:36:24 - INFO - transformers.tokenization_utils -   loading file None\n",
      "03/05/2020 01:36:24 - INFO - transformers.tokenization_utils -   loading file ../../../../model_save/Dos-Fases-all_Stance/special_tokens_map.json\n",
      "03/05/2020 01:36:24 - INFO - transformers.tokenization_utils -   loading file ../../../../model_save/Dos-Fases-all_Stance/tokenizer_config.json\n",
      "03/05/2020 01:36:24 - INFO - __main__ -   Evaluate the following checkpoints: ['../../../../model_save/Dos-Fases-all_Stance']\n",
      "03/05/2020 01:36:24 - INFO - transformers.configuration_utils -   loading configuration file ../../../../model_save/Dos-Fases-all_Stance/config.json\n",
      "03/05/2020 01:36:24 - INFO - transformers.configuration_utils -   Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": null,\n",
      "  \"do_sample\": false,\n",
      "  \"eos_token_ids\": null,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": null,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "03/05/2020 01:36:24 - INFO - transformers.modeling_utils -   loading weights file ../../../../model_save/Dos-Fases-all_Stance/pytorch_model.bin\n",
      "03/05/2020 01:36:26 - INFO - __main__ -   ***** Running evaluation  *****\n",
      "03/05/2020 01:36:26 - INFO - __main__ -     Num examples = 457\n",
      "03/05/2020 01:36:26 - INFO - __main__ -     Batch size = 16\n",
      "Evaluating:   7%|▋         | 2/29 [00:00<00:02, 10.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "directory ../../CSV_Stance\n",
      "FILENAME Stance_raw_test.txt\n",
      "args.model_name_or_path bert-base-uncased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 29/29 [00:02<00:00, 10.58it/s]\n",
      "03/05/2020 01:36:29 - INFO - __main__ -   ***** Eval results  *****\n",
      "03/05/2020 01:36:29 - INFO - __main__ -     perplexity = tensor(33.4012)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity epoch: 33.401226\n",
      "\n",
      "Memory usage         :  3.50 MB\n",
      "GC collected objects : 427\n",
      "Memory usage         :  3.50 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/05/2020 01:36:30 - INFO - transformers.tokenization_utils -   Model name '../../../../model_save/Dos-Fases-all_Stance' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming '../../../../model_save/Dos-Fases-all_Stance' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "03/05/2020 01:36:30 - INFO - transformers.tokenization_utils -   Didn't find file ../../../../model_save/Dos-Fases-all_Stance/added_tokens.json. We won't load it.\n",
      "03/05/2020 01:36:30 - INFO - transformers.tokenization_utils -   loading file ../../../../model_save/Dos-Fases-all_Stance/vocab.txt\n",
      "03/05/2020 01:36:30 - INFO - transformers.tokenization_utils -   loading file None\n",
      "03/05/2020 01:36:30 - INFO - transformers.tokenization_utils -   loading file ../../../../model_save/Dos-Fases-all_Stance/special_tokens_map.json\n",
      "03/05/2020 01:36:30 - INFO - transformers.tokenization_utils -   loading file ../../../../model_save/Dos-Fases-all_Stance/tokenizer_config.json\n",
      "03/05/2020 01:36:30 - INFO - transformers.configuration_utils -   loading configuration file ../../../../model_save/Dos-Fases-all_Stance/config.json\n",
      "03/05/2020 01:36:30 - INFO - transformers.configuration_utils -   Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": null,\n",
      "  \"do_sample\": false,\n",
      "  \"eos_token_ids\": null,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 4,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": null,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "03/05/2020 01:36:30 - INFO - transformers.modeling_utils -   loading weights file ../../../../model_save/Dos-Fases-all_Stance/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GC collected objects : 0\n",
      "Memory usage         :  3.50 MB\n",
      "Loading BERT tokenizer...\n",
      "Loading BERT Seq Class...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/05/2020 01:36:31 - INFO - transformers.modeling_utils -   Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n",
      "03/05/2020 01:36:31 - INFO - transformers.modeling_utils -   Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo cargado correctamente desde  ../../../../model_save/Dos-Fases-all_Stance\n",
      "\n",
      "Padding/truncating all sentences to 50 values...\n",
      "\n",
      "Padding token: \"[PAD]\", ID: 0\n",
      "Completado.\n",
      "\n",
      "Padding/truncating all sentences to 50 values...\n",
      "\n",
      "Padding token: \"[PAD]\", ID: 0\n",
      "Completado.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/05/2020 01:37:42 - INFO - transformers.configuration_utils -   Configuration saved in ../../../../model_save/Dos-Fases-all_Stance/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating F-macro...\n",
      "F macro: 0.393\n",
      "F macro none average:  [0.63157895 0.07142857 0.86935581 0.        ]\n",
      "Accuracy: 0.781\n",
      "Saving model to ../../../../model_save/Dos-Fases-all_Stance\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/05/2020 01:37:44 - INFO - transformers.modeling_utils -   Model weights saved in ../../../../model_save/Dos-Fases-all_Stance/pytorch_model.bin\n",
      "03/05/2020 01:37:58 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "03/05/2020 01:37:58 - INFO - transformers.configuration_utils -   loading configuration file ../../../../model_save/Dos-Fases-all_Stance/config.json\n",
      "03/05/2020 01:37:58 - INFO - transformers.configuration_utils -   Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": null,\n",
      "  \"do_sample\": false,\n",
      "  \"eos_token_ids\": null,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 4,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": null,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "03/05/2020 01:37:58 - INFO - transformers.tokenization_utils -   Model name '../../../../model_save/Dos-Fases-all_Stance' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming '../../../../model_save/Dos-Fases-all_Stance' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "03/05/2020 01:37:58 - INFO - transformers.tokenization_utils -   Didn't find file ../../../../model_save/Dos-Fases-all_Stance/added_tokens.json. We won't load it.\n",
      "03/05/2020 01:37:58 - INFO - transformers.tokenization_utils -   loading file ../../../../model_save/Dos-Fases-all_Stance/vocab.txt\n",
      "03/05/2020 01:37:58 - INFO - transformers.tokenization_utils -   loading file None\n",
      "03/05/2020 01:37:58 - INFO - transformers.tokenization_utils -   loading file ../../../../model_save/Dos-Fases-all_Stance/special_tokens_map.json\n",
      "03/05/2020 01:37:58 - INFO - transformers.tokenization_utils -   loading file ../../../../model_save/Dos-Fases-all_Stance/tokenizer_config.json\n",
      "03/05/2020 01:37:58 - INFO - transformers.modeling_utils -   loading weights file ../../../../model_save/Dos-Fases-all_Stance/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage         :  3.50 MB\n",
      "GC collected objects : 427\n",
      "Memory usage         :  3.50 MB\n",
      "GC collected objects : 0\n",
      "Memory usage         :  3.50 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/05/2020 01:38:00 - INFO - transformers.modeling_utils -   Weights of BertForMaskedLM not initialized from pretrained model: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']\n",
      "03/05/2020 01:38:00 - INFO - transformers.modeling_utils -   Weights from pretrained model not used in BertForMaskedLM: ['classifier.weight', 'classifier.bias']\n",
      "03/05/2020 01:38:00 - INFO - __main__ -   Training/evaluation parameters <__main__.Args_LM object at 0x7f274a67cd30>\n",
      "03/05/2020 01:38:00 - INFO - __main__ -   ***** Running training *****\n",
      "03/05/2020 01:38:00 - INFO - __main__ -     Num examples = 1916\n",
      "03/05/2020 01:38:00 - INFO - __main__ -     Num Epochs = 1\n",
      "03/05/2020 01:38:00 - INFO - __main__ -     Instantaneous batch size per GPU = 16\n",
      "03/05/2020 01:38:00 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "03/05/2020 01:38:00 - INFO - __main__ -     Gradient Accumulation steps = 1\n",
      "03/05/2020 01:38:00 - INFO - __main__ -     Total optimization steps = 120\n",
      "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Iteration:   0%|          | 0/120 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "directory ../../CSV_Stance\n",
      "FILENAME Stance_raw_train.txt\n",
      "args.model_name_or_path ../../../../model_save/Dos-Fases-all_Stance\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration:   1%|          | 1/120 [00:00<00:50,  2.35it/s]\u001b[A\n",
      "Iteration:   2%|▏         | 2/120 [00:00<00:49,  2.39it/s]\u001b[A\n",
      "Iteration:   2%|▎         | 3/120 [00:01<00:48,  2.39it/s]\u001b[A\n",
      "Iteration:   3%|▎         | 4/120 [00:01<00:48,  2.38it/s]\u001b[A\n",
      "Iteration:   4%|▍         | 5/120 [00:02<00:48,  2.39it/s]\u001b[A\n",
      "Iteration:   5%|▌         | 6/120 [00:02<00:47,  2.40it/s]\u001b[A\n",
      "Iteration:   6%|▌         | 7/120 [00:02<00:46,  2.41it/s]\u001b[A\n",
      "Iteration:   7%|▋         | 8/120 [00:03<00:46,  2.41it/s]\u001b[A\n",
      "Iteration:   8%|▊         | 9/120 [00:03<00:45,  2.42it/s]\u001b[A\n",
      "Iteration:   8%|▊         | 10/120 [00:04<00:45,  2.44it/s]\u001b[A\n",
      "Iteration:   9%|▉         | 11/120 [00:04<00:44,  2.47it/s]\u001b[A\n",
      "Iteration:  10%|█         | 12/120 [00:04<00:43,  2.46it/s]\u001b[A\n",
      "Iteration:  11%|█         | 13/120 [00:05<00:43,  2.46it/s]\u001b[A\n",
      "Iteration:  12%|█▏        | 14/120 [00:05<00:43,  2.45it/s]\u001b[A\n",
      "Iteration:  12%|█▎        | 15/120 [00:06<00:42,  2.46it/s]\u001b[A\n",
      "Iteration:  13%|█▎        | 16/120 [00:06<00:42,  2.45it/s]\u001b[A\n",
      "Iteration:  14%|█▍        | 17/120 [00:06<00:41,  2.49it/s]\u001b[A\n",
      "Iteration:  15%|█▌        | 18/120 [00:07<00:41,  2.46it/s]\u001b[A\n",
      "Iteration:  16%|█▌        | 19/120 [00:07<00:41,  2.46it/s]\u001b[A\n",
      "Iteration:  17%|█▋        | 20/120 [00:08<00:40,  2.49it/s]\u001b[A\n",
      "Iteration:  18%|█▊        | 21/120 [00:08<00:39,  2.51it/s]\u001b[A\n",
      "Iteration:  18%|█▊        | 22/120 [00:08<00:38,  2.52it/s]\u001b[A\n",
      "Iteration:  19%|█▉        | 23/120 [00:09<00:38,  2.53it/s]\u001b[A\n",
      "Iteration:  20%|██        | 24/120 [00:09<00:38,  2.50it/s]\u001b[A\n",
      "Iteration:  21%|██        | 25/120 [00:10<00:37,  2.52it/s]\u001b[A\n",
      "Iteration:  22%|██▏       | 26/120 [00:10<00:37,  2.53it/s]\u001b[A\n",
      "Iteration:  22%|██▎       | 27/120 [00:10<00:36,  2.54it/s]\u001b[A\n",
      "Iteration:  23%|██▎       | 28/120 [00:11<00:36,  2.55it/s]\u001b[A\n",
      "Iteration:  24%|██▍       | 29/120 [00:11<00:35,  2.56it/s]\u001b[A\n",
      "Iteration:  25%|██▌       | 30/120 [00:12<00:35,  2.55it/s]\u001b[A\n",
      "Iteration:  26%|██▌       | 31/120 [00:12<00:35,  2.54it/s]\u001b[A\n",
      "Iteration:  27%|██▋       | 32/120 [00:12<00:34,  2.53it/s]\u001b[A\n",
      "Iteration:  28%|██▊       | 33/120 [00:13<00:34,  2.53it/s]\u001b[A\n",
      "Iteration:  28%|██▊       | 34/120 [00:13<00:33,  2.53it/s]\u001b[A\n",
      "Iteration:  29%|██▉       | 35/120 [00:14<00:33,  2.53it/s]\u001b[A\n",
      "Iteration:  30%|███       | 36/120 [00:14<00:33,  2.51it/s]\u001b[A\n",
      "Iteration:  31%|███       | 37/120 [00:14<00:33,  2.50it/s]\u001b[A\n",
      "Iteration:  32%|███▏      | 38/120 [00:15<00:32,  2.51it/s]\u001b[A\n",
      "Iteration:  32%|███▎      | 39/120 [00:15<00:32,  2.52it/s]\u001b[A\n",
      "Iteration:  33%|███▎      | 40/120 [00:16<00:31,  2.53it/s]\u001b[A\n",
      "Iteration:  34%|███▍      | 41/120 [00:16<00:31,  2.51it/s]\u001b[A\n",
      "Iteration:  35%|███▌      | 42/120 [00:16<00:30,  2.53it/s]\u001b[A\n",
      "Iteration:  36%|███▌      | 43/120 [00:17<00:30,  2.53it/s]\u001b[A\n",
      "Iteration:  37%|███▋      | 44/120 [00:17<00:30,  2.52it/s]\u001b[A\n",
      "Iteration:  38%|███▊      | 45/120 [00:18<00:29,  2.54it/s]\u001b[A\n",
      "Iteration:  38%|███▊      | 46/120 [00:18<00:29,  2.55it/s]\u001b[A\n",
      "Iteration:  39%|███▉      | 47/120 [00:18<00:28,  2.55it/s]\u001b[A\n",
      "Iteration:  40%|████      | 48/120 [00:19<00:28,  2.56it/s]\u001b[A\n",
      "Iteration:  41%|████      | 49/120 [00:19<00:27,  2.56it/s]\u001b[A\n",
      "Iteration:  42%|████▏     | 50/120 [00:19<00:27,  2.57it/s]\u001b[A\n",
      "Iteration:  42%|████▎     | 51/120 [00:20<00:26,  2.57it/s]\u001b[A\n",
      "Iteration:  43%|████▎     | 52/120 [00:20<00:26,  2.56it/s]\u001b[A\n",
      "Iteration:  44%|████▍     | 53/120 [00:21<00:26,  2.54it/s]\u001b[A\n",
      "Iteration:  45%|████▌     | 54/120 [00:21<00:26,  2.53it/s]\u001b[A\n",
      "Iteration:  46%|████▌     | 55/120 [00:21<00:25,  2.54it/s]\u001b[A\n",
      "Iteration:  47%|████▋     | 56/120 [00:22<00:25,  2.52it/s]\u001b[A\n",
      "Iteration:  48%|████▊     | 57/120 [00:22<00:25,  2.52it/s]\u001b[A\n",
      "Iteration:  48%|████▊     | 58/120 [00:23<00:24,  2.52it/s]\u001b[A\n",
      "Iteration:  49%|████▉     | 59/120 [00:23<00:23,  2.60it/s]\u001b[A\n",
      "Iteration:  50%|█████     | 60/120 [00:23<00:22,  2.67it/s]\u001b[A\n",
      "Iteration:  51%|█████     | 61/120 [00:24<00:21,  2.73it/s]\u001b[A\n",
      "Iteration:  52%|█████▏    | 62/120 [00:24<00:20,  2.77it/s]\u001b[A\n",
      "Iteration:  52%|█████▎    | 63/120 [00:24<00:20,  2.80it/s]\u001b[A\n",
      "Iteration:  53%|█████▎    | 64/120 [00:25<00:19,  2.81it/s]\u001b[A\n",
      "Iteration:  54%|█████▍    | 65/120 [00:25<00:19,  2.78it/s]\u001b[A\n",
      "Iteration:  55%|█████▌    | 66/120 [00:25<00:19,  2.81it/s]\u001b[A\n",
      "Iteration:  56%|█████▌    | 67/120 [00:26<00:18,  2.83it/s]\u001b[A\n",
      "Iteration:  57%|█████▋    | 68/120 [00:26<00:18,  2.83it/s]\u001b[A\n",
      "Iteration:  57%|█████▊    | 69/120 [00:27<00:18,  2.81it/s]\u001b[A\n",
      "Iteration:  58%|█████▊    | 70/120 [00:27<00:17,  2.83it/s]\u001b[A\n",
      "Iteration:  59%|█████▉    | 71/120 [00:27<00:17,  2.84it/s]\u001b[A\n",
      "Iteration:  60%|██████    | 72/120 [00:28<00:17,  2.79it/s]\u001b[A\n",
      "Iteration:  61%|██████    | 73/120 [00:28<00:17,  2.63it/s]\u001b[A\n",
      "Iteration:  62%|██████▏   | 74/120 [00:28<00:17,  2.58it/s]\u001b[A\n",
      "Iteration:  62%|██████▎   | 75/120 [00:29<00:17,  2.57it/s]\u001b[A\n",
      "Iteration:  63%|██████▎   | 76/120 [00:29<00:16,  2.60it/s]\u001b[A\n",
      "Iteration:  64%|██████▍   | 77/120 [00:30<00:16,  2.54it/s]\u001b[A\n",
      "Iteration:  65%|██████▌   | 78/120 [00:30<00:15,  2.63it/s]\u001b[A\n",
      "Iteration:  66%|██████▌   | 79/120 [00:30<00:16,  2.55it/s]\u001b[A\n",
      "Iteration:  67%|██████▋   | 80/120 [00:31<00:15,  2.62it/s]\u001b[A\n",
      "Iteration:  68%|██████▊   | 81/120 [00:31<00:14,  2.63it/s]\u001b[A\n",
      "Iteration:  68%|██████▊   | 82/120 [00:32<00:14,  2.56it/s]\u001b[A\n",
      "Iteration:  69%|██████▉   | 83/120 [00:32<00:14,  2.48it/s]\u001b[A\n",
      "Iteration:  70%|███████   | 84/120 [00:32<00:14,  2.51it/s]\u001b[A\n",
      "Iteration:  71%|███████   | 85/120 [00:33<00:13,  2.51it/s]\u001b[A\n",
      "Iteration:  72%|███████▏  | 86/120 [00:33<00:13,  2.54it/s]\u001b[A\n",
      "Iteration:  72%|███████▎  | 87/120 [00:34<00:12,  2.59it/s]\u001b[A\n",
      "Iteration:  73%|███████▎  | 88/120 [00:34<00:12,  2.62it/s]\u001b[A\n",
      "Iteration:  74%|███████▍  | 89/120 [00:34<00:11,  2.61it/s]\u001b[A\n",
      "Iteration:  75%|███████▌  | 90/120 [00:35<00:11,  2.65it/s]\u001b[A\n",
      "Iteration:  76%|███████▌  | 91/120 [00:35<00:10,  2.69it/s]\u001b[A\n",
      "Iteration:  77%|███████▋  | 92/120 [00:35<00:10,  2.70it/s]\u001b[A\n",
      "Iteration:  78%|███████▊  | 93/120 [00:36<00:09,  2.72it/s]\u001b[A\n",
      "Iteration:  78%|███████▊  | 94/120 [00:36<00:09,  2.74it/s]\u001b[A\n",
      "Iteration:  79%|███████▉  | 95/120 [00:36<00:09,  2.72it/s]\u001b[A\n",
      "Iteration:  80%|████████  | 96/120 [00:37<00:08,  2.77it/s]\u001b[A\n",
      "Iteration:  81%|████████  | 97/120 [00:37<00:08,  2.78it/s]\u001b[A\n",
      "Iteration:  82%|████████▏ | 98/120 [00:38<00:07,  2.78it/s]\u001b[A\n",
      "Iteration:  82%|████████▎ | 99/120 [00:38<00:07,  2.76it/s]\u001b[A03/05/2020 01:38:38 - INFO - transformers.configuration_utils -   Configuration saved in ../../../../model_save/Dos-Fases-all_Stance/checkpoint-100/config.json\n",
      "03/05/2020 01:38:41 - INFO - transformers.modeling_utils -   Model weights saved in ../../../../model_save/Dos-Fases-all_Stance/checkpoint-100/pytorch_model.bin\n",
      "03/05/2020 01:38:41 - INFO - __main__ -   Saving model checkpoint to ../../../../model_save/Dos-Fases-all_Stance/checkpoint-100\n",
      "03/05/2020 01:38:49 - INFO - __main__ -   Saving optimizer and scheduler states to ../../../../model_save/Dos-Fases-all_Stance/checkpoint-100\n",
      "\n",
      "Iteration:  83%|████████▎ | 100/120 [00:49<01:11,  3.57s/it]\u001b[A\n",
      "Iteration:  84%|████████▍ | 101/120 [00:49<00:49,  2.62s/it]\u001b[A\n",
      "Iteration:  85%|████████▌ | 102/120 [00:50<00:35,  1.95s/it]\u001b[A\n",
      "Iteration:  86%|████████▌ | 103/120 [00:50<00:25,  1.48s/it]\u001b[A\n",
      "Iteration:  87%|████████▋ | 104/120 [00:50<00:18,  1.15s/it]\u001b[A\n",
      "Iteration:  88%|████████▊ | 105/120 [00:51<00:13,  1.10it/s]\u001b[A\n",
      "Iteration:  88%|████████▊ | 106/120 [00:51<00:10,  1.35it/s]\u001b[A\n",
      "Iteration:  89%|████████▉ | 107/120 [00:52<00:08,  1.61it/s]\u001b[A\n",
      "Iteration:  90%|█████████ | 108/120 [00:52<00:06,  1.85it/s]\u001b[A\n",
      "Iteration:  91%|█████████ | 109/120 [00:52<00:05,  2.08it/s]\u001b[A\n",
      "Iteration:  92%|█████████▏| 110/120 [00:53<00:04,  2.27it/s]\u001b[A\n",
      "Iteration:  92%|█████████▎| 111/120 [00:53<00:03,  2.35it/s]\u001b[A\n",
      "Iteration:  93%|█████████▎| 112/120 [00:53<00:03,  2.44it/s]\u001b[A\n",
      "Iteration:  94%|█████████▍| 113/120 [00:54<00:02,  2.56it/s]\u001b[A\n",
      "Iteration:  95%|█████████▌| 114/120 [00:54<00:02,  2.64it/s]\u001b[A\n",
      "Iteration:  96%|█████████▌| 115/120 [00:54<00:01,  2.58it/s]\u001b[A\n",
      "Iteration:  97%|█████████▋| 116/120 [00:55<00:01,  2.66it/s]\u001b[A\n",
      "Iteration:  98%|█████████▊| 117/120 [00:55<00:01,  2.72it/s]\u001b[A\n",
      "Iteration:  98%|█████████▊| 118/120 [00:56<00:00,  2.64it/s]\u001b[A\n",
      "Iteration:  99%|█████████▉| 119/120 [00:56<00:00,  2.69it/s]\u001b[A\n",
      "Iteration: 100%|██████████| 120/120 [00:56<00:00,  2.12it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|██████████| 1/1 [00:56<00:00, 56.70s/it]\n",
      "03/05/2020 01:38:56 - INFO - __main__ -    global_step = 120, average loss = 4.702169469992319\n",
      "03/05/2020 01:38:56 - INFO - __main__ -   Saving model checkpoint to ../../../../model_save/Dos-Fases-all_Stance\n",
      "03/05/2020 01:38:56 - INFO - transformers.configuration_utils -   Configuration saved in ../../../../model_save/Dos-Fases-all_Stance/config.json\n",
      "03/05/2020 01:38:59 - INFO - transformers.modeling_utils -   Model weights saved in ../../../../model_save/Dos-Fases-all_Stance/pytorch_model.bin\n",
      "03/05/2020 01:38:59 - INFO - transformers.configuration_utils -   loading configuration file ../../../../model_save/Dos-Fases-all_Stance/config.json\n",
      "03/05/2020 01:38:59 - INFO - transformers.configuration_utils -   Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": null,\n",
      "  \"do_sample\": false,\n",
      "  \"eos_token_ids\": null,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 4,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": null,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "03/05/2020 01:38:59 - INFO - transformers.modeling_utils -   loading weights file ../../../../model_save/Dos-Fases-all_Stance/pytorch_model.bin\n",
      "03/05/2020 01:39:01 - INFO - transformers.tokenization_utils -   Model name '../../../../model_save/Dos-Fases-all_Stance' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming '../../../../model_save/Dos-Fases-all_Stance' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "03/05/2020 01:39:01 - INFO - transformers.tokenization_utils -   Didn't find file ../../../../model_save/Dos-Fases-all_Stance/added_tokens.json. We won't load it.\n",
      "03/05/2020 01:39:01 - INFO - transformers.tokenization_utils -   loading file ../../../../model_save/Dos-Fases-all_Stance/vocab.txt\n",
      "03/05/2020 01:39:01 - INFO - transformers.tokenization_utils -   loading file None\n",
      "03/05/2020 01:39:01 - INFO - transformers.tokenization_utils -   loading file ../../../../model_save/Dos-Fases-all_Stance/special_tokens_map.json\n",
      "03/05/2020 01:39:01 - INFO - transformers.tokenization_utils -   loading file ../../../../model_save/Dos-Fases-all_Stance/tokenizer_config.json\n",
      "03/05/2020 01:39:01 - INFO - __main__ -   Evaluate the following checkpoints: ['../../../../model_save/Dos-Fases-all_Stance']\n",
      "03/05/2020 01:39:01 - INFO - transformers.configuration_utils -   loading configuration file ../../../../model_save/Dos-Fases-all_Stance/config.json\n",
      "03/05/2020 01:39:01 - INFO - transformers.configuration_utils -   Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": null,\n",
      "  \"do_sample\": false,\n",
      "  \"eos_token_ids\": null,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 4,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": null,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "03/05/2020 01:39:01 - INFO - transformers.modeling_utils -   loading weights file ../../../../model_save/Dos-Fases-all_Stance/pytorch_model.bin\n",
      "03/05/2020 01:39:03 - INFO - __main__ -   ***** Running evaluation  *****\n",
      "03/05/2020 01:39:03 - INFO - __main__ -     Num examples = 457\n",
      "03/05/2020 01:39:03 - INFO - __main__ -     Batch size = 16\n",
      "Evaluating:   7%|▋         | 2/29 [00:00<00:02, 10.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "directory ../../CSV_Stance\n",
      "FILENAME Stance_raw_test.txt\n",
      "args.model_name_or_path ../../../../model_save/Dos-Fases-all_Stance\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 29/29 [00:02<00:00,  9.76it/s]\n",
      "03/05/2020 01:39:06 - INFO - __main__ -   ***** Eval results  *****\n",
      "03/05/2020 01:39:06 - INFO - __main__ -     perplexity = tensor(88.4203)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity epoch: 88.42033\n",
      "\n",
      "Memory usage         :  3.70 MB\n",
      "GC collected objects : 844\n",
      "Memory usage         :  3.70 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/05/2020 01:39:06 - INFO - transformers.tokenization_utils -   Model name '../../../../model_save/Dos-Fases-all_Stance' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming '../../../../model_save/Dos-Fases-all_Stance' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "03/05/2020 01:39:06 - INFO - transformers.tokenization_utils -   Didn't find file ../../../../model_save/Dos-Fases-all_Stance/added_tokens.json. We won't load it.\n",
      "03/05/2020 01:39:06 - INFO - transformers.tokenization_utils -   loading file ../../../../model_save/Dos-Fases-all_Stance/vocab.txt\n",
      "03/05/2020 01:39:06 - INFO - transformers.tokenization_utils -   loading file None\n",
      "03/05/2020 01:39:06 - INFO - transformers.tokenization_utils -   loading file ../../../../model_save/Dos-Fases-all_Stance/special_tokens_map.json\n",
      "03/05/2020 01:39:06 - INFO - transformers.tokenization_utils -   loading file ../../../../model_save/Dos-Fases-all_Stance/tokenizer_config.json\n",
      "03/05/2020 01:39:06 - INFO - transformers.configuration_utils -   loading configuration file ../../../../model_save/Dos-Fases-all_Stance/config.json\n",
      "03/05/2020 01:39:06 - INFO - transformers.configuration_utils -   Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": null,\n",
      "  \"do_sample\": false,\n",
      "  \"eos_token_ids\": null,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 4,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": null,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "03/05/2020 01:39:06 - INFO - transformers.modeling_utils -   loading weights file ../../../../model_save/Dos-Fases-all_Stance/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GC collected objects : 0\n",
      "Memory usage         :  3.70 MB\n",
      "Loading BERT tokenizer...\n",
      "Loading BERT Seq Class...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/05/2020 01:39:08 - INFO - transformers.modeling_utils -   Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n",
      "03/05/2020 01:39:08 - INFO - transformers.modeling_utils -   Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo cargado correctamente desde  ../../../../model_save/Dos-Fases-all_Stance\n",
      "\n",
      "Padding/truncating all sentences to 50 values...\n",
      "\n",
      "Padding token: \"[PAD]\", ID: 0\n",
      "Completado.\n",
      "\n",
      "Padding/truncating all sentences to 50 values...\n",
      "\n",
      "Padding token: \"[PAD]\", ID: 0\n",
      "Completado.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/05/2020 01:40:18 - INFO - transformers.configuration_utils -   Configuration saved in ../../../../model_save/Dos-Fases-all_Stance/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating F-macro...\n",
      "F macro: 0.398\n",
      "F macro none average:  [0.64788732 0.06976744 0.87335723 0.        ]\n",
      "Accuracy: 0.786\n",
      "Saving model to ../../../../model_save/Dos-Fases-all_Stance\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/05/2020 01:40:21 - INFO - transformers.modeling_utils -   Model weights saved in ../../../../model_save/Dos-Fases-all_Stance/pytorch_model.bin\n",
      "03/05/2020 01:40:33 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "03/05/2020 01:40:33 - INFO - transformers.configuration_utils -   loading configuration file ../../../../model_save/Dos-Fases-all_Stance/config.json\n",
      "03/05/2020 01:40:33 - INFO - transformers.configuration_utils -   Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": null,\n",
      "  \"do_sample\": false,\n",
      "  \"eos_token_ids\": null,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 4,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": null,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "03/05/2020 01:40:33 - INFO - transformers.tokenization_utils -   Model name '../../../../model_save/Dos-Fases-all_Stance' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming '../../../../model_save/Dos-Fases-all_Stance' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "03/05/2020 01:40:33 - INFO - transformers.tokenization_utils -   Didn't find file ../../../../model_save/Dos-Fases-all_Stance/added_tokens.json. We won't load it.\n",
      "03/05/2020 01:40:33 - INFO - transformers.tokenization_utils -   loading file ../../../../model_save/Dos-Fases-all_Stance/vocab.txt\n",
      "03/05/2020 01:40:33 - INFO - transformers.tokenization_utils -   loading file None\n",
      "03/05/2020 01:40:33 - INFO - transformers.tokenization_utils -   loading file ../../../../model_save/Dos-Fases-all_Stance/special_tokens_map.json\n",
      "03/05/2020 01:40:33 - INFO - transformers.tokenization_utils -   loading file ../../../../model_save/Dos-Fases-all_Stance/tokenizer_config.json\n",
      "03/05/2020 01:40:33 - INFO - transformers.modeling_utils -   loading weights file ../../../../model_save/Dos-Fases-all_Stance/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage         :  3.70 MB\n",
      "GC collected objects : 427\n",
      "Memory usage         :  3.70 MB\n",
      "GC collected objects : 0\n",
      "Memory usage         :  3.70 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/05/2020 01:40:35 - INFO - transformers.modeling_utils -   Weights of BertForMaskedLM not initialized from pretrained model: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']\n",
      "03/05/2020 01:40:35 - INFO - transformers.modeling_utils -   Weights from pretrained model not used in BertForMaskedLM: ['classifier.weight', 'classifier.bias']\n",
      "03/05/2020 01:40:35 - INFO - __main__ -   Training/evaluation parameters <__main__.Args_LM object at 0x7f274a67cd30>\n",
      "03/05/2020 01:40:35 - INFO - __main__ -   ***** Running training *****\n",
      "03/05/2020 01:40:35 - INFO - __main__ -     Num examples = 1916\n",
      "03/05/2020 01:40:35 - INFO - __main__ -     Num Epochs = 1\n",
      "03/05/2020 01:40:35 - INFO - __main__ -     Instantaneous batch size per GPU = 16\n",
      "03/05/2020 01:40:35 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "03/05/2020 01:40:35 - INFO - __main__ -     Gradient Accumulation steps = 1\n",
      "03/05/2020 01:40:35 - INFO - __main__ -     Total optimization steps = 120\n",
      "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Iteration:   0%|          | 0/120 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "directory ../../CSV_Stance\n",
      "FILENAME Stance_raw_train.txt\n",
      "args.model_name_or_path ../../../../model_save/Dos-Fases-all_Stance\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration:   1%|          | 1/120 [00:00<00:42,  2.79it/s]\u001b[A\n",
      "Iteration:   2%|▏         | 2/120 [00:00<00:41,  2.81it/s]\u001b[A\n",
      "Iteration:   2%|▎         | 3/120 [00:01<00:41,  2.83it/s]\u001b[A\n",
      "Iteration:   3%|▎         | 4/120 [00:01<00:40,  2.85it/s]\u001b[A\n",
      "Iteration:   4%|▍         | 5/120 [00:01<00:40,  2.86it/s]\u001b[A\n",
      "Iteration:   5%|▌         | 6/120 [00:02<00:39,  2.87it/s]\u001b[A\n",
      "Iteration:   6%|▌         | 7/120 [00:02<00:39,  2.87it/s]\u001b[A\n",
      "Iteration:   7%|▋         | 8/120 [00:02<00:38,  2.88it/s]\u001b[A\n",
      "Iteration:   8%|▊         | 9/120 [00:03<00:38,  2.87it/s]\u001b[A\n",
      "Iteration:   8%|▊         | 10/120 [00:03<00:38,  2.87it/s]\u001b[A\n",
      "Iteration:   9%|▉         | 11/120 [00:03<00:37,  2.87it/s]\u001b[A\n",
      "Iteration:  10%|█         | 12/120 [00:04<00:37,  2.88it/s]\u001b[A\n",
      "Iteration:  11%|█         | 13/120 [00:04<00:37,  2.87it/s]\u001b[A\n",
      "Iteration:  12%|█▏        | 14/120 [00:04<00:36,  2.87it/s]\u001b[A\n",
      "Iteration:  12%|█▎        | 15/120 [00:05<00:36,  2.88it/s]\u001b[A\n",
      "Iteration:  13%|█▎        | 16/120 [00:05<00:36,  2.87it/s]\u001b[A\n",
      "Iteration:  14%|█▍        | 17/120 [00:05<00:35,  2.87it/s]\u001b[A\n",
      "Iteration:  15%|█▌        | 18/120 [00:06<00:35,  2.88it/s]\u001b[A\n",
      "Iteration:  16%|█▌        | 19/120 [00:06<00:35,  2.88it/s]\u001b[A\n",
      "Iteration:  17%|█▋        | 20/120 [00:06<00:34,  2.88it/s]\u001b[A\n",
      "Iteration:  18%|█▊        | 21/120 [00:07<00:34,  2.88it/s]\u001b[A\n",
      "Iteration:  18%|█▊        | 22/120 [00:07<00:33,  2.89it/s]\u001b[A\n",
      "Iteration:  19%|█▉        | 23/120 [00:07<00:33,  2.89it/s]\u001b[A\n",
      "Iteration:  20%|██        | 24/120 [00:08<00:33,  2.88it/s]\u001b[A\n",
      "Iteration:  21%|██        | 25/120 [00:08<00:32,  2.88it/s]\u001b[A\n",
      "Iteration:  22%|██▏       | 26/120 [00:09<00:32,  2.88it/s]\u001b[A\n",
      "Iteration:  22%|██▎       | 27/120 [00:09<00:32,  2.88it/s]\u001b[A\n",
      "Iteration:  23%|██▎       | 28/120 [00:09<00:31,  2.88it/s]\u001b[A\n",
      "Iteration:  24%|██▍       | 29/120 [00:10<00:31,  2.88it/s]\u001b[A\n",
      "Iteration:  25%|██▌       | 30/120 [00:10<00:31,  2.88it/s]\u001b[A\n",
      "Iteration:  26%|██▌       | 31/120 [00:10<00:30,  2.88it/s]\u001b[A\n",
      "Iteration:  27%|██▋       | 32/120 [00:11<00:30,  2.89it/s]\u001b[A\n",
      "Iteration:  28%|██▊       | 33/120 [00:11<00:30,  2.88it/s]\u001b[A\n",
      "Iteration:  28%|██▊       | 34/120 [00:11<00:29,  2.89it/s]\u001b[A\n",
      "Iteration:  29%|██▉       | 35/120 [00:12<00:29,  2.88it/s]\u001b[A\n",
      "Iteration:  30%|███       | 36/120 [00:12<00:29,  2.89it/s]\u001b[A\n",
      "Iteration:  31%|███       | 37/120 [00:12<00:28,  2.88it/s]\u001b[A\n",
      "Iteration:  32%|███▏      | 38/120 [00:13<00:28,  2.87it/s]\u001b[A\n",
      "Iteration:  32%|███▎      | 39/120 [00:13<00:28,  2.87it/s]\u001b[A\n",
      "Iteration:  33%|███▎      | 40/120 [00:13<00:27,  2.87it/s]\u001b[A\n",
      "Iteration:  34%|███▍      | 41/120 [00:14<00:27,  2.87it/s]\u001b[A\n",
      "Iteration:  35%|███▌      | 42/120 [00:14<00:27,  2.87it/s]\u001b[A\n",
      "Iteration:  36%|███▌      | 43/120 [00:14<00:26,  2.88it/s]\u001b[A\n",
      "Iteration:  37%|███▋      | 44/120 [00:15<00:26,  2.88it/s]\u001b[A\n",
      "Iteration:  38%|███▊      | 45/120 [00:15<00:26,  2.88it/s]\u001b[A\n",
      "Iteration:  38%|███▊      | 46/120 [00:15<00:25,  2.87it/s]\u001b[A\n",
      "Iteration:  39%|███▉      | 47/120 [00:16<00:25,  2.87it/s]\u001b[A\n",
      "Iteration:  40%|████      | 48/120 [00:16<00:25,  2.87it/s]\u001b[A\n",
      "Iteration:  41%|████      | 49/120 [00:17<00:24,  2.87it/s]\u001b[A\n",
      "Iteration:  42%|████▏     | 50/120 [00:17<00:24,  2.87it/s]\u001b[A\n",
      "Iteration:  42%|████▎     | 51/120 [00:17<00:24,  2.87it/s]\u001b[A\n",
      "Iteration:  43%|████▎     | 52/120 [00:18<00:23,  2.87it/s]\u001b[A\n",
      "Iteration:  44%|████▍     | 53/120 [00:18<00:23,  2.87it/s]\u001b[A\n",
      "Iteration:  45%|████▌     | 54/120 [00:18<00:22,  2.87it/s]\u001b[A\n",
      "Iteration:  46%|████▌     | 55/120 [00:19<00:22,  2.88it/s]\u001b[A\n",
      "Iteration:  47%|████▋     | 56/120 [00:19<00:22,  2.87it/s]\u001b[A\n",
      "Iteration:  48%|████▊     | 57/120 [00:19<00:21,  2.88it/s]\u001b[A\n",
      "Iteration:  48%|████▊     | 58/120 [00:20<00:21,  2.87it/s]\u001b[A\n",
      "Iteration:  49%|████▉     | 59/120 [00:20<00:21,  2.87it/s]\u001b[A\n",
      "Iteration:  50%|█████     | 60/120 [00:20<00:20,  2.87it/s]\u001b[A\n",
      "Iteration:  51%|█████     | 61/120 [00:21<00:20,  2.87it/s]\u001b[A\n",
      "Iteration:  52%|█████▏    | 62/120 [00:21<00:20,  2.85it/s]\u001b[A\n",
      "Iteration:  52%|█████▎    | 63/120 [00:21<00:19,  2.85it/s]\u001b[A\n",
      "Iteration:  53%|█████▎    | 64/120 [00:22<00:19,  2.86it/s]\u001b[A\n",
      "Iteration:  54%|█████▍    | 65/120 [00:22<00:19,  2.86it/s]\u001b[A\n",
      "Iteration:  55%|█████▌    | 66/120 [00:22<00:18,  2.86it/s]\u001b[A\n",
      "Iteration:  56%|█████▌    | 67/120 [00:23<00:18,  2.87it/s]\u001b[A\n",
      "Iteration:  57%|█████▋    | 68/120 [00:23<00:18,  2.86it/s]\u001b[A\n",
      "Iteration:  57%|█████▊    | 69/120 [00:24<00:17,  2.87it/s]\u001b[A\n",
      "Iteration:  58%|█████▊    | 70/120 [00:24<00:17,  2.87it/s]\u001b[A\n",
      "Iteration:  59%|█████▉    | 71/120 [00:24<00:17,  2.87it/s]\u001b[A\n",
      "Iteration:  60%|██████    | 72/120 [00:25<00:16,  2.87it/s]\u001b[A\n",
      "Iteration:  61%|██████    | 73/120 [00:25<00:16,  2.87it/s]\u001b[A\n",
      "Iteration:  62%|██████▏   | 74/120 [00:25<00:16,  2.86it/s]\u001b[A\n",
      "Iteration:  62%|██████▎   | 75/120 [00:26<00:15,  2.86it/s]\u001b[A\n",
      "Iteration:  63%|██████▎   | 76/120 [00:26<00:15,  2.86it/s]\u001b[A\n",
      "Iteration:  64%|██████▍   | 77/120 [00:26<00:15,  2.86it/s]\u001b[A\n",
      "Iteration:  65%|██████▌   | 78/120 [00:27<00:14,  2.87it/s]\u001b[A\n",
      "Iteration:  66%|██████▌   | 79/120 [00:27<00:14,  2.86it/s]\u001b[A\n",
      "Iteration:  67%|██████▋   | 80/120 [00:27<00:13,  2.87it/s]\u001b[A\n",
      "Iteration:  68%|██████▊   | 81/120 [00:28<00:13,  2.85it/s]\u001b[A\n",
      "Iteration:  68%|██████▊   | 82/120 [00:28<00:13,  2.85it/s]\u001b[A\n",
      "Iteration:  69%|██████▉   | 83/120 [00:28<00:12,  2.85it/s]\u001b[A\n",
      "Iteration:  70%|███████   | 84/120 [00:29<00:12,  2.86it/s]\u001b[A\n",
      "Iteration:  71%|███████   | 85/120 [00:29<00:12,  2.86it/s]\u001b[A\n",
      "Iteration:  72%|███████▏  | 86/120 [00:29<00:12,  2.79it/s]\u001b[A\n",
      "Iteration:  72%|███████▎  | 87/120 [00:30<00:12,  2.70it/s]\u001b[A\n",
      "Iteration:  73%|███████▎  | 88/120 [00:30<00:12,  2.58it/s]\u001b[A\n",
      "Iteration:  74%|███████▍  | 89/120 [00:31<00:11,  2.65it/s]\u001b[A\n",
      "Iteration:  75%|███████▌  | 90/120 [00:31<00:11,  2.60it/s]\u001b[A\n",
      "Iteration:  76%|███████▌  | 91/120 [00:31<00:11,  2.57it/s]\u001b[A\n",
      "Iteration:  77%|███████▋  | 92/120 [00:32<00:10,  2.65it/s]\u001b[A\n",
      "Iteration:  78%|███████▊  | 93/120 [00:32<00:10,  2.69it/s]\u001b[A\n",
      "Iteration:  78%|███████▊  | 94/120 [00:33<00:09,  2.73it/s]\u001b[A\n",
      "Iteration:  79%|███████▉  | 95/120 [00:33<00:09,  2.67it/s]\u001b[A\n",
      "Iteration:  80%|████████  | 96/120 [00:33<00:08,  2.67it/s]\u001b[A\n",
      "Iteration:  81%|████████  | 97/120 [00:34<00:08,  2.64it/s]\u001b[A\n",
      "Iteration:  82%|████████▏ | 98/120 [00:34<00:08,  2.70it/s]\u001b[A\n",
      "Iteration:  82%|████████▎ | 99/120 [00:34<00:07,  2.74it/s]\u001b[A03/05/2020 01:41:10 - INFO - transformers.configuration_utils -   Configuration saved in ../../../../model_save/Dos-Fases-all_Stance/checkpoint-100/config.json\n",
      "03/05/2020 01:41:13 - INFO - transformers.modeling_utils -   Model weights saved in ../../../../model_save/Dos-Fases-all_Stance/checkpoint-100/pytorch_model.bin\n",
      "03/05/2020 01:41:13 - INFO - __main__ -   Saving model checkpoint to ../../../../model_save/Dos-Fases-all_Stance/checkpoint-100\n",
      "03/05/2020 01:41:20 - INFO - __main__ -   Saving optimizer and scheduler states to ../../../../model_save/Dos-Fases-all_Stance/checkpoint-100\n",
      "\n",
      "Iteration:  83%|████████▎ | 100/120 [00:45<01:07,  3.35s/it]\u001b[A\n",
      "Iteration:  84%|████████▍ | 101/120 [00:45<00:47,  2.47s/it]\u001b[A\n",
      "Iteration:  85%|████████▌ | 102/120 [00:46<00:33,  1.86s/it]\u001b[A\n",
      "Iteration:  86%|████████▌ | 103/120 [00:46<00:24,  1.43s/it]\u001b[A\n",
      "Iteration:  87%|████████▋ | 104/120 [00:46<00:17,  1.11s/it]\u001b[A\n",
      "Iteration:  88%|████████▊ | 105/120 [00:47<00:13,  1.13it/s]\u001b[A\n",
      "Iteration:  88%|████████▊ | 106/120 [00:47<00:10,  1.38it/s]\u001b[A\n",
      "Iteration:  89%|████████▉ | 107/120 [00:47<00:07,  1.63it/s]\u001b[A\n",
      "Iteration:  90%|█████████ | 108/120 [00:48<00:06,  1.87it/s]\u001b[A\n",
      "Iteration:  91%|█████████ | 109/120 [00:48<00:05,  2.07it/s]\u001b[A\n",
      "Iteration:  92%|█████████▏| 110/120 [00:48<00:04,  2.23it/s]\u001b[A\n",
      "Iteration:  92%|█████████▎| 111/120 [00:49<00:03,  2.32it/s]\u001b[A\n",
      "Iteration:  93%|█████████▎| 112/120 [00:49<00:03,  2.44it/s]\u001b[A\n",
      "Iteration:  94%|█████████▍| 113/120 [00:50<00:02,  2.55it/s]\u001b[A\n",
      "Iteration:  95%|█████████▌| 114/120 [00:50<00:02,  2.62it/s]\u001b[A\n",
      "Iteration:  96%|█████████▌| 115/120 [00:50<00:01,  2.68it/s]\u001b[A\n",
      "Iteration:  97%|█████████▋| 116/120 [00:51<00:01,  2.73it/s]\u001b[A\n",
      "Iteration:  98%|█████████▊| 117/120 [00:51<00:01,  2.77it/s]\u001b[A\n",
      "Iteration:  98%|█████████▊| 118/120 [00:51<00:00,  2.78it/s]\u001b[A\n",
      "Iteration:  99%|█████████▉| 119/120 [00:52<00:00,  2.81it/s]\u001b[A\n",
      "Iteration: 100%|██████████| 120/120 [00:52<00:00,  2.28it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|██████████| 1/1 [00:52<00:00, 52.52s/it]\n",
      "03/05/2020 01:41:27 - INFO - __main__ -    global_step = 120, average loss = 3.8776294350624085\n",
      "03/05/2020 01:41:27 - INFO - __main__ -   Saving model checkpoint to ../../../../model_save/Dos-Fases-all_Stance\n",
      "03/05/2020 01:41:27 - INFO - transformers.configuration_utils -   Configuration saved in ../../../../model_save/Dos-Fases-all_Stance/config.json\n",
      "03/05/2020 01:41:30 - INFO - transformers.modeling_utils -   Model weights saved in ../../../../model_save/Dos-Fases-all_Stance/pytorch_model.bin\n",
      "03/05/2020 01:41:30 - INFO - transformers.configuration_utils -   loading configuration file ../../../../model_save/Dos-Fases-all_Stance/config.json\n",
      "03/05/2020 01:41:30 - INFO - transformers.configuration_utils -   Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": null,\n",
      "  \"do_sample\": false,\n",
      "  \"eos_token_ids\": null,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 4,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": null,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "03/05/2020 01:41:30 - INFO - transformers.modeling_utils -   loading weights file ../../../../model_save/Dos-Fases-all_Stance/pytorch_model.bin\n",
      "03/05/2020 01:41:32 - INFO - transformers.tokenization_utils -   Model name '../../../../model_save/Dos-Fases-all_Stance' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming '../../../../model_save/Dos-Fases-all_Stance' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "03/05/2020 01:41:32 - INFO - transformers.tokenization_utils -   Didn't find file ../../../../model_save/Dos-Fases-all_Stance/added_tokens.json. We won't load it.\n",
      "03/05/2020 01:41:32 - INFO - transformers.tokenization_utils -   loading file ../../../../model_save/Dos-Fases-all_Stance/vocab.txt\n",
      "03/05/2020 01:41:32 - INFO - transformers.tokenization_utils -   loading file None\n",
      "03/05/2020 01:41:32 - INFO - transformers.tokenization_utils -   loading file ../../../../model_save/Dos-Fases-all_Stance/special_tokens_map.json\n",
      "03/05/2020 01:41:32 - INFO - transformers.tokenization_utils -   loading file ../../../../model_save/Dos-Fases-all_Stance/tokenizer_config.json\n",
      "03/05/2020 01:41:32 - INFO - __main__ -   Evaluate the following checkpoints: ['../../../../model_save/Dos-Fases-all_Stance']\n",
      "03/05/2020 01:41:32 - INFO - transformers.configuration_utils -   loading configuration file ../../../../model_save/Dos-Fases-all_Stance/config.json\n",
      "03/05/2020 01:41:32 - INFO - transformers.configuration_utils -   Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": null,\n",
      "  \"do_sample\": false,\n",
      "  \"eos_token_ids\": null,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 4,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": null,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "03/05/2020 01:41:32 - INFO - transformers.modeling_utils -   loading weights file ../../../../model_save/Dos-Fases-all_Stance/pytorch_model.bin\n",
      "03/05/2020 01:41:34 - INFO - __main__ -   ***** Running evaluation  *****\n",
      "03/05/2020 01:41:34 - INFO - __main__ -     Num examples = 457\n",
      "03/05/2020 01:41:34 - INFO - __main__ -     Batch size = 16\n",
      "Evaluating:   7%|▋         | 2/29 [00:00<00:02, 10.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "directory ../../CSV_Stance\n",
      "FILENAME Stance_raw_test.txt\n",
      "args.model_name_or_path ../../../../model_save/Dos-Fases-all_Stance\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 29/29 [00:02<00:00, 10.19it/s]\n",
      "03/05/2020 01:41:37 - INFO - __main__ -   ***** Eval results  *****\n",
      "03/05/2020 01:41:37 - INFO - __main__ -     perplexity = tensor(77.9164)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity epoch: 77.916374\n",
      "\n",
      "Memory usage         :  3.70 MB\n",
      "GC collected objects : 844\n",
      "Memory usage         :  3.70 MB\n",
      "GC collected objects : 0\n",
      "Memory usage         :  3.70 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/05/2020 01:41:37 - INFO - transformers.tokenization_utils -   Model name '../../../../model_save/Dos-Fases-all_Stance' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming '../../../../model_save/Dos-Fases-all_Stance' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "03/05/2020 01:41:37 - INFO - transformers.tokenization_utils -   Didn't find file ../../../../model_save/Dos-Fases-all_Stance/added_tokens.json. We won't load it.\n",
      "03/05/2020 01:41:37 - INFO - transformers.tokenization_utils -   loading file ../../../../model_save/Dos-Fases-all_Stance/vocab.txt\n",
      "03/05/2020 01:41:37 - INFO - transformers.tokenization_utils -   loading file None\n",
      "03/05/2020 01:41:37 - INFO - transformers.tokenization_utils -   loading file ../../../../model_save/Dos-Fases-all_Stance/special_tokens_map.json\n",
      "03/05/2020 01:41:37 - INFO - transformers.tokenization_utils -   loading file ../../../../model_save/Dos-Fases-all_Stance/tokenizer_config.json\n",
      "03/05/2020 01:41:37 - INFO - transformers.configuration_utils -   loading configuration file ../../../../model_save/Dos-Fases-all_Stance/config.json\n",
      "03/05/2020 01:41:37 - INFO - transformers.configuration_utils -   Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": null,\n",
      "  \"do_sample\": false,\n",
      "  \"eos_token_ids\": null,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 4,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": null,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "03/05/2020 01:41:37 - INFO - transformers.modeling_utils -   loading weights file ../../../../model_save/Dos-Fases-all_Stance/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT tokenizer...\n",
      "Loading BERT Seq Class...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/05/2020 01:41:39 - INFO - transformers.modeling_utils -   Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n",
      "03/05/2020 01:41:39 - INFO - transformers.modeling_utils -   Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo cargado correctamente desde  ../../../../model_save/Dos-Fases-all_Stance\n",
      "\n",
      "Padding/truncating all sentences to 50 values...\n",
      "\n",
      "Padding token: \"[PAD]\", ID: 0\n",
      "Completado.\n",
      "\n",
      "Padding/truncating all sentences to 50 values...\n",
      "\n",
      "Padding token: \"[PAD]\", ID: 0\n",
      "Completado.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/05/2020 01:42:47 - INFO - transformers.configuration_utils -   Configuration saved in ../../../../model_save/Dos-Fases-all_Stance/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating F-macro...\n",
      "F macro: 0.385\n",
      "F macro none average:  [0.63255814 0.04301075 0.86538462 0.        ]\n",
      "Accuracy: 0.774\n",
      "Saving model to ../../../../model_save/Dos-Fases-all_Stance\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/05/2020 01:42:49 - INFO - transformers.modeling_utils -   Model weights saved in ../../../../model_save/Dos-Fases-all_Stance/pytorch_model.bin\n",
      "03/05/2020 01:43:00 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "03/05/2020 01:43:00 - INFO - transformers.configuration_utils -   loading configuration file ../../../../model_save/Dos-Fases-all_Stance/config.json\n",
      "03/05/2020 01:43:00 - INFO - transformers.configuration_utils -   Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": null,\n",
      "  \"do_sample\": false,\n",
      "  \"eos_token_ids\": null,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 4,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": null,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "03/05/2020 01:43:00 - INFO - transformers.tokenization_utils -   Model name '../../../../model_save/Dos-Fases-all_Stance' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming '../../../../model_save/Dos-Fases-all_Stance' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "03/05/2020 01:43:00 - INFO - transformers.tokenization_utils -   Didn't find file ../../../../model_save/Dos-Fases-all_Stance/added_tokens.json. We won't load it.\n",
      "03/05/2020 01:43:00 - INFO - transformers.tokenization_utils -   loading file ../../../../model_save/Dos-Fases-all_Stance/vocab.txt\n",
      "03/05/2020 01:43:00 - INFO - transformers.tokenization_utils -   loading file None\n",
      "03/05/2020 01:43:00 - INFO - transformers.tokenization_utils -   loading file ../../../../model_save/Dos-Fases-all_Stance/special_tokens_map.json\n",
      "03/05/2020 01:43:00 - INFO - transformers.tokenization_utils -   loading file ../../../../model_save/Dos-Fases-all_Stance/tokenizer_config.json\n",
      "03/05/2020 01:43:00 - INFO - transformers.modeling_utils -   loading weights file ../../../../model_save/Dos-Fases-all_Stance/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage         :  3.70 MB\n",
      "GC collected objects : 427\n",
      "Memory usage         :  3.70 MB\n",
      "GC collected objects : 0\n",
      "Memory usage         :  3.70 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/05/2020 01:43:02 - INFO - transformers.modeling_utils -   Weights of BertForMaskedLM not initialized from pretrained model: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']\n",
      "03/05/2020 01:43:02 - INFO - transformers.modeling_utils -   Weights from pretrained model not used in BertForMaskedLM: ['classifier.weight', 'classifier.bias']\n",
      "03/05/2020 01:43:02 - INFO - __main__ -   Training/evaluation parameters <__main__.Args_LM object at 0x7f274a67cd30>\n",
      "03/05/2020 01:43:02 - INFO - __main__ -   ***** Running training *****\n",
      "03/05/2020 01:43:02 - INFO - __main__ -     Num examples = 1916\n",
      "03/05/2020 01:43:02 - INFO - __main__ -     Num Epochs = 1\n",
      "03/05/2020 01:43:02 - INFO - __main__ -     Instantaneous batch size per GPU = 16\n",
      "03/05/2020 01:43:02 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "03/05/2020 01:43:02 - INFO - __main__ -     Gradient Accumulation steps = 1\n",
      "03/05/2020 01:43:02 - INFO - __main__ -     Total optimization steps = 120\n",
      "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Iteration:   0%|          | 0/120 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "directory ../../CSV_Stance\n",
      "FILENAME Stance_raw_train.txt\n",
      "args.model_name_or_path ../../../../model_save/Dos-Fases-all_Stance\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration:   1%|          | 1/120 [00:00<00:44,  2.68it/s]\u001b[A\n",
      "Iteration:   2%|▏         | 2/120 [00:00<00:43,  2.73it/s]\u001b[A\n",
      "Iteration:   2%|▎         | 3/120 [00:01<00:42,  2.76it/s]\u001b[A\n",
      "Iteration:   3%|▎         | 4/120 [00:01<00:41,  2.80it/s]\u001b[A\n",
      "Iteration:   4%|▍         | 5/120 [00:01<00:41,  2.79it/s]\u001b[A\n",
      "Iteration:   5%|▌         | 6/120 [00:02<00:40,  2.81it/s]\u001b[A\n",
      "Iteration:   6%|▌         | 7/120 [00:02<00:39,  2.83it/s]\u001b[A\n",
      "Iteration:   7%|▋         | 8/120 [00:02<00:39,  2.84it/s]\u001b[A\n",
      "Iteration:   8%|▊         | 9/120 [00:03<00:39,  2.84it/s]\u001b[A\n",
      "Iteration:   8%|▊         | 10/120 [00:03<00:38,  2.85it/s]\u001b[A\n",
      "Iteration:   9%|▉         | 11/120 [00:03<00:38,  2.85it/s]\u001b[A\n",
      "Iteration:  10%|█         | 12/120 [00:04<00:38,  2.84it/s]\u001b[A\n",
      "Iteration:  11%|█         | 13/120 [00:04<00:38,  2.81it/s]\u001b[A\n",
      "Iteration:  12%|█▏        | 14/120 [00:04<00:37,  2.81it/s]\u001b[A\n",
      "Iteration:  12%|█▎        | 15/120 [00:05<00:37,  2.82it/s]\u001b[A\n",
      "Iteration:  13%|█▎        | 16/120 [00:05<00:36,  2.82it/s]\u001b[A\n",
      "Iteration:  14%|█▍        | 17/120 [00:06<00:36,  2.83it/s]\u001b[A\n",
      "Iteration:  15%|█▌        | 18/120 [00:06<00:36,  2.83it/s]\u001b[A\n",
      "Iteration:  16%|█▌        | 19/120 [00:06<00:35,  2.83it/s]\u001b[A\n",
      "Iteration:  17%|█▋        | 20/120 [00:07<00:35,  2.84it/s]\u001b[A\n",
      "Iteration:  18%|█▊        | 21/120 [00:07<00:34,  2.84it/s]\u001b[A\n",
      "Iteration:  18%|█▊        | 22/120 [00:07<00:34,  2.84it/s]\u001b[A\n",
      "Iteration:  19%|█▉        | 23/120 [00:08<00:34,  2.84it/s]\u001b[A\n",
      "Iteration:  20%|██        | 24/120 [00:08<00:33,  2.85it/s]\u001b[A\n",
      "Iteration:  21%|██        | 25/120 [00:08<00:33,  2.85it/s]\u001b[A\n",
      "Iteration:  22%|██▏       | 26/120 [00:09<00:33,  2.85it/s]\u001b[A\n",
      "Iteration:  22%|██▎       | 27/120 [00:09<00:32,  2.85it/s]\u001b[A\n",
      "Iteration:  23%|██▎       | 28/120 [00:09<00:32,  2.84it/s]\u001b[A\n",
      "Iteration:  24%|██▍       | 29/120 [00:10<00:32,  2.80it/s]\u001b[A\n",
      "Iteration:  25%|██▌       | 30/120 [00:10<00:32,  2.81it/s]\u001b[A\n",
      "Iteration:  26%|██▌       | 31/120 [00:10<00:31,  2.82it/s]\u001b[A\n",
      "Iteration:  27%|██▋       | 32/120 [00:11<00:32,  2.75it/s]\u001b[A\n",
      "Iteration:  28%|██▊       | 33/120 [00:11<00:32,  2.70it/s]\u001b[A\n",
      "Iteration:  28%|██▊       | 34/120 [00:12<00:33,  2.60it/s]\u001b[A\n",
      "Iteration:  29%|██▉       | 35/120 [00:12<00:32,  2.58it/s]\u001b[A\n",
      "Iteration:  30%|███       | 36/120 [00:12<00:31,  2.65it/s]\u001b[A\n",
      "Iteration:  31%|███       | 37/120 [00:13<00:30,  2.70it/s]\u001b[A\n",
      "Iteration:  32%|███▏      | 38/120 [00:13<00:29,  2.74it/s]\u001b[A\n",
      "Iteration:  32%|███▎      | 39/120 [00:13<00:29,  2.73it/s]\u001b[A\n",
      "Iteration:  33%|███▎      | 40/120 [00:14<00:29,  2.75it/s]\u001b[A\n",
      "Iteration:  34%|███▍      | 41/120 [00:14<00:28,  2.73it/s]\u001b[A\n",
      "Iteration:  35%|███▌      | 42/120 [00:15<00:29,  2.64it/s]\u001b[A\n",
      "Iteration:  36%|███▌      | 43/120 [00:15<00:29,  2.58it/s]\u001b[A\n",
      "Iteration:  37%|███▋      | 44/120 [00:15<00:30,  2.50it/s]\u001b[A\n",
      "Iteration:  38%|███▊      | 45/120 [00:16<00:30,  2.47it/s]\u001b[A\n",
      "Iteration:  38%|███▊      | 46/120 [00:16<00:28,  2.56it/s]\u001b[A\n",
      "Iteration:  39%|███▉      | 47/120 [00:17<00:28,  2.57it/s]\u001b[A\n",
      "Iteration:  40%|████      | 48/120 [00:17<00:27,  2.64it/s]\u001b[A\n",
      "Iteration:  41%|████      | 49/120 [00:17<00:26,  2.69it/s]\u001b[A\n",
      "Iteration:  42%|████▏     | 50/120 [00:18<00:25,  2.71it/s]\u001b[A\n",
      "Iteration:  42%|████▎     | 51/120 [00:18<00:25,  2.73it/s]\u001b[A\n",
      "Iteration:  43%|████▎     | 52/120 [00:18<00:24,  2.74it/s]\u001b[A\n",
      "Iteration:  44%|████▍     | 53/120 [00:19<00:24,  2.69it/s]\u001b[A\n",
      "Iteration:  45%|████▌     | 54/120 [00:19<00:24,  2.71it/s]\u001b[A\n",
      "Iteration:  46%|████▌     | 55/120 [00:20<00:23,  2.72it/s]\u001b[A\n",
      "Iteration:  47%|████▋     | 56/120 [00:20<00:23,  2.75it/s]\u001b[A\n",
      "Iteration:  48%|████▊     | 57/120 [00:20<00:24,  2.60it/s]\u001b[A\n",
      "Iteration:  48%|████▊     | 58/120 [00:21<00:24,  2.54it/s]\u001b[A\n",
      "Iteration:  49%|████▉     | 59/120 [00:21<00:24,  2.54it/s]\u001b[A\n",
      "Iteration:  50%|█████     | 60/120 [00:21<00:22,  2.63it/s]\u001b[A\n",
      "Iteration:  51%|█████     | 61/120 [00:22<00:23,  2.56it/s]\u001b[A\n",
      "Iteration:  52%|█████▏    | 62/120 [00:22<00:22,  2.55it/s]\u001b[A\n",
      "Iteration:  52%|█████▎    | 63/120 [00:23<00:22,  2.55it/s]\u001b[A\n",
      "Iteration:  53%|█████▎    | 64/120 [00:23<00:21,  2.57it/s]\u001b[A\n",
      "Iteration:  54%|█████▍    | 65/120 [00:23<00:21,  2.57it/s]\u001b[A\n",
      "Iteration:  55%|█████▌    | 66/120 [00:24<00:21,  2.55it/s]\u001b[A\n",
      "Iteration:  56%|█████▌    | 67/120 [00:24<00:20,  2.55it/s]\u001b[A\n",
      "Iteration:  57%|█████▋    | 68/120 [00:25<00:20,  2.56it/s]\u001b[A\n",
      "Iteration:  57%|█████▊    | 69/120 [00:25<00:19,  2.63it/s]\u001b[A\n",
      "Iteration:  58%|█████▊    | 70/120 [00:25<00:18,  2.65it/s]\u001b[A\n",
      "Iteration:  59%|█████▉    | 71/120 [00:26<00:18,  2.64it/s]\u001b[A\n",
      "Iteration:  60%|██████    | 72/120 [00:26<00:18,  2.66it/s]\u001b[A\n",
      "Iteration:  61%|██████    | 73/120 [00:26<00:17,  2.63it/s]\u001b[A\n",
      "Iteration:  62%|██████▏   | 74/120 [00:27<00:17,  2.69it/s]\u001b[A\n",
      "Iteration:  62%|██████▎   | 75/120 [00:27<00:16,  2.74it/s]\u001b[A\n",
      "Iteration:  63%|██████▎   | 76/120 [00:28<00:16,  2.70it/s]\u001b[A\n",
      "Iteration:  64%|██████▍   | 77/120 [00:28<00:15,  2.74it/s]\u001b[A\n",
      "Iteration:  65%|██████▌   | 78/120 [00:28<00:15,  2.78it/s]\u001b[A\n",
      "Iteration:  66%|██████▌   | 79/120 [00:29<00:14,  2.80it/s]\u001b[A\n",
      "Iteration:  67%|██████▋   | 80/120 [00:29<00:14,  2.83it/s]\u001b[A\n",
      "Iteration:  68%|██████▊   | 81/120 [00:29<00:13,  2.84it/s]\u001b[A\n",
      "Iteration:  68%|██████▊   | 82/120 [00:30<00:13,  2.85it/s]\u001b[A\n",
      "Iteration:  69%|██████▉   | 83/120 [00:30<00:12,  2.86it/s]\u001b[A\n",
      "Iteration:  70%|███████   | 84/120 [00:30<00:12,  2.80it/s]\u001b[A\n",
      "Iteration:  71%|███████   | 85/120 [00:31<00:12,  2.82it/s]\u001b[A\n",
      "Iteration:  72%|███████▏  | 86/120 [00:31<00:11,  2.83it/s]\u001b[A\n",
      "Iteration:  72%|███████▎  | 87/120 [00:31<00:11,  2.84it/s]\u001b[A\n",
      "Iteration:  73%|███████▎  | 88/120 [00:32<00:11,  2.85it/s]\u001b[A\n",
      "Iteration:  74%|███████▍  | 89/120 [00:32<00:10,  2.85it/s]\u001b[A\n",
      "Iteration:  75%|███████▌  | 90/120 [00:32<00:10,  2.86it/s]\u001b[A\n",
      "Iteration:  76%|███████▌  | 91/120 [00:33<00:10,  2.85it/s]\u001b[A\n",
      "Iteration:  77%|███████▋  | 92/120 [00:33<00:09,  2.86it/s]\u001b[A\n",
      "Iteration:  78%|███████▊  | 93/120 [00:34<00:09,  2.86it/s]\u001b[A\n",
      "Iteration:  78%|███████▊  | 94/120 [00:34<00:09,  2.85it/s]\u001b[A\n",
      "Iteration:  79%|███████▉  | 95/120 [00:34<00:08,  2.80it/s]\u001b[A\n",
      "Iteration:  80%|████████  | 96/120 [00:35<00:08,  2.82it/s]\u001b[A\n",
      "Iteration:  81%|████████  | 97/120 [00:35<00:08,  2.81it/s]\u001b[A\n",
      "Iteration:  82%|████████▏ | 98/120 [00:35<00:07,  2.83it/s]\u001b[A\n",
      "Iteration:  82%|████████▎ | 99/120 [00:36<00:07,  2.84it/s]\u001b[A03/05/2020 01:43:39 - INFO - transformers.configuration_utils -   Configuration saved in ../../../../model_save/Dos-Fases-all_Stance/checkpoint-100/config.json\n",
      "03/05/2020 01:43:42 - INFO - transformers.modeling_utils -   Model weights saved in ../../../../model_save/Dos-Fases-all_Stance/checkpoint-100/pytorch_model.bin\n",
      "03/05/2020 01:43:42 - INFO - __main__ -   Saving model checkpoint to ../../../../model_save/Dos-Fases-all_Stance/checkpoint-100\n",
      "03/05/2020 01:43:48 - INFO - __main__ -   Saving optimizer and scheduler states to ../../../../model_save/Dos-Fases-all_Stance/checkpoint-100\n",
      "\n",
      "Iteration:  83%|████████▎ | 100/120 [00:45<01:03,  3.19s/it]\u001b[A\n",
      "Iteration:  84%|████████▍ | 101/120 [00:46<00:44,  2.34s/it]\u001b[A\n",
      "Iteration:  85%|████████▌ | 102/120 [00:46<00:31,  1.75s/it]\u001b[A\n",
      "Iteration:  86%|████████▌ | 103/120 [00:47<00:22,  1.33s/it]\u001b[A\n",
      "Iteration:  87%|████████▋ | 104/120 [00:47<00:16,  1.03s/it]\u001b[A\n",
      "Iteration:  88%|████████▊ | 105/120 [00:47<00:12,  1.21it/s]\u001b[A\n",
      "Iteration:  88%|████████▊ | 106/120 [00:48<00:09,  1.46it/s]\u001b[A\n",
      "Iteration:  89%|████████▉ | 107/120 [00:48<00:07,  1.71it/s]\u001b[A\n",
      "Iteration:  90%|█████████ | 108/120 [00:48<00:06,  1.94it/s]\u001b[A\n",
      "Iteration:  91%|█████████ | 109/120 [00:49<00:05,  2.15it/s]\u001b[A\n",
      "Iteration:  92%|█████████▏| 110/120 [00:49<00:04,  2.32it/s]\u001b[A\n",
      "Iteration:  92%|█████████▎| 111/120 [00:49<00:03,  2.46it/s]\u001b[A\n",
      "Iteration:  93%|█████████▎| 112/120 [00:50<00:03,  2.57it/s]\u001b[A\n",
      "Iteration:  94%|█████████▍| 113/120 [00:50<00:02,  2.65it/s]\u001b[A\n",
      "Iteration:  95%|█████████▌| 114/120 [00:50<00:02,  2.71it/s]\u001b[A\n",
      "Iteration:  96%|█████████▌| 115/120 [00:51<00:01,  2.76it/s]\u001b[A\n",
      "Iteration:  97%|█████████▋| 116/120 [00:51<00:01,  2.78it/s]\u001b[A\n",
      "Iteration:  98%|█████████▊| 117/120 [00:51<00:01,  2.81it/s]\u001b[A\n",
      "Iteration:  98%|█████████▊| 118/120 [00:52<00:00,  2.82it/s]\u001b[A\n",
      "Iteration:  99%|█████████▉| 119/120 [00:52<00:00,  2.83it/s]\u001b[A\n",
      "Iteration: 100%|██████████| 120/120 [00:52<00:00,  2.27it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|██████████| 1/1 [00:52<00:00, 52.93s/it]\n",
      "03/05/2020 01:43:55 - INFO - __main__ -    global_step = 120, average loss = 3.3516285359859466\n",
      "03/05/2020 01:43:55 - INFO - __main__ -   Saving model checkpoint to ../../../../model_save/Dos-Fases-all_Stance\n",
      "03/05/2020 01:43:55 - INFO - transformers.configuration_utils -   Configuration saved in ../../../../model_save/Dos-Fases-all_Stance/config.json\n",
      "03/05/2020 01:43:57 - INFO - transformers.modeling_utils -   Model weights saved in ../../../../model_save/Dos-Fases-all_Stance/pytorch_model.bin\n",
      "03/05/2020 01:43:59 - INFO - transformers.configuration_utils -   loading configuration file ../../../../model_save/Dos-Fases-all_Stance/config.json\n",
      "03/05/2020 01:43:59 - INFO - transformers.configuration_utils -   Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": null,\n",
      "  \"do_sample\": false,\n",
      "  \"eos_token_ids\": null,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 4,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": null,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "03/05/2020 01:43:59 - INFO - transformers.modeling_utils -   loading weights file ../../../../model_save/Dos-Fases-all_Stance/pytorch_model.bin\n",
      "03/05/2020 01:44:01 - INFO - transformers.tokenization_utils -   Model name '../../../../model_save/Dos-Fases-all_Stance' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming '../../../../model_save/Dos-Fases-all_Stance' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "03/05/2020 01:44:01 - INFO - transformers.tokenization_utils -   Didn't find file ../../../../model_save/Dos-Fases-all_Stance/added_tokens.json. We won't load it.\n",
      "03/05/2020 01:44:01 - INFO - transformers.tokenization_utils -   loading file ../../../../model_save/Dos-Fases-all_Stance/vocab.txt\n",
      "03/05/2020 01:44:01 - INFO - transformers.tokenization_utils -   loading file None\n",
      "03/05/2020 01:44:01 - INFO - transformers.tokenization_utils -   loading file ../../../../model_save/Dos-Fases-all_Stance/special_tokens_map.json\n",
      "03/05/2020 01:44:01 - INFO - transformers.tokenization_utils -   loading file ../../../../model_save/Dos-Fases-all_Stance/tokenizer_config.json\n",
      "03/05/2020 01:44:01 - INFO - __main__ -   Evaluate the following checkpoints: ['../../../../model_save/Dos-Fases-all_Stance']\n",
      "03/05/2020 01:44:01 - INFO - transformers.configuration_utils -   loading configuration file ../../../../model_save/Dos-Fases-all_Stance/config.json\n",
      "03/05/2020 01:44:01 - INFO - transformers.configuration_utils -   Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": null,\n",
      "  \"do_sample\": false,\n",
      "  \"eos_token_ids\": null,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 4,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": null,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "03/05/2020 01:44:01 - INFO - transformers.modeling_utils -   loading weights file ../../../../model_save/Dos-Fases-all_Stance/pytorch_model.bin\n",
      "03/05/2020 01:44:03 - INFO - __main__ -   ***** Running evaluation  *****\n",
      "03/05/2020 01:44:03 - INFO - __main__ -     Num examples = 457\n",
      "03/05/2020 01:44:03 - INFO - __main__ -     Batch size = 16\n",
      "Evaluating:   7%|▋         | 2/29 [00:00<00:02, 10.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "directory ../../CSV_Stance\n",
      "FILENAME Stance_raw_test.txt\n",
      "args.model_name_or_path ../../../../model_save/Dos-Fases-all_Stance\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 29/29 [00:02<00:00, 10.37it/s]\n",
      "03/05/2020 01:44:06 - INFO - __main__ -   ***** Eval results  *****\n",
      "03/05/2020 01:44:06 - INFO - __main__ -     perplexity = tensor(76.5429)\n",
      "03/05/2020 01:44:06 - INFO - transformers.tokenization_utils -   Model name '../../../../model_save/Dos-Fases-all_Stance' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming '../../../../model_save/Dos-Fases-all_Stance' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "03/05/2020 01:44:06 - INFO - transformers.tokenization_utils -   Didn't find file ../../../../model_save/Dos-Fases-all_Stance/added_tokens.json. We won't load it.\n",
      "03/05/2020 01:44:06 - INFO - transformers.tokenization_utils -   loading file ../../../../model_save/Dos-Fases-all_Stance/vocab.txt\n",
      "03/05/2020 01:44:06 - INFO - transformers.tokenization_utils -   loading file None\n",
      "03/05/2020 01:44:06 - INFO - transformers.tokenization_utils -   loading file ../../../../model_save/Dos-Fases-all_Stance/special_tokens_map.json\n",
      "03/05/2020 01:44:06 - INFO - transformers.tokenization_utils -   loading file ../../../../model_save/Dos-Fases-all_Stance/tokenizer_config.json\n",
      "03/05/2020 01:44:06 - INFO - transformers.configuration_utils -   loading configuration file ../../../../model_save/Dos-Fases-all_Stance/config.json\n",
      "03/05/2020 01:44:06 - INFO - transformers.configuration_utils -   Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": null,\n",
      "  \"do_sample\": false,\n",
      "  \"eos_token_ids\": null,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 4,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": null,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "03/05/2020 01:44:06 - INFO - transformers.modeling_utils -   loading weights file ../../../../model_save/Dos-Fases-all_Stance/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity epoch: 76.542946\n",
      "\n",
      "Memory usage         :  3.70 MB\n",
      "GC collected objects : 844\n",
      "Memory usage         :  3.70 MB\n",
      "GC collected objects : 0\n",
      "Memory usage         :  3.70 MB\n",
      "Loading BERT tokenizer...\n",
      "Loading BERT Seq Class...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/05/2020 01:44:08 - INFO - transformers.modeling_utils -   Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n",
      "03/05/2020 01:44:08 - INFO - transformers.modeling_utils -   Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo cargado correctamente desde  ../../../../model_save/Dos-Fases-all_Stance\n",
      "\n",
      "Padding/truncating all sentences to 50 values...\n",
      "\n",
      "Padding token: \"[PAD]\", ID: 0\n",
      "Completado.\n",
      "\n",
      "Padding/truncating all sentences to 50 values...\n",
      "\n",
      "Padding token: \"[PAD]\", ID: 0\n",
      "Completado.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/05/2020 01:45:22 - INFO - transformers.configuration_utils -   Configuration saved in ../../../../model_save/Dos-Fases-all_Stance/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating F-macro...\n",
      "F macro: 0.413\n",
      "F macro none average:  [0.62264151 0.1221374  0.856084   0.05      ]\n",
      "Accuracy: 0.753\n",
      "Saving model to ../../../../model_save/Dos-Fases-all_Stance\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/05/2020 01:45:24 - INFO - transformers.modeling_utils -   Model weights saved in ../../../../model_save/Dos-Fases-all_Stance/pytorch_model.bin\n",
      "03/05/2020 01:45:38 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "03/05/2020 01:45:38 - INFO - transformers.configuration_utils -   loading configuration file ../../../../model_save/Dos-Fases-all_Stance/config.json\n",
      "03/05/2020 01:45:38 - INFO - transformers.configuration_utils -   Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": null,\n",
      "  \"do_sample\": false,\n",
      "  \"eos_token_ids\": null,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 4,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": null,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "03/05/2020 01:45:38 - INFO - transformers.tokenization_utils -   Model name '../../../../model_save/Dos-Fases-all_Stance' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming '../../../../model_save/Dos-Fases-all_Stance' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "03/05/2020 01:45:38 - INFO - transformers.tokenization_utils -   Didn't find file ../../../../model_save/Dos-Fases-all_Stance/added_tokens.json. We won't load it.\n",
      "03/05/2020 01:45:38 - INFO - transformers.tokenization_utils -   loading file ../../../../model_save/Dos-Fases-all_Stance/vocab.txt\n",
      "03/05/2020 01:45:38 - INFO - transformers.tokenization_utils -   loading file None\n",
      "03/05/2020 01:45:38 - INFO - transformers.tokenization_utils -   loading file ../../../../model_save/Dos-Fases-all_Stance/special_tokens_map.json\n",
      "03/05/2020 01:45:38 - INFO - transformers.tokenization_utils -   loading file ../../../../model_save/Dos-Fases-all_Stance/tokenizer_config.json\n",
      "03/05/2020 01:45:38 - INFO - transformers.modeling_utils -   loading weights file ../../../../model_save/Dos-Fases-all_Stance/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage         :  3.70 MB\n",
      "GC collected objects : 427\n",
      "Memory usage         :  3.70 MB\n",
      "GC collected objects : 0\n",
      "Memory usage         :  3.70 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/05/2020 01:45:40 - INFO - transformers.modeling_utils -   Weights of BertForMaskedLM not initialized from pretrained model: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']\n",
      "03/05/2020 01:45:40 - INFO - transformers.modeling_utils -   Weights from pretrained model not used in BertForMaskedLM: ['classifier.weight', 'classifier.bias']\n",
      "03/05/2020 01:45:40 - INFO - __main__ -   Training/evaluation parameters <__main__.Args_LM object at 0x7f274a67cd30>\n",
      "03/05/2020 01:45:40 - INFO - __main__ -   ***** Running training *****\n",
      "03/05/2020 01:45:40 - INFO - __main__ -     Num examples = 1916\n",
      "03/05/2020 01:45:40 - INFO - __main__ -     Num Epochs = 1\n",
      "03/05/2020 01:45:40 - INFO - __main__ -     Instantaneous batch size per GPU = 16\n",
      "03/05/2020 01:45:40 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "03/05/2020 01:45:40 - INFO - __main__ -     Gradient Accumulation steps = 1\n",
      "03/05/2020 01:45:40 - INFO - __main__ -     Total optimization steps = 120\n",
      "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Iteration:   0%|          | 0/120 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "directory ../../CSV_Stance\n",
      "FILENAME Stance_raw_train.txt\n",
      "args.model_name_or_path ../../../../model_save/Dos-Fases-all_Stance\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration:   1%|          | 1/120 [00:00<00:48,  2.45it/s]\u001b[A\n",
      "Iteration:   2%|▏         | 2/120 [00:00<00:47,  2.48it/s]\u001b[A\n",
      "Iteration:   2%|▎         | 3/120 [00:01<00:46,  2.51it/s]\u001b[A\n",
      "Iteration:   3%|▎         | 4/120 [00:01<00:46,  2.48it/s]\u001b[A\n",
      "Iteration:   4%|▍         | 5/120 [00:02<00:46,  2.48it/s]\u001b[A\n",
      "Iteration:   5%|▌         | 6/120 [00:02<00:45,  2.49it/s]\u001b[A\n",
      "Iteration:   6%|▌         | 7/120 [00:02<00:45,  2.49it/s]\u001b[A\n",
      "Iteration:   7%|▋         | 8/120 [00:03<00:45,  2.49it/s]\u001b[A\n",
      "Iteration:   8%|▊         | 9/120 [00:03<00:45,  2.43it/s]\u001b[A\n",
      "Iteration:   8%|▊         | 10/120 [00:04<00:45,  2.40it/s]\u001b[A\n",
      "Iteration:   9%|▉         | 11/120 [00:04<00:46,  2.37it/s]\u001b[A\n",
      "Iteration:  10%|█         | 12/120 [00:04<00:45,  2.40it/s]\u001b[A\n",
      "Iteration:  11%|█         | 13/120 [00:05<00:42,  2.52it/s]\u001b[A\n",
      "Iteration:  12%|█▏        | 14/120 [00:05<00:40,  2.62it/s]\u001b[A\n",
      "Iteration:  12%|█▎        | 15/120 [00:05<00:39,  2.65it/s]\u001b[A\n",
      "Iteration:  13%|█▎        | 16/120 [00:06<00:39,  2.61it/s]\u001b[A\n",
      "Iteration:  14%|█▍        | 17/120 [00:06<00:40,  2.56it/s]\u001b[A\n",
      "Iteration:  15%|█▌        | 18/120 [00:07<00:40,  2.54it/s]\u001b[A\n",
      "Iteration:  16%|█▌        | 19/120 [00:07<00:40,  2.50it/s]\u001b[A\n",
      "Iteration:  17%|█▋        | 20/120 [00:08<00:40,  2.46it/s]\u001b[A\n",
      "Iteration:  18%|█▊        | 21/120 [00:08<00:40,  2.46it/s]\u001b[A\n",
      "Iteration:  18%|█▊        | 22/120 [00:08<00:39,  2.47it/s]\u001b[A\n",
      "Iteration:  19%|█▉        | 23/120 [00:09<00:39,  2.48it/s]\u001b[A\n",
      "Iteration:  20%|██        | 24/120 [00:09<00:39,  2.45it/s]\u001b[A\n",
      "Iteration:  21%|██        | 25/120 [00:10<00:38,  2.44it/s]\u001b[A\n",
      "Iteration:  22%|██▏       | 26/120 [00:10<00:38,  2.42it/s]\u001b[A\n",
      "Iteration:  22%|██▎       | 27/120 [00:10<00:38,  2.39it/s]\u001b[A\n",
      "Iteration:  23%|██▎       | 28/120 [00:11<00:37,  2.47it/s]\u001b[A\n",
      "Iteration:  24%|██▍       | 29/120 [00:11<00:36,  2.51it/s]\u001b[A\n",
      "Iteration:  25%|██▌       | 30/120 [00:12<00:35,  2.56it/s]\u001b[A\n",
      "Iteration:  26%|██▌       | 31/120 [00:12<00:34,  2.60it/s]\u001b[A\n",
      "Iteration:  27%|██▋       | 32/120 [00:12<00:33,  2.63it/s]\u001b[A\n",
      "Iteration:  28%|██▊       | 33/120 [00:13<00:32,  2.67it/s]\u001b[A\n",
      "Iteration:  28%|██▊       | 34/120 [00:13<00:31,  2.69it/s]\u001b[A\n",
      "Iteration:  29%|██▉       | 35/120 [00:13<00:31,  2.72it/s]\u001b[A\n",
      "Iteration:  30%|███       | 36/120 [00:14<00:30,  2.74it/s]\u001b[A\n",
      "Iteration:  31%|███       | 37/120 [00:14<00:30,  2.75it/s]\u001b[A\n",
      "Iteration:  32%|███▏      | 38/120 [00:14<00:29,  2.75it/s]\u001b[A\n",
      "Iteration:  32%|███▎      | 39/120 [00:15<00:29,  2.76it/s]\u001b[A\n",
      "Iteration:  33%|███▎      | 40/120 [00:15<00:28,  2.77it/s]\u001b[A\n",
      "Iteration:  34%|███▍      | 41/120 [00:16<00:28,  2.77it/s]\u001b[A\n",
      "Iteration:  35%|███▌      | 42/120 [00:16<00:28,  2.76it/s]\u001b[A\n",
      "Iteration:  36%|███▌      | 43/120 [00:16<00:27,  2.76it/s]\u001b[A\n",
      "Iteration:  37%|███▋      | 44/120 [00:17<00:27,  2.77it/s]\u001b[A\n",
      "Iteration:  38%|███▊      | 45/120 [00:17<00:27,  2.76it/s]\u001b[A\n",
      "Iteration:  38%|███▊      | 46/120 [00:17<00:26,  2.77it/s]\u001b[A\n",
      "Iteration:  39%|███▉      | 47/120 [00:18<00:26,  2.77it/s]\u001b[A\n",
      "Iteration:  40%|████      | 48/120 [00:18<00:25,  2.78it/s]\u001b[A\n",
      "Iteration:  41%|████      | 49/120 [00:18<00:25,  2.78it/s]\u001b[A\n",
      "Iteration:  42%|████▏     | 50/120 [00:19<00:25,  2.77it/s]\u001b[A\n",
      "Iteration:  42%|████▎     | 51/120 [00:19<00:24,  2.77it/s]\u001b[A\n",
      "Iteration:  43%|████▎     | 52/120 [00:19<00:24,  2.76it/s]\u001b[A\n",
      "Iteration:  44%|████▍     | 53/120 [00:20<00:24,  2.76it/s]\u001b[A\n",
      "Iteration:  45%|████▌     | 54/120 [00:20<00:23,  2.76it/s]\u001b[A\n",
      "Iteration:  46%|████▌     | 55/120 [00:21<00:23,  2.75it/s]\u001b[A\n",
      "Iteration:  47%|████▋     | 56/120 [00:21<00:23,  2.76it/s]\u001b[A\n",
      "Iteration:  48%|████▊     | 57/120 [00:21<00:22,  2.77it/s]\u001b[A\n",
      "Iteration:  48%|████▊     | 58/120 [00:22<00:22,  2.76it/s]\u001b[A\n",
      "Iteration:  49%|████▉     | 59/120 [00:22<00:22,  2.74it/s]\u001b[A\n",
      "Iteration:  50%|█████     | 60/120 [00:22<00:21,  2.75it/s]\u001b[A\n",
      "Iteration:  51%|█████     | 61/120 [00:23<00:21,  2.75it/s]\u001b[A\n",
      "Iteration:  52%|█████▏    | 62/120 [00:23<00:21,  2.76it/s]\u001b[A\n",
      "Iteration:  52%|█████▎    | 63/120 [00:23<00:20,  2.73it/s]\u001b[A\n",
      "Iteration:  53%|█████▎    | 64/120 [00:24<00:20,  2.74it/s]\u001b[A\n",
      "Iteration:  54%|█████▍    | 65/120 [00:24<00:20,  2.75it/s]\u001b[A\n",
      "Iteration:  55%|█████▌    | 66/120 [00:25<00:19,  2.74it/s]\u001b[A\n",
      "Iteration:  56%|█████▌    | 67/120 [00:25<00:19,  2.75it/s]\u001b[A\n",
      "Iteration:  57%|█████▋    | 68/120 [00:25<00:18,  2.75it/s]\u001b[A\n",
      "Iteration:  57%|█████▊    | 69/120 [00:26<00:18,  2.74it/s]\u001b[A\n",
      "Iteration:  58%|█████▊    | 70/120 [00:26<00:18,  2.75it/s]\u001b[A\n",
      "Iteration:  59%|█████▉    | 71/120 [00:26<00:17,  2.75it/s]\u001b[A\n",
      "Iteration:  60%|██████    | 72/120 [00:27<00:17,  2.76it/s]\u001b[A\n",
      "Iteration:  61%|██████    | 73/120 [00:27<00:17,  2.75it/s]\u001b[A\n",
      "Iteration:  62%|██████▏   | 74/120 [00:28<00:16,  2.73it/s]\u001b[A\n",
      "Iteration:  62%|██████▎   | 75/120 [00:28<00:16,  2.74it/s]\u001b[A\n",
      "Iteration:  63%|██████▎   | 76/120 [00:28<00:16,  2.74it/s]\u001b[A\n",
      "Iteration:  64%|██████▍   | 77/120 [00:29<00:15,  2.74it/s]\u001b[A\n",
      "Iteration:  65%|██████▌   | 78/120 [00:29<00:15,  2.74it/s]\u001b[A\n",
      "Iteration:  66%|██████▌   | 79/120 [00:29<00:14,  2.74it/s]\u001b[A\n",
      "Iteration:  67%|██████▋   | 80/120 [00:30<00:14,  2.73it/s]\u001b[A\n",
      "Iteration:  68%|██████▊   | 81/120 [00:30<00:14,  2.74it/s]\u001b[A\n",
      "Iteration:  68%|██████▊   | 82/120 [00:30<00:13,  2.74it/s]\u001b[A\n",
      "Iteration:  69%|██████▉   | 83/120 [00:31<00:13,  2.74it/s]\u001b[A\n",
      "Iteration:  70%|███████   | 84/120 [00:31<00:13,  2.73it/s]\u001b[A\n",
      "Iteration:  71%|███████   | 85/120 [00:32<00:12,  2.73it/s]\u001b[A\n",
      "Iteration:  72%|███████▏  | 86/120 [00:32<00:12,  2.65it/s]\u001b[A\n",
      "Iteration:  72%|███████▎  | 87/120 [00:32<00:12,  2.60it/s]\u001b[A\n",
      "Iteration:  73%|███████▎  | 88/120 [00:33<00:12,  2.53it/s]\u001b[A\n",
      "Iteration:  74%|███████▍  | 89/120 [00:33<00:12,  2.56it/s]\u001b[A\n",
      "Iteration:  75%|███████▌  | 90/120 [00:34<00:11,  2.59it/s]\u001b[A\n",
      "Iteration:  76%|███████▌  | 91/120 [00:34<00:10,  2.66it/s]\u001b[A\n",
      "Iteration:  77%|███████▋  | 92/120 [00:34<00:10,  2.70it/s]\u001b[A\n",
      "Iteration:  78%|███████▊  | 93/120 [00:35<00:09,  2.75it/s]\u001b[A\n",
      "Iteration:  78%|███████▊  | 94/120 [00:35<00:09,  2.79it/s]\u001b[A\n",
      "Iteration:  79%|███████▉  | 95/120 [00:35<00:09,  2.72it/s]\u001b[A\n",
      "Iteration:  80%|████████  | 96/120 [00:36<00:08,  2.72it/s]\u001b[A\n",
      "Iteration:  81%|████████  | 97/120 [00:36<00:08,  2.74it/s]\u001b[A\n",
      "Iteration:  82%|████████▏ | 98/120 [00:36<00:08,  2.66it/s]\u001b[A\n",
      "Iteration:  82%|████████▎ | 99/120 [00:37<00:07,  2.72it/s]\u001b[A03/05/2020 01:46:17 - INFO - transformers.configuration_utils -   Configuration saved in ../../../../model_save/Dos-Fases-all_Stance/checkpoint-100/config.json\n",
      "03/05/2020 01:46:20 - INFO - transformers.modeling_utils -   Model weights saved in ../../../../model_save/Dos-Fases-all_Stance/checkpoint-100/pytorch_model.bin\n",
      "03/05/2020 01:46:20 - INFO - __main__ -   Saving model checkpoint to ../../../../model_save/Dos-Fases-all_Stance/checkpoint-100\n",
      "03/05/2020 01:46:27 - INFO - __main__ -   Saving optimizer and scheduler states to ../../../../model_save/Dos-Fases-all_Stance/checkpoint-100\n",
      "\n",
      "Iteration:  83%|████████▎ | 100/120 [00:47<01:05,  3.30s/it]\u001b[A\n",
      "Iteration:  84%|████████▍ | 101/120 [00:47<00:46,  2.42s/it]\u001b[A\n",
      "Iteration:  85%|████████▌ | 102/120 [00:48<00:32,  1.80s/it]\u001b[A\n",
      "Iteration:  86%|████████▌ | 103/120 [00:48<00:23,  1.37s/it]\u001b[A\n",
      "Iteration:  87%|████████▋ | 104/120 [00:48<00:16,  1.06s/it]\u001b[A\n",
      "Iteration:  88%|████████▊ | 105/120 [00:49<00:12,  1.18it/s]\u001b[A\n",
      "Iteration:  88%|████████▊ | 106/120 [00:49<00:09,  1.41it/s]\u001b[A\n",
      "Iteration:  89%|████████▉ | 107/120 [00:49<00:07,  1.64it/s]\u001b[A\n",
      "Iteration:  90%|█████████ | 108/120 [00:50<00:06,  1.86it/s]\u001b[A\n",
      "Iteration:  91%|█████████ | 109/120 [00:50<00:05,  2.08it/s]\u001b[A\n",
      "Iteration:  92%|█████████▏| 110/120 [00:51<00:04,  2.26it/s]\u001b[A\n",
      "Iteration:  92%|█████████▎| 111/120 [00:51<00:03,  2.41it/s]\u001b[A\n",
      "Iteration:  93%|█████████▎| 112/120 [00:51<00:03,  2.53it/s]\u001b[A\n",
      "Iteration:  94%|█████████▍| 113/120 [00:52<00:02,  2.60it/s]\u001b[A\n",
      "Iteration:  95%|█████████▌| 114/120 [00:52<00:02,  2.67it/s]\u001b[A\n",
      "Iteration:  96%|█████████▌| 115/120 [00:52<00:01,  2.73it/s]\u001b[A\n",
      "Iteration:  97%|█████████▋| 116/120 [00:53<00:01,  2.77it/s]\u001b[A\n",
      "Iteration:  98%|█████████▊| 117/120 [00:53<00:01,  2.80it/s]\u001b[A\n",
      "Iteration:  98%|█████████▊| 118/120 [00:53<00:00,  2.82it/s]\u001b[A\n",
      "Iteration:  99%|█████████▉| 119/120 [00:54<00:00,  2.84it/s]\u001b[A\n",
      "Iteration: 100%|██████████| 120/120 [00:54<00:00,  2.20it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|██████████| 1/1 [00:54<00:00, 54.47s/it]\n",
      "03/05/2020 01:46:34 - INFO - __main__ -    global_step = 120, average loss = 2.9185398757457732\n",
      "03/05/2020 01:46:34 - INFO - __main__ -   Saving model checkpoint to ../../../../model_save/Dos-Fases-all_Stance\n",
      "03/05/2020 01:46:34 - INFO - transformers.configuration_utils -   Configuration saved in ../../../../model_save/Dos-Fases-all_Stance/config.json\n",
      "03/05/2020 01:46:37 - INFO - transformers.modeling_utils -   Model weights saved in ../../../../model_save/Dos-Fases-all_Stance/pytorch_model.bin\n",
      "03/05/2020 01:46:37 - INFO - transformers.configuration_utils -   loading configuration file ../../../../model_save/Dos-Fases-all_Stance/config.json\n",
      "03/05/2020 01:46:37 - INFO - transformers.configuration_utils -   Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": null,\n",
      "  \"do_sample\": false,\n",
      "  \"eos_token_ids\": null,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 4,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": null,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "03/05/2020 01:46:37 - INFO - transformers.modeling_utils -   loading weights file ../../../../model_save/Dos-Fases-all_Stance/pytorch_model.bin\n",
      "03/05/2020 01:46:39 - INFO - transformers.tokenization_utils -   Model name '../../../../model_save/Dos-Fases-all_Stance' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming '../../../../model_save/Dos-Fases-all_Stance' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "03/05/2020 01:46:39 - INFO - transformers.tokenization_utils -   Didn't find file ../../../../model_save/Dos-Fases-all_Stance/added_tokens.json. We won't load it.\n",
      "03/05/2020 01:46:39 - INFO - transformers.tokenization_utils -   loading file ../../../../model_save/Dos-Fases-all_Stance/vocab.txt\n",
      "03/05/2020 01:46:39 - INFO - transformers.tokenization_utils -   loading file None\n",
      "03/05/2020 01:46:39 - INFO - transformers.tokenization_utils -   loading file ../../../../model_save/Dos-Fases-all_Stance/special_tokens_map.json\n",
      "03/05/2020 01:46:39 - INFO - transformers.tokenization_utils -   loading file ../../../../model_save/Dos-Fases-all_Stance/tokenizer_config.json\n",
      "03/05/2020 01:46:39 - INFO - __main__ -   Evaluate the following checkpoints: ['../../../../model_save/Dos-Fases-all_Stance']\n",
      "03/05/2020 01:46:39 - INFO - transformers.configuration_utils -   loading configuration file ../../../../model_save/Dos-Fases-all_Stance/config.json\n",
      "03/05/2020 01:46:39 - INFO - transformers.configuration_utils -   Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": null,\n",
      "  \"do_sample\": false,\n",
      "  \"eos_token_ids\": null,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 4,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": null,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "03/05/2020 01:46:39 - INFO - transformers.modeling_utils -   loading weights file ../../../../model_save/Dos-Fases-all_Stance/pytorch_model.bin\n",
      "03/05/2020 01:46:41 - INFO - __main__ -   ***** Running evaluation  *****\n",
      "03/05/2020 01:46:41 - INFO - __main__ -     Num examples = 457\n",
      "03/05/2020 01:46:41 - INFO - __main__ -     Batch size = 16\n",
      "Evaluating:   7%|▋         | 2/29 [00:00<00:02, 10.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "directory ../../CSV_Stance\n",
      "FILENAME Stance_raw_test.txt\n",
      "args.model_name_or_path ../../../../model_save/Dos-Fases-all_Stance\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 29/29 [00:02<00:00, 10.11it/s]\n",
      "03/05/2020 01:46:44 - INFO - __main__ -   ***** Eval results  *****\n",
      "03/05/2020 01:46:44 - INFO - __main__ -     perplexity = tensor(77.6279)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity epoch: 77.62793\n",
      "\n",
      "Memory usage         :  3.70 MB\n",
      "GC collected objects : 844\n",
      "Memory usage         :  3.70 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/05/2020 01:46:44 - INFO - transformers.tokenization_utils -   Model name '../../../../model_save/Dos-Fases-all_Stance' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming '../../../../model_save/Dos-Fases-all_Stance' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "03/05/2020 01:46:44 - INFO - transformers.tokenization_utils -   Didn't find file ../../../../model_save/Dos-Fases-all_Stance/added_tokens.json. We won't load it.\n",
      "03/05/2020 01:46:44 - INFO - transformers.tokenization_utils -   loading file ../../../../model_save/Dos-Fases-all_Stance/vocab.txt\n",
      "03/05/2020 01:46:44 - INFO - transformers.tokenization_utils -   loading file None\n",
      "03/05/2020 01:46:44 - INFO - transformers.tokenization_utils -   loading file ../../../../model_save/Dos-Fases-all_Stance/special_tokens_map.json\n",
      "03/05/2020 01:46:44 - INFO - transformers.tokenization_utils -   loading file ../../../../model_save/Dos-Fases-all_Stance/tokenizer_config.json\n",
      "03/05/2020 01:46:44 - INFO - transformers.configuration_utils -   loading configuration file ../../../../model_save/Dos-Fases-all_Stance/config.json\n",
      "03/05/2020 01:46:44 - INFO - transformers.configuration_utils -   Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": null,\n",
      "  \"do_sample\": false,\n",
      "  \"eos_token_ids\": null,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 4,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": null,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "03/05/2020 01:46:44 - INFO - transformers.modeling_utils -   loading weights file ../../../../model_save/Dos-Fases-all_Stance/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GC collected objects : 0\n",
      "Memory usage         :  3.70 MB\n",
      "Loading BERT tokenizer...\n",
      "Loading BERT Seq Class...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/05/2020 01:46:46 - INFO - transformers.modeling_utils -   Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n",
      "03/05/2020 01:46:46 - INFO - transformers.modeling_utils -   Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo cargado correctamente desde  ../../../../model_save/Dos-Fases-all_Stance\n",
      "\n",
      "Padding/truncating all sentences to 50 values...\n",
      "\n",
      "Padding token: \"[PAD]\", ID: 0\n",
      "Completado.\n",
      "\n",
      "Padding/truncating all sentences to 50 values...\n",
      "\n",
      "Padding token: \"[PAD]\", ID: 0\n",
      "Completado.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/05/2020 01:47:55 - INFO - transformers.configuration_utils -   Configuration saved in ../../../../model_save/Dos-Fases-all_Stance/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating F-macro...\n",
      "F macro: 0.416\n",
      "F macro none average:  [0.62686567 0.11111111 0.84037855 0.08510638]\n",
      "Accuracy: 0.727\n",
      "Saving model to ../../../../model_save/Dos-Fases-all_Stance\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/05/2020 01:47:58 - INFO - transformers.modeling_utils -   Model weights saved in ../../../../model_save/Dos-Fases-all_Stance/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage         :  3.70 MB\n",
      "GC collected objects : 427\n",
      "Memory usage         :  3.70 MB\n",
      "GC collected objects : 0\n",
      "Memory usage         :  3.70 MB\n"
     ]
    }
   ],
   "source": [
    "Num_epochs= 5\n",
    "out= '../../../../model_save/Dos-Fases-all_Stance'\n",
    "from_pre = True\n",
    "\n",
    "f= open(out+\"/Summary_macro.txt\",\"w\")\n",
    "f.write(\"This is a summary of the running \\n\")\n",
    "f.close()\n",
    "\n",
    "while epoch_macro <= Num_epochs:\n",
    "    ####################################################\n",
    "    ################## LANGUAGE MODEL ##################\n",
    "    f=open(out+\"/Summary_macro.txt\", \"a+\")\n",
    "    f.write(\"------------------------------------\\n\")\n",
    "    f.write(\"Running Language Model epoch: \"+str(epoch_macro)+'\\n')\n",
    "    resultado_lm = fase_LM(from_pre, my_args, logger)\n",
    "    print (\"Perplexity epoch: \"+str(list(resultado_lm.values())[0].cpu().numpy())+'\\n')\n",
    "    f.write(\"Perplexity epoch: \"+str(list(resultado_lm.values())[0].cpu().numpy())+'\\n')\n",
    "    f.close()\n",
    "    ####################################################\n",
    "    ################### Free memory ####################\n",
    "    mem()\n",
    "    print('GC collected objects : %d' % gc.collect())\n",
    "    mem()\n",
    "    \n",
    "    gc.collect()    \n",
    "    \n",
    "    print('GC collected objects : %d' % gc.collect())\n",
    "    mem()\n",
    "    ####################################################\n",
    "    ################### SEQUENCE CLASS #################\n",
    "    f=open(out+\"/Summary_macro.txt\", \"a+\")\n",
    "    f.write(\"------------------------------------\\n\")\n",
    "    f.write(\"Running Sentence Classification epoch: \"+str(epoch_macro)+'\\n')\n",
    "    \n",
    "    from_pre = False\n",
    "    #qscd\n",
    "    d_lab=dict()\n",
    "    d_lab[\"questioning\"]=0\n",
    "    d_lab[\"support\"]=1\n",
    "    d_lab[\"commenting\"]=2\n",
    "    d_lab[\"denying\"]=3\n",
    "      \n",
    "    max_len = 50\n",
    "    batch = 16 \n",
    "    fma, fno, acc = fase_SC(from_pre, batch, d_lab, max_len)\n",
    "    f.write(\"F macro: \"+str(fma)+'\\n')\n",
    "    f.write(\"F macro none avergage: \"+str(fno)+'\\n')\n",
    "    f.write(\"Accuracy: \"+str(acc)+'\\n')\n",
    "    f.write(\"------------------------------------\\n\")\n",
    "    f.close()\n",
    "    \n",
    "    ####################################################\n",
    "    ################### Free memory ####################\n",
    "    mem()\n",
    "    print('GC collected objects : %d' % gc.collect())\n",
    "    mem()\n",
    "    \n",
    "    gc.collect()    \n",
    "    \n",
    "    print('GC collected objects : %d' % gc.collect())\n",
    "    mem()\n",
    "    \n",
    "    epoch_macro += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:newpy3]",
   "language": "python",
   "name": "conda-env-newpy3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
