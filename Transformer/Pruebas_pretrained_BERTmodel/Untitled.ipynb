{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/casapanshop/anaconda2/envs/newpy3_tf1/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/casapanshop/anaconda2/envs/newpy3_tf1/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/casapanshop/anaconda2/envs/newpy3_tf1/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/casapanshop/anaconda2/envs/newpy3_tf1/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/casapanshop/anaconda2/envs/newpy3_tf1/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/casapanshop/anaconda2/envs/newpy3_tf1/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/casapanshop/anaconda2/envs/newpy3_tf1/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/casapanshop/anaconda2/envs/newpy3_tf1/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/casapanshop/anaconda2/envs/newpy3_tf1/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/casapanshop/anaconda2/envs/newpy3_tf1/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/casapanshop/anaconda2/envs/newpy3_tf1/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/casapanshop/anaconda2/envs/newpy3_tf1/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: GeForce GTX 1060 6GB\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "from collections import Counter\n",
    "from gensim import utils, matutils \n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.utils import deprecated\n",
    "import itertools\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "from numpy import linalg as LA\n",
    "from numpy.random import binomial\n",
    "from numpy import dot, float32 as REAL, memmap as np_memmap, \\\n",
    "    double, array, zeros, vstack, sqrt, newaxis, integer, \\\n",
    "    ndarray, sum as np_sum, prod, argmax\n",
    "from numpy.linalg import norm\n",
    "import math, copy, time\n",
    "import warnings \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_context(context=\"talk\")\n",
    "%matplotlib inline\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras import backend as K\n",
    "import keras\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import load_model\n",
    "\n",
    "import os, sys, re, io, nltk, torch\n",
    "import pandas as pd\n",
    "\n",
    "from scipy import stats\n",
    "from scipy.stats import bernoulli\n",
    "from scipy.spatial import distance\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.preprocessing import Normalizer, normalize\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.metrics import accuracy_score,f1_score, confusion_matrix, recall_score, precision_score\n",
    "\n",
    "from six import string_types, integer_types\n",
    "from six.moves import zip, range\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "\n",
    "from tqdm import tqdm, trange\n",
    "import time\n",
    "import transformers\n",
    "from transformers import (\n",
    "    WEIGHTS_NAME,\n",
    "    AdamW,\n",
    "    BertConfig,\n",
    "    BertForMaskedLM,\n",
    "    BertForSequenceClassification,\n",
    "    BertTokenizer,\n",
    "    CamembertConfig,\n",
    "    CamembertForMaskedLM,\n",
    "    CamembertTokenizer,\n",
    "    DistilBertConfig,\n",
    "    DistilBertForMaskedLM,\n",
    "    DistilBertTokenizer,\n",
    "    GPT2Config,\n",
    "    GPT2LMHeadModel,\n",
    "    GPT2Tokenizer,\n",
    "    OpenAIGPTConfig,\n",
    "    OpenAIGPTLMHeadModel,\n",
    "    OpenAIGPTTokenizer,\n",
    "    RobertaConfig,\n",
    "    RobertaForMaskedLM,\n",
    "    RobertaTokenizer,\n",
    "    get_linear_schedule_with_warmup,\n",
    ")\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.autograd import Variable\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import WordNetLemmatizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "\n",
    "stemmer_sn = SnowballStemmer(\"english\")\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "stoplist = stopwords.words(\"english\")\n",
    "lemmatizer=WordNetLemmatizer()\n",
    "\n",
    "\n",
    "'''\n",
    "Compatible with tensorflow backend\n",
    "gamma entre más alto más tolerante (queremos un gamma chico pero no demasiado! 0.25 - 5)\n",
    "alpha pesos por clase (weights)\n",
    "'''\n",
    "def focal_loss(gamma=2., weights=1):   #weights np.asarray()\n",
    "    weights= K.variable(weights)\n",
    "    def focal_loss_fixed(y_true, y_pred):\n",
    "        y_true = K.clip(y_true, K.epsilon(),1)\n",
    "        y_pred = K.clip(y_pred,K.epsilon(),1)\n",
    "        return - K.sum(weights* K.pow(1. - y_pred, gamma)* y_true * K.log(y_pred), axis=-1) \n",
    "    return focal_loss_fixed\n",
    "\n",
    "keras.losses.focal_loss=keras.losses.MSE\n",
    "keras.losses.focal_loss_fixed=keras.losses.MSE\n",
    "\n",
    "n_gpu = torch.cuda.device_count()\n",
    "torch.cuda.get_device_name(0)\n",
    "\n",
    "if torch.cuda.is_available():     \n",
    "    device = torch.device(\"cuda\")\n",
    "    print('There are',torch.cuda.device_count(),'GPU(s) available.')\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    \n",
    "class Dummy_Embeddings(nn.Module):\n",
    "    def __init__(self, d_model, dummy_vectors): \n",
    "        super(Dummy_Embeddings, self).__init__()\n",
    "        aux = torch.from_numpy(dummy_vectors)#, dtype='float32'))\n",
    "        self.index2dummy = nn.Embedding(aux.size()[0], d_model)\n",
    "        self.index2dummy.weigth=nn.Parameter(aux)\n",
    "        self.index2dummy.weigth.requires_grad=False\n",
    "        self.d_model = d_model\n",
    "        \n",
    "    def forward(self, x):\n",
    "        aux=x.numpy()\n",
    "        #print (\"aux original\", aux)\n",
    "        #print (\"aux restado...\",aux-np.ones(aux.shape))\n",
    "        new_x= aux #- np.ones(aux.shape)\n",
    "        new_x= torch.from_numpy(new_x)\n",
    "        return self.index2dummy(new_x.long()) * math.sqrt(self.d_model) #debiese retornar matriz de batch_size x [ind_tw, k1,k2,k3,k4,k5,k6] (si son 6 modelos)\n",
    "    \n",
    "def match(objetos,ejemplo):\n",
    "    i=0\n",
    "    for obj in objetos:\n",
    "        if obj==ejemplo:\n",
    "            return i\n",
    "        i+=1\n",
    "        \n",
    "def plot_confusion_matrix(cm, target_names, title='Confusion matrix (f1-score)',cmap=None, normalize=True):    \n",
    "    accuracy = np.trace(cm) / float(np.sum(cm))\n",
    "    misclass = 1 - accuracy\n",
    "    if cmap is None:\n",
    "        cmap = plt.get_cmap('Blues')\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap, vmin=0, vmax=1)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    \n",
    "    if target_names is not None:\n",
    "        tick_marks = np.arange(len(target_names))\n",
    "        plt.xticks(tick_marks, target_names, rotation=45)\n",
    "        plt.yticks(tick_marks, target_names)\n",
    "    thresh = cm.max() / 1.5 if normalize else cm.max() / 2\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        if normalize:\n",
    "            plt.text(j, i, \"{:0.4f}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "        else:\n",
    "            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "class EncoderDecoderLogSoft(nn.Module):\n",
    "    def __init__(self, encoder, src_embed, sequential):\n",
    "        super(EncoderDecoderLogSoft, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.linearSoft = sequential\n",
    "        self.src_embed = src_embed\n",
    "        \n",
    "    def forward(self, src, src_mask):\n",
    "        out=self.encode(src, src_mask)\n",
    "        retorno=self.toSoftmax(out)\n",
    "        return retorno\n",
    "    \n",
    "    def encode(self, src, src_mask):\n",
    "        return self.encoder(self.src_embed(src), src_mask)\n",
    "    \n",
    "    def toSoftmax(self, tensor):\n",
    "        a=tensor.size()[0]\n",
    "        b=tensor.size()[-1]\n",
    "        c=tensor.size()[-2]\n",
    "        new_tensor= np.zeros((a, b))\n",
    "        ini=True\n",
    "        for ai in range(a):\n",
    "            for ci in range(c):\n",
    "                if ini:\n",
    "                    new_tensor[ai]=tensor.data[ai][ci].numpy()\n",
    "                    ini=False\n",
    "                else: \n",
    "                    new_tensor[ai]*=tensor.data[ai][ci].numpy()\n",
    "            ini=True\n",
    "        new_tensor=torch.from_numpy(new_tensor)\n",
    "        lineal=self.linearSoft(new_tensor.float())\n",
    "        return F.log_softmax(lineal, dim = -1)\n",
    "    \n",
    "    def decode(self, memory, src_mask, tgt, tgt_mask):\n",
    "        return self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)\n",
    "    \n",
    "def clones(module, N):\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])\n",
    "\n",
    "\n",
    "class EncoderDecoderSoft(nn.Module):\n",
    "    def __init__(self, encoder, src_embed, sequential):\n",
    "        super(EncoderDecoderSoft, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.linearSoft = sequential\n",
    "        self.src_embed = src_embed\n",
    "        \n",
    "    def forward(self, src, src_mask):\n",
    "        out=self.encode(src, src_mask)\n",
    "        retorno=self.toSoftmax(out)\n",
    "        return retorno\n",
    "    \n",
    "    def encode(self, src, src_mask):\n",
    "        return self.encoder(self.src_embed(src), src_mask)\n",
    "    \n",
    "    def toSoftmax(self, tensor):\n",
    "        a=tensor.size()[0]\n",
    "        b=tensor.size()[-1]\n",
    "        c=tensor.size()[-2]\n",
    "        new_tensor= np.zeros((a, b))\n",
    "        ini=True\n",
    "        for ai in range(a):\n",
    "            for ci in range(c):\n",
    "                if ini:\n",
    "                    new_tensor[ai]=tensor.data[ai][ci].numpy()\n",
    "                    ini=False\n",
    "                else: \n",
    "                    new_tensor[ai]*=tensor.data[ai][ci].numpy()\n",
    "            ini=True\n",
    "            \n",
    "        new_tensor=torch.from_numpy(new_tensor)\n",
    "        lineal=self.linearSoft(new_tensor.float())\n",
    "        return F.softmax(lineal, dim = -1)\n",
    "    \n",
    "    def decode(self, memory, src_mask, tgt, tgt_mask):\n",
    "        return self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)\n",
    "    \n",
    "def clones(module, N):\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])\n",
    "\n",
    "class EncoderDecoder(nn.Module):\n",
    "    def __init__(self, encoder, src_embed, generator):\n",
    "        super(EncoderDecoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.src_embed = src_embed\n",
    "        self.generator = generator\n",
    "        \n",
    "    def forward(self, src, src_mask):\n",
    "        return  self.encode(src, src_mask)\n",
    "\n",
    "    def encode(self, src, src_mask):\n",
    "        return self.encoder(self.src_embed(src), src_mask)\n",
    "    \n",
    "    def decode(self, memory, src_mask, tgt, tgt_mask):\n",
    "        return self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)\n",
    "    \n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, layer, N):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.layers = clones(layer, N)\n",
    "        self.norm = LayerNorm(layer.size)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return self.norm(x)\n",
    "    \n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, features, eps=1e-6):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.a_2 = nn.Parameter(torch.ones(features))\n",
    "        self.b_2 = nn.Parameter(torch.zeros(features))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2\n",
    "    \n",
    "class SublayerConnection(nn.Module):\n",
    "    def __init__(self, size, dropout):\n",
    "        super(SublayerConnection, self).__init__()\n",
    "        self.norm = LayerNorm(size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        return x + self.dropout(sublayer(self.norm(x)))    \n",
    "    \n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, size, self_attn, feed_forward, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = self_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer = clones(SublayerConnection(size, dropout), 2)\n",
    "        self.size = size\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))\n",
    "        return self.sublayer[1](x, self.feed_forward)\n",
    "    \n",
    "def subsequent_mask(size):\n",
    "    attn_shape = (1, size, size)\n",
    "    subsequent_mask = np.triu(np.ones(attn_shape), k=1).astype('uint8')\n",
    "    return torch.from_numpy(subsequent_mask) == 0\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, d_model, vocab):\n",
    "        super(Generator, self).__init__()\n",
    "        self.proj = nn.Linear(d_model, vocab)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.log_softmax(self.proj(x), dim=-1)\n",
    "    \n",
    "def attention(query, key, value, mask=None, dropout=None):\n",
    "    d_k = query.size(-1)\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) \\\n",
    "             / math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)\n",
    "    p_attn = F.softmax(scores, dim = -1)\n",
    "    if dropout is not None:\n",
    "        p_attn = dropout(p_attn)\n",
    "    return torch.matmul(p_attn, value), p_attn\n",
    "\n",
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, h, d_model, dropout=0.1):\n",
    "        super(MultiHeadedAttention, self).__init__()\n",
    "        assert d_model % h == 0\n",
    "        self.d_k = d_model // h\n",
    "        self.h = h\n",
    "        self.linears = clones(nn.Linear(d_model, d_model), 4)\n",
    "        self.attn = None\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        #print (\"mask para attn...\", mask)\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1)\n",
    "        nbatches = query.size(0)\n",
    "        \n",
    "        query, key, value = \\\n",
    "            [l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n",
    "             for l, x in zip(self.linears, (query, key, value))]\n",
    "         \n",
    "        x, self.attn = attention(query, key, value, mask=mask, \n",
    "                                 dropout=self.dropout)\n",
    "        \n",
    "        x = x.transpose(1, 2).contiguous() \\\n",
    "             .view(nbatches, -1, self.h * self.d_k)\n",
    "        return self.linears[-1](x)\n",
    "    \n",
    "class PositionalText(nn.Module):\n",
    "    def __init__(self, d_model, dropout, max_len=50):\n",
    "        super(PositionalText, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x_text=x.data[:,0][:,None,:]\n",
    "        x_machines=x.data[:,1:]\n",
    "        \n",
    "        x_return = x_machines* x_text\n",
    "        return self.dropout(x_return)\n",
    "    \n",
    "class PositionalText_concat(nn.Module):\n",
    "    def __init__(self, d_model, dropout, max_len=50):\n",
    "        super(PositionalText_concat, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.dropout(x)   \n",
    "\n",
    "    \n",
    "global max_src_in_batch, max_tgt_in_batch\n",
    "\n",
    "def batch_size_fn(new, count, sofar):\n",
    "    global max_src_in_batch, max_tgt_in_batch\n",
    "    if count == 1:\n",
    "        max_src_in_batch = 0\n",
    "        max_tgt_in_batch = 0\n",
    "    max_src_in_batch = max(max_src_in_batch,  len(new.src))\n",
    "    max_tgt_in_batch = max(max_tgt_in_batch,  len(new.trg) + 2)\n",
    "    src_elements = count * max_src_in_batch\n",
    "    tgt_elements = count * max_tgt_in_batch\n",
    "    maximo=max(src_elements, tgt_elements)\n",
    "    return maximo\n",
    "\n",
    "class NoamOpt:\n",
    "    \"Optim wrapper that implements rate.\"\n",
    "    def __init__(self, model_size, factor, warmup, optimizer):\n",
    "        self.optimizer = optimizer\n",
    "        self._step = 0\n",
    "        self.warmup = warmup\n",
    "        self.factor = factor\n",
    "        self.model_size = model_size\n",
    "        self._rate = 0\n",
    "        \n",
    "    def step(self):\n",
    "        \"Update parameters and rate\"\n",
    "        self._step += 1\n",
    "        rate = self.rate()\n",
    "        for p in self.optimizer.param_groups:\n",
    "            p['lr'] = rate\n",
    "        self._rate = rate\n",
    "        #print (\"Rate de Noam_opt\", self._rate, self._step)\n",
    "        self.optimizer.step()\n",
    "        \n",
    "    def rate(self, step = None):\n",
    "        \"Implement `lrate` above\"\n",
    "        if step is None:\n",
    "            step = self._step\n",
    "        return self.factor * \\\n",
    "            (self.model_size ** (-0.5) *\n",
    "            min(step ** (-0.5), step * self.warmup ** (-1.5)))\n",
    "        \n",
    "def get_std_opt(model):\n",
    "    return NoamOpt(model.src_embed[0].d_model, 2, 4000,\n",
    "            torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9))\n",
    "\n",
    "class LabelSmoothing(nn.Module):\n",
    "    \"Implement label smoothing.\"\n",
    "    def __init__(self, size, padding_idx, smoothing=0.0):\n",
    "        super(LabelSmoothing, self).__init__()\n",
    "        self.criterion = nn.KLDivLoss(size_average=False)\n",
    "        self.padding_idx = padding_idx\n",
    "        self.confidence = 1.0 - smoothing\n",
    "        self.smoothing = smoothing\n",
    "        self.size = size\n",
    "        self.true_dist = None\n",
    "        \n",
    "    def forward(self, x, target):\n",
    "        assert x.size(1) == self.size\n",
    "        true_dist = x.data.clone()\n",
    "        true_dist.fill_(self.smoothing / (self.size - 2))\n",
    "        true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
    "        mask = torch.nonzero(target.data == self.padding_idx)\n",
    "        self.true_dist = true_dist\n",
    "        return self.criterion(x, Variable(true_dist, requires_grad=False))\n",
    "    \n",
    "class LabelCCE(nn.Module):\n",
    "    def __init__(self, class_weights):\n",
    "        super(LabelCCE, self).__init__()\n",
    "        self.criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "                       \n",
    "    def forward(self, x, target):\n",
    "        return self.criterion(x, target)\n",
    "    \n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, weights, gamma=2.0, reduce=True):#, logits=False, reduce=True):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.weights = weights\n",
    "        self.gamma = gamma\n",
    "        #self.logits = logits\n",
    "        self.reduce = reduce\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        tensors = []\n",
    "        for l in y:\n",
    "            tensors.append(torch.zeros(4).scatter_(0, torch.tensor(l), 1))\n",
    "        result = torch.stack(tensors, 0)\n",
    "        \n",
    "        CCE_loss = F.cross_entropy(x, result.long().argmax(dim=-1), reduction='none', weight=self.weights.float())\n",
    "        pt = torch.exp(-CCE_loss)\n",
    "        factor= (1-pt)**self.gamma * CCE_loss\n",
    "        \n",
    "        F_loss = factor  #*self.weights.float()-- peso esta dentro de la funcion\n",
    "\n",
    "        if self.reduce:\n",
    "            return torch.mean(F_loss)\n",
    "        else:\n",
    "            return F_loss\n",
    "        \n",
    "class SimpleLossComputeFL:\n",
    "    \"A simple loss compute and train function.\"\n",
    "    def __init__(self, criterion, opt=None):\n",
    "        self.criterion = criterion\n",
    "        self.opt = opt\n",
    "        \n",
    "    def __call__(self, x, y, norm, mode):\n",
    "        y_new=(y.float()-torch.ones((y.shape))).int() \n",
    "        #print (\"nuevo Y en simplelosscompute\", y_new, \"antes era\", y)\n",
    "        # ahora\n",
    "        #y_new=(y.float()).int()\n",
    "        ac=accuracy_scorer(x, y_new)\n",
    "        f1=f_scorer(x, y_new)\n",
    "        cm=compute_confusion_matrix(x, y_new)\n",
    "        tempa= x.contiguous().view(-1, x.size(-1))\n",
    "        tempb= y_new.long().contiguous().view(-1)\n",
    "        loss = self.criterion(tempa, tempb) #/ norm\n",
    "        if mode!='Test':\n",
    "            loss.backward()\n",
    "        if self.opt is not None:\n",
    "            self.opt.step()\n",
    "            self.opt.optimizer.zero_grad()\n",
    "        return [loss.data, ac ,f1, cm] #*norm #[0] * norm\n",
    "    \n",
    "class PositionwiseFeedForward(nn.Module):\n",
    "    \"Implements FFN equation.\"\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.w_1 = nn.Linear(d_model, d_ff)\n",
    "        self.w_2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.w_2(self.dropout(F.relu(self.w_1(x))))\n",
    "    \n",
    "def accuracy_scorer(predicho, target):\n",
    "    predicho= torch.argmax(predicho, dim=1)\n",
    "    return accuracy_score(target.numpy(),predicho.numpy())\n",
    "\n",
    "def f_scorer(predicho, target):\n",
    "    predicho= torch.argmax(predicho, dim=1)\n",
    "    return f1_score(target.numpy(),predicho.numpy(), average=None, labels=[0,1,2,3])\n",
    "\n",
    "def compute_confusion_matrix(predicho, target):\n",
    "    predicho= torch.argmax(predicho, dim=1)\n",
    "    return confusion_matrix(target.numpy(), predicho.numpy(), labels=[0,1,2,3])\n",
    "\n",
    "def data_gen(batch_size,mode,n_machines,new_matrix):  \n",
    "    \"\"\"ESTA FUNCION DEBERÁ RETORNAR MATRICES DE TAMAÑO BATCH_SIZE * TW,M1,M2,M3,M4,M5,M6\n",
    "    CADA EJEMPLO DEBERÁ SER ESTILO [ID_T, 1,2,1,2,1,2] SIENDO 1 CUANDO LA CLASE PREDICHA ES LA 0\n",
    "    2 PARA CLASE PREDICHA 1, ETC. ID_T DEBEŔA INICIAR EN 5 PARA REFERIRSE AL PRIMER TW (EL PRIMERO DE TRAIN)\"\"\"\n",
    "    x_data=[] \n",
    "    y_data=[]\n",
    "    if mode=='Train':\n",
    "        max_objs=matrix.shape[1]\n",
    "        x_list=np.arange(max_objs)\n",
    "        np.random.shuffle(x_list)        #indices desordenados de seccion train\n",
    "        objs=ids_train\n",
    "        labels=etiq     #np.load(\"matrices/y_train_num_4.npy\")\n",
    "        labels=np.asarray(labels) #+ np.ones(labels.shape),dtype='uint8')\n",
    "    if mode=='Val':\n",
    "        max_objs=matrix_val.shape[1] \n",
    "        x_list=np.arange(max_objs)\n",
    "        x_list=[x+matrix.shape[1] for x in x_list]   \n",
    "        np.random.shuffle(x_list)        #indices desordenados de seccion val\n",
    "        objs=val['Tw_id']    #df_val['Unnamed: 0'].values        \n",
    "        labels=np.asarray(etiq_v)     #np.load(\"matrices/y_val_num_4.npy\")\n",
    "        labels=np.asarray(labels) #+ np.ones(labels.shape),dtype='uint8') \n",
    "    if mode=='Test':\n",
    "        max_objs=matrix_test.shape[1] \n",
    "        x_list=np.arange(max_objs)\n",
    "        x_list=[x+matrix.shape[1]+matrix_val.shape[1] for x in x_list]\n",
    "        total=matrix.shape[1]+matrix_val.shape[1]\n",
    "        objs=test['Tw_id']     #np.arange(total, total+max_objs)\n",
    "        \n",
    "    #----------------------------------- COMPLETA MINI BATCHES ----------------------    \n",
    "    if len(x_list)%batch_size==0:\n",
    "        n_batches=len(x_list)/batch_size\n",
    "        for nb in range(n_batches): \n",
    "            x_data.append(x_list[nb*batch_size:(nb+1)*batch_size])\n",
    "    else:\n",
    "        n_batches=int(len(x_list)/batch_size)\n",
    "        resto=int(len(x_list)-n_batches*batch_size)\n",
    "        to_repeat=batch_size-resto        \n",
    "        for nb in range(n_batches): \n",
    "            x_data.append(x_list[nb*batch_size:(nb+1)*batch_size])\n",
    "        cola=list(x_list[(nb+1)*batch_size:])\n",
    "        for i in range(to_repeat):\n",
    "            indice= np.random.randint(len(x_list))\n",
    "            cola.append(x_list[indice])\n",
    "        x_data.append(np.asarray(cola))\n",
    "        if mode=='Test':\n",
    "            print (\"SE TUVIERON QUE REPETIR\", to_repeat, \"EJEMPLOS\")\n",
    "    #print (\"los indices de train k voy a acceder random son\", x_data[0])\n",
    "            \n",
    "    #-------------------------------- CONSTRUCCION MINI BATCHES (X,Y)----------------------   \n",
    "    new_x_data=[] \n",
    "    for batch in x_data:\n",
    "        #print (\"transformando batch\", batch)\n",
    "        temp=[]\n",
    "        for pos in batch:\n",
    "            #print(\"en especifico el numerito\", pos)\n",
    "            temp2=[index_embeddings[pos+5]]#fromId2num[objs[exam]]] ### obj evaluado en maquinas  \n",
    "            #print (\"me dice que es el embed en index\", temp2)\n",
    "            for m in range(n_machines): \n",
    "                #print (\"machine \",m,\"dice:\", np.argmax(new_matrix[m][pos])+1)\n",
    "                temp2= temp2 + [np.argmax(new_matrix[m][pos])+1] \n",
    "                #matrix[4][cosa-5])+1)\n",
    "            temp.append(temp2)\n",
    "        new_x_data.append(temp)   \n",
    "\n",
    "    if mode=='Train' or mode=='Val':    \n",
    "        y_data=[]\n",
    "        for conjunto in range(len(new_x_data)):\n",
    "            batch=new_x_data[conjunto]\n",
    "            temp_y=[]\n",
    "            for linea in range(len(batch)):\n",
    "                indice=x_data[conjunto][linea]  #pos\n",
    "                lab=d_lab[lab_str_embeddings[indice+5]]+1 #labels_train\n",
    "                temp_y.append([lab])       \n",
    "            y_data.append(temp_y)\n",
    "            \n",
    "        y_data=np.asarray(y_data)\n",
    "        \n",
    "    new_x_data=np.asarray(new_x_data)\n",
    "    #print (\"primer batch X.Y\", new_x_data[0], y_data[0])\n",
    "    if mode=='Test':\n",
    "        for x in new_x_data:\n",
    "            src = Variable(torch.from_numpy(np.asarray(x)), requires_grad=False)\n",
    "            yield Batch(src, None, 0)        \n",
    "    else:          \n",
    "        for x,y in zip(new_x_data, y_data):\n",
    "            src = Variable(torch.from_numpy(np.asarray(x)), requires_grad=False)\n",
    "            har = Variable(torch.from_numpy(np.asarray(y)), requires_grad=False)\n",
    "            yield Batch(src, har, 0)\n",
    "            \n",
    "\n",
    "def make_model_concat(target, N=2, d_model=768, d_ff=1024, h=4, dropout=0.3, soft=True):  #recibir src_vocab si lo utiliza dummy_Emb\n",
    "    c = copy.deepcopy\n",
    "    attn = MultiHeadedAttention(h, d_model)\n",
    "    ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "    position = PositionalText_concat(d_model, dropout)\n",
    "    if soft:\n",
    "        print (\"Creando modelo con salida Softmax\")\n",
    "        model = EncoderDecoderSoft(\n",
    "            Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N),\n",
    "            nn.Sequential(Dummy_Embeddings(768, K_list_new), c(position)),\n",
    "            nn.Linear(d_model, 4))\n",
    "    else:\n",
    "        print (\"\")\n",
    "        print (\"Creando modelo con salida Log_softmax\")\n",
    "        model = EncoderDecoderLogSoft(\n",
    "            Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N),\n",
    "            nn.Sequential(Dummy_Embeddings(768, K_list_new), c(position)),\n",
    "            nn.Linear(d_model, 4))\n",
    "\n",
    "    for p in model.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform(p)\n",
    "    return model\n",
    "\n",
    "class Batch:\n",
    "    def __init__(self, src, trg=None, pad=0):\n",
    "        self.src = src\n",
    "        self.src_mask = (src != pad).unsqueeze(-2)\n",
    "        if trg is not None:\n",
    "            self.trg = trg[:, :]\n",
    "            self.trg_y = trg[:, 0:]\n",
    "            self.trg_mask = \\\n",
    "                self.make_std_mask(self.trg, pad)\n",
    "            self.ntokens = (self.trg_y != pad).data.sum()\n",
    "            \n",
    "    @staticmethod\n",
    "    def make_std_mask(tgt, pad):\n",
    "        tgt_mask = (tgt != pad).unsqueeze(-2)\n",
    "        tgt_mask = tgt_mask & Variable(\n",
    "            subsequent_mask(tgt.size(-1)).type_as(tgt_mask.data))\n",
    "        return tgt_mask\n",
    "    \n",
    "def run_epoch(data_iter, model, loss_compute, mode):\n",
    "    if mode=='Test':\n",
    "        with torch.no_grad():\n",
    "            start = time.time()\n",
    "            pred_classes=[]\n",
    "            for i, batch in enumerate(data_iter):\n",
    "                #print (\"batch.src_mask en run epoch\", batch.src_mask)\n",
    "                out = model.forward(batch.src, batch.src_mask)\n",
    "                pred_classes.append(out.argmax(dim=-1).numpy())\n",
    "                #la loss del batch es sobre todos los ejemplos o es promedio en batch?\n",
    "            return pred_classes\n",
    "    \n",
    "    else:\n",
    "        start = time.time()\n",
    "        total_learners = 0\n",
    "        total_datos = 0\n",
    "        total_loss = 0\n",
    "        total_acc=0\n",
    "        total_fm1=0\n",
    "        total_fs=0\n",
    "        n_batches = 0\n",
    "        cm_last=np.zeros((4,4))\n",
    "        for i, batch in enumerate(data_iter):\n",
    "            #print (\"BATCH\\n\", i, '\\n', batch.src)\n",
    "            out = model.forward(batch.src, batch.src_mask)\n",
    "            loss,acc,f1,cm = loss_compute(out, batch.trg_y, batch.ntokens, mode)\n",
    "            cm_last+=cm\n",
    "            f_ma=np.mean(f1)\n",
    "            total_loss += loss\n",
    "            total_acc += np.array(acc)\n",
    "            total_fm1 += np.sum(f1)/4.0    #4 clases (macro)\n",
    "            total_fs += f1                 #none  \n",
    "            total_datos += batch.ntokens\n",
    "            n_batches += 1\n",
    "        return total_loss/n_batches, torch.from_numpy(np.array(total_acc))/n_batches, torch.from_numpy(np.array(total_fm1))/n_batches, torch.from_numpy(np.array(total_fs))/n_batches, cm_last\n",
    "\n",
    "emoji_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "\n",
    "def cleaner(post): \n",
    "    #re.sub(r'([a-z])\\1+', r'\\1', \"user i think that ' s all you loooooove \")\n",
    "    s= re.sub(r\"http\\S+\", \" html \", post)   ##########elimina http    \n",
    "    s= re.sub(r'#\\w+ ?', \" hashtag \", s) ########3\n",
    "    s= re.sub(r'@\\w+ ?', \" user \", s)##############    \n",
    "    s= s.lower()\n",
    "    s=emoji_pattern.sub(r'', s)\n",
    "    s=re.sub(r\"'\\b\", \" ' \", s)\n",
    "    s=re.sub(r\"\\b'\\b\", \" ' \", s)\n",
    "    s=re.sub(r\"\\b’\", \" ’ \", s)\n",
    "    s=re.sub(r\"‘\\b\", \" ‘ \", s)\n",
    "    s=re.sub(r\"\\b’\\b\", \" ‘ \", s)\n",
    "    s = re.sub(r\"-\", \" - \", s)\n",
    "    s = re.sub(r\"\\(\", \" \", s)\n",
    "    s = re.sub(r\"\\)\", \" \", s)\n",
    "    s = re.sub(r\"\\?\", \" ? \", s)    \n",
    "    s = re.sub(r\"\\/\", \" \", s)\n",
    "    s = re.sub(r\"' \", \" ' \", s)\n",
    "    s = re.sub(r\" '\", \" ' \", s)\n",
    "    s = re.sub(r\"\\!\", \" ! \", s)\n",
    "    s=re.sub(\"[\\.]+\", \" . \", s)\n",
    "    s=re.sub(\"[\\,]+\", \" , \", s)\n",
    "    s=re.sub(\"[\\;]+\", \" ; \", s)\n",
    "    s=re.sub(\"[\\:]+\", \" : \", s)\n",
    "    s=re.sub('[\\\"]+', ' \" ', s)\n",
    "    s=re.sub(r'\\b[0-9]\\b', \" number \",  s)\n",
    "    s=re.sub(r'\\b[0-9]*[0-9]\\b', \" number \",  s)    \n",
    "    s=re.sub(r'\\b”', ' \" ', s)\n",
    "    sl= list(s.split())\n",
    "    #sl= [stemmer.stem(wd) for wd in sl]\n",
    "    ############3\n",
    "    sl_2=[]\n",
    "    for wd in sl:\n",
    "        try: \n",
    "            q=Word2Index[wd]\n",
    "            sl_2.append(wd)\n",
    "        except:\n",
    "            try: \n",
    "                r=Word2Index[stemmer.stem(wd)]\n",
    "                sl_2.append(stemmer.stem(wd))\n",
    "            except:\n",
    "                sl_2.append(wd)\n",
    "    sl=sl_2\n",
    "    ##########\n",
    "    s=' '.join([word for word in sl])# if word not in stoplist])\n",
    "    return s, sl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5fbe136dc2f4b4592de9419d0715270",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=433.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pesos de clases: [1. 1. 1. 1.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1021, 50, 768)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "dict_cl=dict()\n",
    "dict_cl[0]='support'\n",
    "dict_cl[1]='denying'\n",
    "dict_cl[2]='questioning'\n",
    "dict_cl[3]='commenting'\n",
    "#sdqc\n",
    "d_lab=dict()\n",
    "d_lab[\"support\"]=0\n",
    "d_lab[\"denying\"]=1\n",
    "d_lab[\"questioning\"]=2\n",
    "d_lab[\"commenting\"]=3\n",
    "\n",
    "\n",
    "tokenizer_loaded = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model_loaded = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=4)#'bert-base-uncased',  num_labels=4) \n",
    "idx_2_token = tokenizer_loaded.ids_to_tokens\n",
    "Word2Index=tokenizer_loaded.vocab\n",
    "M_BERT_space = model_loaded.bert.embeddings.word_embeddings.weight.detach().cpu().numpy()\n",
    "transformer = Normalizer().fit(M_BERT_space) \n",
    "M_BERT_space=transformer.transform(M_BERT_space)\n",
    "\n",
    "train=pd.read_csv(\"../../Fine-Tuning/CSV_Stance/train_semeval_raw.csv\")\n",
    "val=pd.read_csv(\"../../Fine-Tuning/CSV_Stance/dev_semeval_raw.csv\")\n",
    "test=pd.read_csv(\"../../Fine-Tuning/CSV_Stance/test_semeval_raw.csv\")\n",
    "\n",
    "##########################################################\n",
    "class_weights= [1.0, 1.0, 1.0, 1.0]\n",
    "class_weights=np.asarray(class_weights)\n",
    "print (\"Pesos de clases:\", class_weights)\n",
    "\n",
    "MAX_LEN=50\n",
    "#########################################################\n",
    "\n",
    "\n",
    "not_found=[]\n",
    "found=[]\n",
    "found_train=[]\n",
    "pos_tag_nf=[]\n",
    "\n",
    "MAX_LEN=50\n",
    "\n",
    "etiq=[]\n",
    "for et in train['Label'].values:\n",
    "    etiq.append(d_lab[et])\n",
    "n_labels = np.array(etiq)\n",
    "y_train=to_categorical(n_labels,num_classes=4)\n",
    "sentences = train['Tweet'].values\n",
    "input_ids = []\n",
    "i=0\n",
    "\n",
    "for sent in sentences:\n",
    "    temp=cleaner(sent)\n",
    "    encoded_sent2 = tokenizer_loaded.encode(temp[0],add_special_tokens = False) \n",
    "    encoded_sent=[]\n",
    "    iterar=temp[1]\n",
    "    for wd in iterar:\n",
    "        try:\n",
    "            indice=Word2Index[wd]\n",
    "            encoded_sent.append(wd)\n",
    "            if wd not in found:\n",
    "                found.append(wd)\n",
    "                found_train.append(wd)\n",
    "        except: \n",
    "            if wd not in not_found:\n",
    "                not_found.append(wd)\n",
    "                pos_tag_nf.append(nltk.pos_tag([wd])[0][-1])\n",
    "\n",
    "    input_ids.append(encoded_sent2)\n",
    "\n",
    "\n",
    "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", value=0, truncating=\"post\", padding=\"post\")\n",
    "\n",
    "etiq_v=[]\n",
    "for et in val['Label'].values:\n",
    "    etiq_v.append(d_lab[et])\n",
    "n_labels_val = np.array(etiq_v)\n",
    "y_val=to_categorical(n_labels_val,num_classes=4)\n",
    "sentences_val = val['Tweet'].values\n",
    "input_ids_val = []\n",
    "i=0\n",
    "for sent in sentences_val:\n",
    "    temp=cleaner(sent)\n",
    "    encoded_sent2 = tokenizer_loaded.encode(temp[0],add_special_tokens = False) \n",
    "    encoded_sent=[]\n",
    "    iterar=temp[1]\n",
    "    for wd in iterar:\n",
    "        try:\n",
    "            indice=Word2Index[wd]\n",
    "            encoded_sent.append(wd)\n",
    "            if wd not in found: \n",
    "                found.append(wd)\n",
    "        except: \n",
    "            if wd not in not_found:\n",
    "                not_found.append(wd)\n",
    "                pos_tag_nf.append(nltk.pos_tag([wd])[0][-1])\n",
    "            \n",
    "    input_ids_val.append(encoded_sent2)\n",
    "\n",
    "    \n",
    "input_ids_val = pad_sequences(input_ids_val, maxlen=MAX_LEN, dtype=\"long\", value=0, truncating=\"post\", padding=\"post\")\n",
    "\n",
    "\n",
    "etiq_t=[]\n",
    "for et in test['Label'].values:\n",
    "    etiq_t.append(d_lab[et])\n",
    "n_labels_test = np.array(etiq_t)\n",
    "y_test=to_categorical(n_labels_test,num_classes=4)\n",
    "sentences_test = test['Tweet'].values\n",
    "input_ids_test = []\n",
    "i=0\n",
    "for sent in sentences_test:\n",
    "    temp=cleaner(sent)\n",
    "    #encoded_sent=[Word2Index[wd] for wd in temp]\n",
    "    encoded_sent2 = tokenizer_loaded.encode(temp[0],add_special_tokens = False) \n",
    "    encoded_sent=[]\n",
    "    iterar=temp[1]\n",
    "    for wd in iterar:\n",
    "        try:\n",
    "            indice=Word2Index[wd]\n",
    "            encoded_sent.append(wd)\n",
    "            if wd not in found:\n",
    "                found.append(wd)            \n",
    "        except: \n",
    "            if wd not in not_found:\n",
    "                #print (\"not found\", wd)\n",
    "                not_found.append(wd)\n",
    "                pos_tag_nf.append(nltk.pos_tag([wd])[0][-1])\n",
    "            \n",
    "    input_ids_test.append(encoded_sent2)\n",
    "\n",
    "\n",
    "input_ids_test = pad_sequences(input_ids_test, maxlen=MAX_LEN, dtype=\"long\", value=0, truncating=\"post\", padding=\"post\")\n",
    "\n",
    "\n",
    "shape_val=np.asarray(input_ids_val).shape\n",
    "x_val=np.zeros((shape_val[0], shape_val[1], 768))\n",
    "i=0\n",
    "for in_id in input_ids_val:\n",
    "    x_val[i]=M_BERT_space[in_id]\n",
    "    i+=1\n",
    "    \n",
    "shape_test=np.asarray(input_ids_test).shape\n",
    "x_test=np.zeros((shape_test[0], shape_test[1], 768))\n",
    "i=0\n",
    "for in_id in input_ids_test:\n",
    "    x_test[i]=M_BERT_space[in_id]\n",
    "    i+=1\n",
    "\n",
    "x_test.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:newpy3_tf1]",
   "language": "python",
   "name": "conda-env-newpy3_tf1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
