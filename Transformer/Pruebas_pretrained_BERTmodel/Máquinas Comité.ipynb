{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/casapanshop/anaconda2/envs/newpy3_tf1/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/casapanshop/anaconda2/envs/newpy3_tf1/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/casapanshop/anaconda2/envs/newpy3_tf1/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/casapanshop/anaconda2/envs/newpy3_tf1/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/casapanshop/anaconda2/envs/newpy3_tf1/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/casapanshop/anaconda2/envs/newpy3_tf1/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/casapanshop/anaconda2/envs/newpy3_tf1/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/casapanshop/anaconda2/envs/newpy3_tf1/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/casapanshop/anaconda2/envs/newpy3_tf1/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/casapanshop/anaconda2/envs/newpy3_tf1/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/casapanshop/anaconda2/envs/newpy3_tf1/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/casapanshop/anaconda2/envs/newpy3_tf1/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: GeForce GTX 1060 6GB\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "from collections import Counter\n",
    "from gensim import utils, matutils \n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.utils import deprecated\n",
    "import itertools\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "from numpy import linalg as LA\n",
    "from numpy.random import binomial\n",
    "from numpy import dot, float32 as REAL, memmap as np_memmap, \\\n",
    "    double, array, zeros, vstack, sqrt, newaxis, integer, \\\n",
    "    ndarray, sum as np_sum, prod, argmax\n",
    "from numpy.linalg import norm\n",
    "import math, copy, time\n",
    "import warnings \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_context(context=\"talk\")\n",
    "%matplotlib inline\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras import backend as K\n",
    "import keras\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import load_model\n",
    "\n",
    "import os, sys, re, io, nltk, torch\n",
    "import pandas as pd\n",
    "\n",
    "from scipy import stats\n",
    "from scipy.stats import bernoulli\n",
    "from scipy.spatial import distance\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.preprocessing import Normalizer, normalize\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.metrics import accuracy_score,f1_score, confusion_matrix, recall_score, precision_score\n",
    "\n",
    "from six import string_types, integer_types\n",
    "from six.moves import zip, range\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "\n",
    "from tqdm import tqdm, trange\n",
    "import time\n",
    "import transformers\n",
    "from transformers import (\n",
    "    WEIGHTS_NAME,\n",
    "    AdamW,\n",
    "    BertConfig,\n",
    "    BertForMaskedLM,\n",
    "    BertForSequenceClassification,\n",
    "    BertTokenizer,\n",
    "    CamembertConfig,\n",
    "    CamembertForMaskedLM,\n",
    "    CamembertTokenizer,\n",
    "    DistilBertConfig,\n",
    "    DistilBertForMaskedLM,\n",
    "    DistilBertTokenizer,\n",
    "    GPT2Config,\n",
    "    GPT2LMHeadModel,\n",
    "    GPT2Tokenizer,\n",
    "    OpenAIGPTConfig,\n",
    "    OpenAIGPTLMHeadModel,\n",
    "    OpenAIGPTTokenizer,\n",
    "    RobertaConfig,\n",
    "    RobertaForMaskedLM,\n",
    "    RobertaTokenizer,\n",
    "    get_linear_schedule_with_warmup,\n",
    ")\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.autograd import Variable\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import WordNetLemmatizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "\n",
    "stemmer_sn = SnowballStemmer(\"english\")\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "stoplist = stopwords.words(\"english\")\n",
    "lemmatizer=WordNetLemmatizer()\n",
    "\n",
    "\n",
    "'''\n",
    "Compatible with tensorflow backend\n",
    "gamma entre más alto más tolerante (queremos un gamma chico pero no demasiado! 0.25 - 5)\n",
    "alpha pesos por clase (weights)\n",
    "'''\n",
    "def focal_loss(gamma=2., weights=1):   #weights np.asarray()\n",
    "    weights= K.variable(weights)\n",
    "    def focal_loss_fixed(y_true, y_pred):\n",
    "        y_true = K.clip(y_true, K.epsilon(),1)\n",
    "        y_pred = K.clip(y_pred,K.epsilon(),1)\n",
    "        return - K.sum(weights* K.pow(1. - y_pred, gamma)* y_true * K.log(y_pred), axis=-1) \n",
    "    return focal_loss_fixed\n",
    "\n",
    "keras.losses.focal_loss=keras.losses.MSE\n",
    "keras.losses.focal_loss_fixed=keras.losses.MSE\n",
    "\n",
    "n_gpu = torch.cuda.device_count()\n",
    "torch.cuda.get_device_name(0)\n",
    "\n",
    "if torch.cuda.is_available():     \n",
    "    device = torch.device(\"cuda\")\n",
    "    print('There are',torch.cuda.device_count(),'GPU(s) available.')\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    \n",
    "class Dummy_Embeddings(nn.Module):\n",
    "    def __init__(self, d_model, dummy_vectors): \n",
    "        super(Dummy_Embeddings, self).__init__()\n",
    "        aux = torch.from_numpy(dummy_vectors)#, dtype='float32'))\n",
    "        self.index2dummy = nn.Embedding(aux.size()[0], d_model)\n",
    "        self.index2dummy.weigth=nn.Parameter(aux)\n",
    "        self.index2dummy.weigth.requires_grad=False\n",
    "        self.d_model = d_model\n",
    "        \n",
    "    def forward(self, x):\n",
    "        aux=x.numpy()\n",
    "        #print (\"aux original\", aux)\n",
    "        #print (\"aux restado...\",aux-np.ones(aux.shape))\n",
    "        new_x= aux #- np.ones(aux.shape)\n",
    "        new_x= torch.from_numpy(new_x)\n",
    "        return self.index2dummy(new_x.long()) * math.sqrt(self.d_model) #debiese retornar matriz de batch_size x [ind_tw, k1,k2,k3,k4,k5,k6] (si son 6 modelos)\n",
    "    \n",
    "def match(objetos,ejemplo):\n",
    "    i=0\n",
    "    for obj in objetos:\n",
    "        if obj==ejemplo:\n",
    "            return i\n",
    "        i+=1\n",
    "        \n",
    "def plot_confusion_matrix(cm, target_names, title='Confusion matrix (f1-score)',cmap=None, normalize=True):    \n",
    "    accuracy = np.trace(cm) / float(np.sum(cm))\n",
    "    misclass = 1 - accuracy\n",
    "    if cmap is None:\n",
    "        cmap = plt.get_cmap('Blues')\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap, vmin=0, vmax=1)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    \n",
    "    if target_names is not None:\n",
    "        tick_marks = np.arange(len(target_names))\n",
    "        plt.xticks(tick_marks, target_names, rotation=45)\n",
    "        plt.yticks(tick_marks, target_names)\n",
    "    thresh = cm.max() / 1.5 if normalize else cm.max() / 2\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        if normalize:\n",
    "            plt.text(j, i, \"{:0.4f}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "        else:\n",
    "            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))\n",
    "    plt.show()\n",
    "    \n",
    "emoji_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "\n",
    "def cleaner(post): \n",
    "    #re.sub(r'([a-z])\\1+', r'\\1', \"user i think that ' s all you loooooove \")\n",
    "    s= re.sub(r\"http\\S+\", \" html \", post)   ##########elimina http    \n",
    "    s= re.sub(r'#\\w+ ?', \" hashtag \", s) ########3\n",
    "    s= re.sub(r'@\\w+ ?', \" user \", s)##############    \n",
    "    s= s.lower()\n",
    "    s=emoji_pattern.sub(r'', s)\n",
    "    s=re.sub(r\"'\\b\", \" ' \", s)\n",
    "    s=re.sub(r\"\\b'\\b\", \" ' \", s)\n",
    "    s=re.sub(r\"\\b’\", \" ’ \", s)\n",
    "    s=re.sub(r\"‘\\b\", \" ‘ \", s)\n",
    "    s=re.sub(r\"\\b’\\b\", \" ‘ \", s)\n",
    "    s = re.sub(r\"-\", \" - \", s)\n",
    "    s = re.sub(r\"\\(\", \" \", s)\n",
    "    s = re.sub(r\"\\)\", \" \", s)\n",
    "    s = re.sub(r\"\\?\", \" ? \", s)    \n",
    "    s = re.sub(r\"\\/\", \" \", s)\n",
    "    s = re.sub(r\"' \", \" ' \", s)\n",
    "    s = re.sub(r\" '\", \" ' \", s)\n",
    "    s = re.sub(r\"\\!\", \" ! \", s)\n",
    "    s=re.sub(\"[\\.]+\", \" . \", s)\n",
    "    s=re.sub(\"[\\,]+\", \" , \", s)\n",
    "    s=re.sub(\"[\\;]+\", \" ; \", s)\n",
    "    s=re.sub(\"[\\:]+\", \" : \", s)\n",
    "    s=re.sub('[\\\"]+', ' \" ', s)\n",
    "    s=re.sub(r'\\b[0-9]\\b', \" number \",  s)\n",
    "    s=re.sub(r'\\b[0-9]*[0-9]\\b', \" number \",  s)    \n",
    "    s=re.sub(r'\\b”', ' \" ', s)\n",
    "    sl= list(s.split())\n",
    "    #sl= [stemmer.stem(wd) for wd in sl]\n",
    "    ############3\n",
    "    sl_2=[]\n",
    "    for wd in sl:\n",
    "        try: \n",
    "            q=Word2Index[wd]\n",
    "            sl_2.append(wd)\n",
    "        except:\n",
    "            try: \n",
    "                r=Word2Index[stemmer.stem(wd)]\n",
    "                sl_2.append(stemmer.stem(wd))\n",
    "            except:\n",
    "                sl_2.append(wd)\n",
    "    sl=sl_2\n",
    "    ##########\n",
    "    s=' '.join([word for word in sl])# if word not in stoplist])\n",
    "    return s, sl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_data(trained, x_train, x_val, x_test, etiq, etiq_v, etiq_t, name_model):\n",
    "    etiq = etiq.astype(\"int\")\n",
    "    trainPredict = trained.predict(x_train, batch_size=32)\n",
    "    trainPredict=[np.argmax(pred) for pred in trainPredict]\n",
    "    acc= accuracy_score(etiq, trainPredict)  \n",
    "    f1=f1_score(etiq, trainPredict, average=None)  # labels=np.unique(trainPredict)\n",
    "    f1_ma=f1_score(etiq, trainPredict, average='macro')  # labels=np.unique(trainPredict)\n",
    "    #matriz=normalize(confusion_matrix(etiq, trainPredict))\n",
    "    print (\"\")\n",
    "    print (\"Accuracy sobre Train\", name_model, \":\",acc)  \n",
    "    print (\"F1-score None sobre Train\", name_model, \":\",f1)\n",
    "    print (\"F1-score macro sobre Train\", name_model, \":\",f1_ma)\n",
    "    ########################################\n",
    "    trainPredict = trained.predict(x_val, batch_size=32)\n",
    "    trainPredict=[np.argmax(pred) for pred in trainPredict]\n",
    "    acc= accuracy_score(etiq_v, trainPredict)  \n",
    "    f1=f1_score(etiq_v, trainPredict, average=None)  # labels=np.unique(trainPredict)\n",
    "    f1_ma=f1_score(etiq_v, trainPredict, average='macro')  # labels=np.unique(trainPredict)\n",
    "    #matriz=normalize(confusion_matrix(etiq_v, trainPredict))\n",
    "    print (\"\")\n",
    "    print (\"Accuracy sobre Val\", name_model, \":\",acc)  \n",
    "    print (\"F1-score None sobre Val\", name_model, \":\",f1)\n",
    "    print (\"F1-score macro sobre Val\", name_model, \":\",f1_ma)\n",
    "    ########################################\n",
    "    trainPredict = trained.predict(x_test, batch_size=32)\n",
    "    trainPredict=[np.argmax(pred) for pred in trainPredict]\n",
    "    acc_t= accuracy_score(etiq_t, trainPredict)  \n",
    "    f1_t=f1_score(etiq_t, trainPredict, average=None)  # labels=np.unique(trainPredict)\n",
    "    f1_ma_t=f1_score(etiq_t, trainPredict, average='macro')  # labels=np.unique(trainPredict)\n",
    "    matriz_t=normalize(confusion_matrix(etiq_t, trainPredict))\n",
    "    print (\"\")\n",
    "    print (\"Accuracy sobre Test\", name_model, \":\",acc_t)  \n",
    "    print (\"F1-score None sobre Test\", name_model, \":\",f1_t)\n",
    "    print (\"F1-score macro sobre Test\", name_model, \":\",f1_ma_t)\n",
    "    \n",
    "    return f1_ma_t, f1_t, acc_t, matriz_t\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conteo por stance val\n",
      " Counter({'commenting': 173, 'support': 69, 'questioning': 28, 'denying': 11})\n",
      "Conteo por stance test\n",
      " Counter({'commenting': 778, 'questioning': 106, 'denying': 69, 'support': 68})\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tokenizer_loaded = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model_loaded = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=4)#'bert-base-uncased',  num_labels=4) \n",
    "idx_2_token = tokenizer_loaded.ids_to_tokens\n",
    "Word2Index=tokenizer_loaded.vocab\n",
    "M_BERT_space = model_loaded.bert.embeddings.word_embeddings.weight.detach().cpu().numpy()\n",
    "transformer = Normalizer().fit(M_BERT_space) \n",
    "M_BERT_space=transformer.transform(M_BERT_space)\n",
    "\n",
    "val=pd.read_csv(\"../../Fine-Tuning/CSV_Stance/dev_semeval_raw.csv\")\n",
    "test=pd.read_csv(\"../../Fine-Tuning/CSV_Stance/test_semeval_raw.csv\")\n",
    "MAX_LEN = 50\n",
    "\n",
    "print (\"Conteo por stance val\\n\", Counter(val['Label']))\n",
    "print (\"Conteo por stance test\\n\", Counter(test['Label']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1021, 50, 768)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "dict_cl=dict()\n",
    "dict_cl[0]='support'\n",
    "dict_cl[1]='denying'\n",
    "dict_cl[2]='questioning'\n",
    "dict_cl[3]='commenting'\n",
    "#sdqc\n",
    "d_lab=dict()\n",
    "d_lab[\"support\"]=0\n",
    "d_lab[\"denying\"]=1\n",
    "d_lab[\"questioning\"]=2\n",
    "d_lab[\"commenting\"]=3\n",
    "\n",
    "etiq_v=[d_lab[et]for et in val['Label']]\n",
    "n_labels_val = np.array(etiq_v)\n",
    "y_val=to_categorical(n_labels_val,num_classes=4)\n",
    "sentences_val = val['Tweet'].values\n",
    "input_ids_val = []\n",
    "i=0\n",
    "for sent in sentences_val:\n",
    "    temp=cleaner(sent)\n",
    "    encoded_sent2 = tokenizer_loaded.encode(temp[0],add_special_tokens = False) \n",
    "    encoded_sent=[]\n",
    "    iterar=temp[1]\n",
    "    for wd in iterar:\n",
    "        try:\n",
    "            indice=Word2Index[wd]\n",
    "            encoded_sent.append(wd)\n",
    "        except: \n",
    "            continue            \n",
    "    input_ids_val.append(encoded_sent2)    \n",
    "input_ids_val = pad_sequences(input_ids_val, maxlen=MAX_LEN, dtype=\"long\", value=0, truncating=\"post\", padding=\"post\")\n",
    "\n",
    "\n",
    "etiq_t=[d_lab[et] for et in test['Label'].values]\n",
    "n_labels_test = np.array(etiq_t)\n",
    "y_test=to_categorical(n_labels_test,num_classes=4)\n",
    "sentences_test = test['Tweet'].values\n",
    "input_ids_test = []\n",
    "i=0\n",
    "for sent in sentences_test:\n",
    "    temp=cleaner(sent)\n",
    "    encoded_sent2 = tokenizer_loaded.encode(temp[0],add_special_tokens = False) \n",
    "    encoded_sent=[]\n",
    "    iterar=temp[1]\n",
    "    for wd in iterar:\n",
    "        try:\n",
    "            indice=Word2Index[wd]\n",
    "            encoded_sent.append(wd)          \n",
    "        except: \n",
    "            continue            \n",
    "    input_ids_test.append(encoded_sent2)\n",
    "input_ids_test = pad_sequences(input_ids_test, maxlen=MAX_LEN, dtype=\"long\", value=0, truncating=\"post\", padding=\"post\")\n",
    "\n",
    "\n",
    "shape_val=np.asarray(input_ids_val).shape\n",
    "x_val=np.zeros((shape_val[0], shape_val[1], 768))\n",
    "i=0\n",
    "for in_id in input_ids_val:\n",
    "    x_val[i]=M_BERT_space[in_id]\n",
    "    i+=1    \n",
    "shape_test=np.asarray(input_ids_test).shape\n",
    "x_test=np.zeros((shape_test[0], shape_test[1], 768))\n",
    "i=0\n",
    "for in_id in input_ids_test:\n",
    "    x_test[i]=M_BERT_space[in_id]\n",
    "    i+=1\n",
    "\n",
    "x_test.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predecir_modelos(bs,predichos_all):  #best fit\n",
    "    temp=predichos_all.copy()\n",
    "    final=[np.argmax(pred) for pred in temp]  \n",
    "    confianzas=[temp[i][final[i]] for i in range(len(final))]\n",
    "    predichos_new=[]\n",
    "    for i in range(len(temp)):\n",
    "        indice=final[i]%4\n",
    "        predichos_new.append(int(indice))\n",
    "    return predichos_new,final,confianzas    \n",
    "\n",
    "def predecir_modelos_norm(bs,predichos_all):  #normalizado\n",
    "    temp=predichos_all.copy()\n",
    "    final=[]\n",
    "    confianzas=[]\n",
    "    for pred in temp:   \n",
    "        aux=np.ones(4)\n",
    "        for i in range(1,6): #for machine pred\n",
    "            sub=pred[(4*(i-1)):(4*i)]\n",
    "            aux=aux*np.asarray(sub)\n",
    "            \n",
    "        aux=aux/np.sum(aux)\n",
    "        final.append(np.argmax(aux))\n",
    "        confianzas.append(aux[np.argmax(aux)])  \n",
    "           \n",
    "    predichos_new=[]\n",
    "    for i in range(len(temp)):\n",
    "        predichos_new.append(final[i])\n",
    "    return predichos_new,final,confianzas\n",
    "    \n",
    "def predecir_modelos_average(bs,predichos_all): #average\n",
    "    temp=predichos_all.copy()\n",
    "    final=[]\n",
    "    confianzas=[]\n",
    "    for pred in temp:\n",
    "        aux=np.zeros(4)\n",
    "        for i in range(1,6):\n",
    "            sub=pred[(4*(i-1)):(4*i)]\n",
    "            aux=aux+np.asarray(sub)\n",
    "            \n",
    "        aux=aux/5.0 #dividido en el total de machines\n",
    "        final.append(np.argmax(aux))\n",
    "        confianzas.append(aux[np.argmax(aux)])  \n",
    "           \n",
    "    predichos_new=[]\n",
    "    for i in range(len(temp)):\n",
    "        predichos_new.append(final[i])\n",
    "    return predichos_new,final,confianzas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comité sin peso sin aumento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------- EJECUCIÓN #1----------------\n",
      "WARNING:tensorflow:From /home/casapanshop/anaconda2/envs/newpy3_tf1/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/casapanshop/anaconda2/envs/newpy3_tf1/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/casapanshop/anaconda2/envs/newpy3_tf1/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/casapanshop/anaconda2/envs/newpy3_tf1/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/casapanshop/anaconda2/envs/newpy3_tf1/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/casapanshop/anaconda2/envs/newpy3_tf1/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:Large dropout rate: 0.65 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.65 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:From /home/casapanshop/anaconda2/envs/newpy3_tf1/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/casapanshop/anaconda2/envs/newpy3_tf1/lib/python3.6/site-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/casapanshop/anaconda2/envs/newpy3_tf1/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1205: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:Large dropout rate: 0.65 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.65 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.65 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "Agregando predicciones Val set\n",
      "Agregando predicciones Test set rnn3\n",
      "--------VALIDATION SET--------\n",
      "\n",
      "Committee Best Fit\n",
      "Accuracy: 0.6654804270462633\n",
      "F1-macro: 0.3692434210526316\n",
      "F1-score SDQC: 0.3692434210526316\n",
      "\n",
      "Committee Norm\n",
      "Accuracy: 0.6192170818505338\n",
      "F1-macro: 0.20819060668341327\n",
      "F1-score SDQC: 0.20819060668341327\n",
      "\n",
      "Committee Voting\n",
      "Accuracy: 0.6263345195729537\n",
      "F1-macro: 0.252142808935682\n",
      "F1-score SDQC: 0.252142808935682\n",
      "--------TESTING SET--------\n",
      "\n",
      "Committee Best Fit\n",
      "Accuracy: 0.791380999020568\n",
      "F1-macro: 0.37980367859786923\n",
      "F1-score SDQC: 0.37980367859786923\n",
      "\n",
      "Committee Norm\n",
      "Accuracy: 0.761998041136141\n",
      "F1-macro: 0.2162312395775431\n",
      "F1-score SDQC: 0.2162312395775431\n",
      "\n",
      "Committee Voting\n",
      "Accuracy: 0.7776689520078355\n",
      "F1-macro: 0.2995605775266792\n",
      "F1-score SDQC: 0.2995605775266792\n",
      "\n",
      "---------------- EJECUCIÓN #2----------------\n",
      "Agregando predicciones Val set\n",
      "Agregando predicciones Test set rnn3\n",
      "--------VALIDATION SET--------\n",
      "\n",
      "Committee Best Fit\n",
      "Accuracy: 0.6654804270462633\n",
      "F1-macro: 0.3692807912464468\n",
      "F1-score SDQC: 0.3692807912464468\n",
      "\n",
      "Committee Norm\n",
      "Accuracy: 0.6192170818505338\n",
      "F1-macro: 0.20819060668341327\n",
      "F1-score SDQC: 0.20819060668341327\n",
      "\n",
      "Committee Voting\n",
      "Accuracy: 0.6405693950177936\n",
      "F1-macro: 0.30838687272095855\n",
      "F1-score SDQC: 0.30838687272095855\n",
      "--------TESTING SET--------\n",
      "\n",
      "Committee Best Fit\n",
      "Accuracy: 0.7904015670910872\n",
      "F1-macro: 0.3744089114220251\n",
      "F1-score SDQC: 0.3744089114220251\n",
      "\n",
      "Committee Norm\n",
      "Accuracy: 0.7639569049951028\n",
      "F1-macro: 0.2382894002174616\n",
      "F1-score SDQC: 0.2382894002174616\n",
      "\n",
      "Committee Voting\n",
      "Accuracy: 0.7796278158667973\n",
      "F1-macro: 0.3187804803473422\n",
      "F1-score SDQC: 0.3187804803473422\n",
      "\n",
      "---------------- EJECUCIÓN #3----------------\n",
      "Agregando predicciones Val set\n",
      "Agregando predicciones Test set rnn3\n",
      "--------VALIDATION SET--------\n",
      "\n",
      "Committee Best Fit\n",
      "Accuracy: 0.6619217081850534\n",
      "F1-macro: 0.3680472488038278\n",
      "F1-score SDQC: 0.3680472488038278\n",
      "\n",
      "Committee Norm\n",
      "Accuracy: 0.6441281138790036\n",
      "F1-macro: 0.31211474632527264\n",
      "F1-score SDQC: 0.31211474632527264\n",
      "\n",
      "Committee Voting\n",
      "Accuracy: 0.6476868327402135\n",
      "F1-macro: 0.33769282360831654\n",
      "F1-score SDQC: 0.33769282360831654\n",
      "--------TESTING SET--------\n",
      "\n",
      "Committee Best Fit\n",
      "Accuracy: 0.7952987267384917\n",
      "F1-macro: 0.3842113109248972\n",
      "F1-score SDQC: 0.3842113109248972\n",
      "\n",
      "Committee Norm\n",
      "Accuracy: 0.7698334965719883\n",
      "F1-macro: 0.2832482625611013\n",
      "F1-score SDQC: 0.2832482625611013\n",
      "\n",
      "Committee Voting\n",
      "Accuracy: 0.791380999020568\n",
      "F1-macro: 0.36470388843155294\n",
      "F1-score SDQC: 0.36470388843155294\n",
      "\n",
      "---------------- EJECUCIÓN #4----------------\n",
      "Agregando predicciones Val set\n",
      "Agregando predicciones Test set rnn3\n",
      "--------VALIDATION SET--------\n",
      "\n",
      "Committee Best Fit\n",
      "Accuracy: 0.6654804270462633\n",
      "F1-macro: 0.369244105409154\n",
      "F1-score SDQC: 0.369244105409154\n",
      "\n",
      "Committee Norm\n",
      "Accuracy: 0.6263345195729537\n",
      "F1-macro: 0.2401831056433731\n",
      "F1-score SDQC: 0.2401831056433731\n",
      "\n",
      "Committee Voting\n",
      "Accuracy: 0.6548042704626335\n",
      "F1-macro: 0.3583685751463891\n",
      "F1-score SDQC: 0.3583685751463891\n",
      "--------TESTING SET--------\n",
      "\n",
      "Committee Best Fit\n",
      "Accuracy: 0.7855044074436827\n",
      "F1-macro: 0.3793110508389658\n",
      "F1-score SDQC: 0.3793110508389658\n",
      "\n",
      "Committee Norm\n",
      "Accuracy: 0.7708129285014691\n",
      "F1-macro: 0.2842731570830208\n",
      "F1-score SDQC: 0.2842731570830208\n",
      "\n",
      "Committee Voting\n",
      "Accuracy: 0.7943192948090108\n",
      "F1-macro: 0.37337139468497643\n",
      "F1-score SDQC: 0.37337139468497643\n",
      "\n",
      "---------------- EJECUCIÓN #5----------------\n",
      "Agregando predicciones Val set\n",
      "Agregando predicciones Test set rnn3\n",
      "--------VALIDATION SET--------\n",
      "\n",
      "Committee Best Fit\n",
      "Accuracy: 0.6370106761565836\n",
      "F1-macro: 0.30480769230769234\n",
      "F1-score SDQC: 0.30480769230769234\n",
      "\n",
      "Committee Norm\n",
      "Accuracy: 0.6156583629893239\n",
      "F1-macro: 0.1905286343612335\n",
      "F1-score SDQC: 0.1905286343612335\n",
      "\n",
      "Committee Voting\n",
      "Accuracy: 0.6192170818505338\n",
      "F1-macro: 0.20819060668341327\n",
      "F1-score SDQC: 0.20819060668341327\n",
      "--------TESTING SET--------\n",
      "\n",
      "Committee Best Fit\n",
      "Accuracy: 0.7806072477962782\n",
      "F1-macro: 0.31906521838834445\n",
      "F1-score SDQC: 0.31906521838834445\n",
      "\n",
      "Committee Norm\n",
      "Accuracy: 0.7610186092066601\n",
      "F1-macro: 0.2160734149054505\n",
      "F1-score SDQC: 0.2160734149054505\n",
      "\n",
      "Committee Voting\n",
      "Accuracy: 0.7649363369245837\n",
      "F1-macro: 0.24256740344911343\n",
      "F1-score SDQC: 0.24256740344911343\n"
     ]
    }
   ],
   "source": [
    "prob=0.0\n",
    "path='../../../new_data_augmented/stance_baselines/'\n",
    "bs=32\n",
    "    \n",
    "fs_macro_val={'cnn1':[], 'cnn2':[], 'rnn1':[],'rnn2':[],'rnn3':[]}\n",
    "fs_none_val={'cnn1':[], 'cnn2':[], 'rnn1':[],'rnn2':[],'rnn3':[]}\n",
    "accs_val={'cnn1':[], 'cnn2':[], 'rnn1':[],'rnn2':[],'rnn3':[]}\n",
    "confusions_val={'cnn1':[], 'cnn2':[], 'rnn1':[],'rnn2':[],'rnn3':[]}\n",
    "\n",
    "fs_macro_test={'cnn1':[], 'cnn2':[], 'rnn1':[],'rnn2':[],'rnn3':[]}\n",
    "fs_none_test={'cnn1':[], 'cnn2':[], 'rnn1':[],'rnn2':[],'rnn3':[]}\n",
    "accs_test={'cnn1':[], 'cnn2':[], 'rnn1':[],'rnn2':[],'rnn3':[]}\n",
    "confusions_test={'cnn1':[], 'cnn2':[], 'rnn1':[],'rnn2':[],'rnn3':[]}\n",
    "\n",
    "best={'acc':[], 'none':[], 'macro':[]}\n",
    "norm={'acc':[], 'none':[], 'macro':[]}\n",
    "voting={'acc':[], 'none':[], 'macro':[]}\n",
    "best_test={'acc':[], 'none':[], 'macro':[]}\n",
    "norm_test={'acc':[], 'none':[], 'macro':[]}\n",
    "voting_test={'acc':[], 'none':[], 'macro':[]}\n",
    "\n",
    "for j in range(1,6):\n",
    "    print (\"\")\n",
    "    print (\"---------------- EJECUCIÓN #\"+str(j)+'----------------')\n",
    "    cnn1= load_model(path+str(prob)+'/cnn1_'+str(j)+'-exec.h5', custom_objects={'FocalLoss': focal_loss, 'focal_loss_fixed': focal_loss()})  \n",
    "    cnn2= load_model(path+str(prob)+'/cnn2_'+str(j)+'-exec.h5', custom_objects={'FocalLoss': focal_loss, 'focal_loss_fixed': focal_loss()}) \n",
    "    rnn1= load_model(path+str(prob)+'/rnn1_'+str(j)+'-exec.h5', custom_objects={'FocalLoss': focal_loss, 'focal_loss_fixed': focal_loss()}) \n",
    "    rnn2= load_model(path+str(prob)+'/rnn2_'+str(j)+'-exec.h5', custom_objects={'FocalLoss': focal_loss, 'focal_loss_fixed': focal_loss()}) \n",
    "    rnn3= load_model(path+str(prob)+'/rnn3_'+str(j)+'-exec.h5') \n",
    "    \n",
    "    list_models=['cnn1', 'cnn2', 'rnn1', 'rnn2', 'rnn3']\n",
    "    index_models=np.arange(5)\n",
    "    dict_models=dict((key, value) for (key, value) in zip(index_models,list_models))\n",
    "    modelos=[cnn1, cnn2, rnn1, rnn2, rnn3]\n",
    "    ind=np.arange(5)\n",
    "    dict_trainedModel=dict((key, value) for (key, value) in zip(ind,modelos))\n",
    "    \n",
    "    print (\"Agregando predicciones Val set\")\n",
    "    for i in dict_trainedModel.keys(): \n",
    "        a_evaluar=dict_models[i]\n",
    "        predicho=dict_trainedModel[i].predict(x_val, batch_size=bs)\n",
    "        predicho=[np.argmax(pred) for pred in predicho]\n",
    "        acc= accuracy_score(etiq_v, predicho)  \n",
    "        f1_ma=f1_score(etiq_v, predicho, average='macro')\n",
    "        f1_no=f1_score(etiq_v, predicho, average=None)\n",
    "        matriz=normalize(confusion_matrix(etiq_v, predicho))\n",
    "        fs_macro_val[dict_models[i]].append(f1_ma)\n",
    "        fs_none_val[dict_models[i]].append(f1_no)\n",
    "        accs_val[dict_models[i]].append(acc)\n",
    "        confusions_val[dict_models[i]].append(matriz)\n",
    "        \n",
    "    predicciones_all_val=[]\n",
    "    for i in dict_trainedModel.keys(): \n",
    "        a_evaluar=dict_models[i]\n",
    "        predicciones_all_val.append(dict_trainedModel[i].predict(x_val, batch_size=bs))\n",
    "    predichos_all_val=np.concatenate(np.asarray(predicciones_all_val),axis=-1)\n",
    "    \n",
    "    print (\"Agregando predicciones Test set\", dict_models[i])    \n",
    "    for i in dict_trainedModel.keys(): \n",
    "        a_evaluar=dict_models[i]\n",
    "        predicho=dict_trainedModel[i].predict(x_test, batch_size=bs)\n",
    "        predicho=[np.argmax(pred) for pred in predicho]\n",
    "        acc= accuracy_score(etiq_t, predicho)  \n",
    "        f1_ma=f1_score(etiq_t, predicho, average='macro')\n",
    "        f1_no=f1_score(etiq_t, predicho, average=None)\n",
    "        matriz=normalize(confusion_matrix(etiq_t, predicho))\n",
    "        fs_macro_test[dict_models[i]].append(f1_ma)\n",
    "        fs_none_test[dict_models[i]].append(f1_no)\n",
    "        accs_test[dict_models[i]].append(acc)\n",
    "        confusions_test[dict_models[i]].append(matriz)\n",
    "\n",
    "    predicciones_all_test=[]\n",
    "    bs=32\n",
    "    for i in dict_trainedModel.keys(): \n",
    "        a_evaluar=dict_models[i]\n",
    "        predicciones_all_test.append(dict_trainedModel[i].predict(x_test, batch_size=bs))\n",
    "    predichos_all_test=np.concatenate(np.asarray(predicciones_all_test),axis=-1)\n",
    "    \n",
    "    print (\"--------VALIDATION SET--------\")\n",
    "    \n",
    "    print (\"\\nCommittee Best Fit\")\n",
    "    trainPredicho, maxPredicho, confianzas = predecir_modelos(bs,predichos_all_val)\n",
    "    acc_comite= accuracy_score(etiq_v, trainPredicho)  \n",
    "    print(\"Accuracy:\",acc_comite)\n",
    "    f1=f1_score(etiq_v, trainPredicho, average='macro') \n",
    "    print(\"F1-macro:\",f1)\n",
    "    f1_no=f1_score(etiq_v, trainPredicho, average=None)  \n",
    "    print(\"F1-score SDQC:\",f1)\n",
    "    best['acc'].append(acc_comite)\n",
    "    best['macro'].append(f1)\n",
    "    best['none'].append(f1_no)\n",
    "    \n",
    "    print (\"\\nCommittee Norm\")\n",
    "    trainPredicho, maxPredicho, confianzas = predecir_modelos_norm(bs,predichos_all_val)\n",
    "    acc_comite= accuracy_score(etiq_v, trainPredicho)  \n",
    "    print(\"Accuracy:\",acc_comite)\n",
    "    f1=f1_score(etiq_v, trainPredicho, average='macro') \n",
    "    print(\"F1-macro:\",f1)\n",
    "    f1_no=f1_score(etiq_v, trainPredicho, average=None) \n",
    "    print(\"F1-score SDQC:\",f1)\n",
    "    norm['acc'].append(acc_comite)\n",
    "    norm['macro'].append(f1)\n",
    "    norm['none'].append(f1_no)\n",
    "\n",
    "    print (\"\\nCommittee Voting\")\n",
    "    trainPredicho, maxPredicho, confianzas = predecir_modelos_average(bs,predichos_all_val)\n",
    "    acc_comite= accuracy_score(etiq_v, trainPredicho)  \n",
    "    print(\"Accuracy:\",acc_comite)\n",
    "    f1=f1_score(etiq_v, trainPredicho, average='macro') \n",
    "    print(\"F1-macro:\",f1)\n",
    "    f1_no=f1_score(etiq_v, trainPredicho, average=None)  \n",
    "    print(\"F1-score SDQC:\",f1)\n",
    "    voting['acc'].append(acc_comite)\n",
    "    voting['macro'].append(f1)\n",
    "    voting['none'].append(f1_no)\n",
    "    \n",
    "    print (\"--------TESTING SET--------\")\n",
    "    \n",
    "    print (\"\\nCommittee Best Fit\")\n",
    "    trainPredicho, maxPredicho, confianzas = predecir_modelos(bs,predichos_all_test)\n",
    "    acc_comite= accuracy_score(etiq_t, trainPredicho)  \n",
    "    print(\"Accuracy:\",acc_comite)\n",
    "    f1=f1_score(etiq_t, trainPredicho, average='macro') \n",
    "    print(\"F1-macro:\",f1)\n",
    "    f1_no=f1_score(etiq_t, trainPredicho, average=None)  \n",
    "    print(\"F1-score SDQC:\",f1)\n",
    "    best_test['acc'].append(acc_comite)\n",
    "    best_test['macro'].append(f1)\n",
    "    best_test['none'].append(f1_no)\n",
    "\n",
    "    print (\"\\nCommittee Norm\")\n",
    "    trainPredicho, maxPredicho, confianzas = predecir_modelos_norm(bs,predichos_all_test)\n",
    "    acc_comite= accuracy_score(etiq_t, trainPredicho)  \n",
    "    print(\"Accuracy:\",acc_comite)\n",
    "    f1=f1_score(etiq_t, trainPredicho, average='macro') \n",
    "    print(\"F1-macro:\",f1)\n",
    "    f1_no=f1_score(etiq_t, trainPredicho, average=None) \n",
    "    print(\"F1-score SDQC:\",f1)\n",
    "    norm_test['acc'].append(acc_comite)\n",
    "    norm_test['macro'].append(f1)\n",
    "    norm_test['none'].append(f1_no)\n",
    "\n",
    "    print (\"\\nCommittee Voting\")\n",
    "    trainPredicho, maxPredicho, confianzas = predecir_modelos_average(bs,predichos_all_test)\n",
    "    acc_comite= accuracy_score(etiq_t, trainPredicho)  \n",
    "    print(\"Accuracy:\",acc_comite)\n",
    "    f1=f1_score(etiq_t, trainPredicho, average='macro') \n",
    "    print(\"F1-macro:\",f1)\n",
    "    f1_no=f1_score(etiq_t, trainPredicho, average=None)  \n",
    "    print(\"F1-score SDQC:\",f1)\n",
    "    voting_test['acc'].append(acc_comite)\n",
    "    voting_test['macro'].append(f1)\n",
    "    voting_test['none'].append(f1_no)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0.        , 0.        , 0.        , 0.86492496]),\n",
       " array([0.       , 0.       , 0.0877193, 0.8654383]),\n",
       " array([0.        , 0.        , 0.26470588, 0.86828717]),\n",
       " array([0.        , 0.        , 0.26865672, 0.86843591]),\n",
       " array([0.        , 0.        , 0.        , 0.86429366])]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_test['none']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TEST] Acc Best comité: 0.7886385896180216\n",
      "[TEST] F1-score SDQC Best comité: [0.         0.         0.5940348  0.87540534]\n",
      "[TEST] F1 macro Best comité: 0.36736003403442036\n",
      "-----------\n",
      "[TEST] Acc Norm comité: 0.7655239960822723\n",
      "[TEST] F1-score SDQC Norm comité: [0.         0.         0.12421638 0.866276  ]\n",
      "[TEST] F1 macro Norm comité: 0.24762309486891546\n",
      "-----------\n",
      "[TEST] Acc Voting comité: 0.781586679725759\n",
      "[TEST] F1-score SDQC Voting comité: [0.         0.         0.40542612 0.87376088]\n",
      "[TEST] F1 macro Voting comité: 0.31979674888793286\n"
     ]
    }
   ],
   "source": [
    "m=5.0\n",
    "\n",
    "print (\"[TEST] Acc Best comité:\",np.sum(np.asarray(best_test['acc'])/m))\n",
    "temp=np.zeros(4)\n",
    "for result in best_test['none']:\n",
    "    temp+=result\n",
    "print (\"[TEST] F1-score SDQC Best comité:\" ,temp/m)\n",
    "print (\"[TEST] F1 macro Best comité:\",np.sum(np.asarray(best_test['macro'])/m))\n",
    "print (\"-----------\")\n",
    "print (\"[TEST] Acc Norm comité:\",np.sum(np.asarray(norm_test['acc'])/m))\n",
    "temp=np.zeros(4)\n",
    "for result in norm_test['none']:\n",
    "    temp+=result\n",
    "print (\"[TEST] F1-score SDQC Norm comité:\" ,temp/m)\n",
    "print (\"[TEST] F1 macro Norm comité:\",np.sum(np.asarray(norm_test['macro'])/m))\n",
    "print (\"-----------\")\n",
    "print (\"[TEST] Acc Voting comité:\",np.sum(np.asarray(voting_test['acc'])/m))\n",
    "temp=np.zeros(4)\n",
    "for result in voting_test['none']:\n",
    "    temp+=result\n",
    "print (\"[TEST] F1-score SDQC Voting comité:\" ,temp/m)\n",
    "print (\"[TEST] F1 macro Voting comité:\",np.sum(np.asarray(voting_test['macro'])/m))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comité con CW sin aumento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------- EJECUCIÓN #1----------------\n",
      "Agregando predicciones Val set\n",
      "Agregando predicciones Test set rnn3\n",
      "--------VALIDATION SET--------\n",
      "\n",
      "Committee Best Fit\n",
      "Accuracy: 0.6583629893238434\n",
      "F1-macro: 0.3735489220563847\n",
      "F1-score SDQC: 0.3735489220563847\n",
      "\n",
      "Committee Norm\n",
      "Accuracy: 0.6619217081850534\n",
      "F1-macro: 0.3723667220070097\n",
      "F1-score SDQC: 0.3723667220070097\n",
      "\n",
      "Committee Voting\n",
      "Accuracy: 0.6654804270462633\n",
      "F1-macro: 0.37728532828016986\n",
      "F1-score SDQC: 0.37728532828016986\n",
      "--------TESTING SET--------\n",
      "\n",
      "Committee Best Fit\n",
      "Accuracy: 0.7786483839373164\n",
      "F1-macro: 0.376688555347092\n",
      "F1-score SDQC: 0.376688555347092\n",
      "\n",
      "Committee Norm\n",
      "Accuracy: 0.7835455435847208\n",
      "F1-macro: 0.37306938767391673\n",
      "F1-score SDQC: 0.37306938767391673\n",
      "\n",
      "Committee Voting\n",
      "Accuracy: 0.7796278158667973\n",
      "F1-macro: 0.37383980582524273\n",
      "F1-score SDQC: 0.37383980582524273\n",
      "\n",
      "---------------- EJECUCIÓN #2----------------\n",
      "Agregando predicciones Val set\n",
      "Agregando predicciones Test set rnn3\n",
      "--------VALIDATION SET--------\n",
      "\n",
      "Committee Best Fit\n",
      "Accuracy: 0.6619217081850534\n",
      "F1-macro: 0.36176946592339504\n",
      "F1-score SDQC: 0.36176946592339504\n",
      "\n",
      "Committee Norm\n",
      "Accuracy: 0.6619217081850534\n",
      "F1-macro: 0.36176946592339504\n",
      "F1-score SDQC: 0.36176946592339504\n",
      "\n",
      "Committee Voting\n",
      "Accuracy: 0.6619217081850534\n",
      "F1-macro: 0.36176946592339504\n",
      "F1-score SDQC: 0.36176946592339504\n",
      "--------TESTING SET--------\n",
      "\n",
      "Committee Best Fit\n",
      "Accuracy: 0.7933398628795298\n",
      "F1-macro: 0.38385965719897497\n",
      "F1-score SDQC: 0.38385965719897497\n",
      "\n",
      "Committee Norm\n",
      "Accuracy: 0.7923604309500489\n",
      "F1-macro: 0.38157302673431703\n",
      "F1-score SDQC: 0.38157302673431703\n",
      "\n",
      "Committee Voting\n",
      "Accuracy: 0.7923604309500489\n",
      "F1-macro: 0.38157302673431703\n",
      "F1-score SDQC: 0.38157302673431703\n",
      "\n",
      "---------------- EJECUCIÓN #3----------------\n",
      "Agregando predicciones Val set\n",
      "Agregando predicciones Test set rnn3\n",
      "--------VALIDATION SET--------\n",
      "\n",
      "Committee Best Fit\n",
      "Accuracy: 0.6548042704626335\n",
      "F1-macro: 0.3611774065234685\n",
      "F1-score SDQC: 0.3611774065234685\n",
      "\n",
      "Committee Norm\n",
      "Accuracy: 0.6512455516014235\n",
      "F1-macro: 0.35264612954186414\n",
      "F1-score SDQC: 0.35264612954186414\n",
      "\n",
      "Committee Voting\n",
      "Accuracy: 0.6512455516014235\n",
      "F1-macro: 0.35097580015612806\n",
      "F1-score SDQC: 0.35097580015612806\n",
      "--------TESTING SET--------\n",
      "\n",
      "Committee Best Fit\n",
      "Accuracy: 0.7904015670910872\n",
      "F1-macro: 0.37812802593086303\n",
      "F1-score SDQC: 0.37812802593086303\n",
      "\n",
      "Committee Norm\n",
      "Accuracy: 0.7933398628795298\n",
      "F1-macro: 0.37964285714285717\n",
      "F1-score SDQC: 0.37964285714285717\n",
      "\n",
      "Committee Voting\n",
      "Accuracy: 0.7943192948090108\n",
      "F1-macro: 0.38201832846875683\n",
      "F1-score SDQC: 0.38201832846875683\n",
      "\n",
      "---------------- EJECUCIÓN #4----------------\n",
      "Agregando predicciones Val set\n",
      "Agregando predicciones Test set rnn3\n",
      "--------VALIDATION SET--------\n",
      "\n",
      "Committee Best Fit\n",
      "Accuracy: 0.6654804270462633\n",
      "F1-macro: 0.3649839743589744\n",
      "F1-score SDQC: 0.3649839743589744\n",
      "\n",
      "Committee Norm\n",
      "Accuracy: 0.6619217081850534\n",
      "F1-macro: 0.3657834101382489\n",
      "F1-score SDQC: 0.3657834101382489\n",
      "\n",
      "Committee Voting\n",
      "Accuracy: 0.6654804270462633\n",
      "F1-macro: 0.3649839743589744\n",
      "F1-score SDQC: 0.3649839743589744\n",
      "--------TESTING SET--------\n",
      "\n",
      "Committee Best Fit\n",
      "Accuracy: 0.7962781586679726\n",
      "F1-macro: 0.3865948533812088\n",
      "F1-score SDQC: 0.3865948533812088\n",
      "\n",
      "Committee Norm\n",
      "Accuracy: 0.7972575905974535\n",
      "F1-macro: 0.3852856505481431\n",
      "F1-score SDQC: 0.3852856505481431\n",
      "\n",
      "Committee Voting\n",
      "Accuracy: 0.7943192948090108\n",
      "F1-macro: 0.3828762142226378\n",
      "F1-score SDQC: 0.3828762142226378\n",
      "\n",
      "---------------- EJECUCIÓN #5----------------\n",
      "Agregando predicciones Val set\n",
      "Agregando predicciones Test set rnn3\n",
      "--------VALIDATION SET--------\n",
      "\n",
      "Committee Best Fit\n",
      "Accuracy: 0.6583629893238434\n",
      "F1-macro: 0.3711676812396236\n",
      "F1-score SDQC: 0.3711676812396236\n",
      "\n",
      "Committee Norm\n",
      "Accuracy: 0.6583629893238434\n",
      "F1-macro: 0.3598964214789143\n",
      "F1-score SDQC: 0.3598964214789143\n",
      "\n",
      "Committee Voting\n",
      "Accuracy: 0.6583629893238434\n",
      "F1-macro: 0.36467441431470204\n",
      "F1-score SDQC: 0.36467441431470204\n",
      "--------TESTING SET--------\n",
      "\n",
      "Committee Best Fit\n",
      "Accuracy: 0.7884427032321254\n",
      "F1-macro: 0.3779525733248139\n",
      "F1-score SDQC: 0.3779525733248139\n",
      "\n",
      "Committee Norm\n",
      "Accuracy: 0.7933398628795298\n",
      "F1-macro: 0.380051536419428\n",
      "F1-score SDQC: 0.380051536419428\n",
      "\n",
      "Committee Voting\n",
      "Accuracy: 0.7904015670910872\n",
      "F1-macro: 0.37817232128268513\n",
      "F1-score SDQC: 0.37817232128268513\n"
     ]
    }
   ],
   "source": [
    "prob=0.0\n",
    "path='../../../new_data_augmented/stance_baselines/'\n",
    "bs=32\n",
    "    \n",
    "fs_macro_val={'cnn1':[], 'cnn2':[], 'rnn1':[],'rnn2':[],'rnn3':[]}\n",
    "fs_none_val={'cnn1':[], 'cnn2':[], 'rnn1':[],'rnn2':[],'rnn3':[]}\n",
    "accs_val={'cnn1':[], 'cnn2':[], 'rnn1':[],'rnn2':[],'rnn3':[]}\n",
    "confusions_val={'cnn1':[], 'cnn2':[], 'rnn1':[],'rnn2':[],'rnn3':[]}\n",
    "\n",
    "fs_macro_test={'cnn1':[], 'cnn2':[], 'rnn1':[],'rnn2':[],'rnn3':[]}\n",
    "fs_none_test={'cnn1':[], 'cnn2':[], 'rnn1':[],'rnn2':[],'rnn3':[]}\n",
    "accs_test={'cnn1':[], 'cnn2':[], 'rnn1':[],'rnn2':[],'rnn3':[]}\n",
    "confusions_test={'cnn1':[], 'cnn2':[], 'rnn1':[],'rnn2':[],'rnn3':[]}\n",
    "\n",
    "best={'acc':[], 'none':[], 'macro':[]}\n",
    "norm={'acc':[], 'none':[], 'macro':[]}\n",
    "voting={'acc':[], 'none':[], 'macro':[]}\n",
    "best_test={'acc':[], 'none':[], 'macro':[]}\n",
    "norm_test={'acc':[], 'none':[], 'macro':[]}\n",
    "voting_test={'acc':[], 'none':[], 'macro':[]}\n",
    "\n",
    "for j in range(1,6):\n",
    "    print (\"\")\n",
    "    print (\"---------------- EJECUCIÓN #\"+str(j)+'----------------')\n",
    "    cnn1= load_model(path+str(prob)+'_cw/cnn1_'+str(j)+'-exec.h5', custom_objects={'FocalLoss': focal_loss, 'focal_loss_fixed': focal_loss()})  \n",
    "    cnn2= load_model(path+str(prob)+'_cw/cnn2_'+str(j)+'-exec.h5', custom_objects={'FocalLoss': focal_loss, 'focal_loss_fixed': focal_loss()}) \n",
    "    rnn1= load_model(path+str(prob)+'_cw/rnn1_'+str(j)+'-exec.h5', custom_objects={'FocalLoss': focal_loss, 'focal_loss_fixed': focal_loss()}) \n",
    "    rnn2= load_model(path+str(prob)+'_cw/rnn2_'+str(j)+'-exec.h5', custom_objects={'FocalLoss': focal_loss, 'focal_loss_fixed': focal_loss()}) \n",
    "    rnn3= load_model(path+str(prob)+'_cw/rnn3_'+str(j)+'-exec.h5') \n",
    "    \n",
    "    list_models=['cnn1', 'cnn2', 'rnn1', 'rnn2', 'rnn3']\n",
    "    index_models=np.arange(5)\n",
    "    dict_models=dict((key, value) for (key, value) in zip(index_models,list_models))\n",
    "    modelos=[cnn1, cnn2, rnn1, rnn2, rnn3]\n",
    "    ind=np.arange(5)\n",
    "    dict_trainedModel=dict((key, value) for (key, value) in zip(ind,modelos))\n",
    "    \n",
    "    print (\"Agregando predicciones Val set\")\n",
    "    for i in dict_trainedModel.keys(): \n",
    "        a_evaluar=dict_models[i]\n",
    "        predicho=dict_trainedModel[i].predict(x_val, batch_size=bs)\n",
    "        predicho=[np.argmax(pred) for pred in predicho]\n",
    "        acc= accuracy_score(etiq_v, predicho)  \n",
    "        f1_ma=f1_score(etiq_v, predicho, average='macro')\n",
    "        f1_no=f1_score(etiq_v, predicho, average=None)\n",
    "        matriz=normalize(confusion_matrix(etiq_v, predicho))\n",
    "        fs_macro_val[dict_models[i]].append(f1_ma)\n",
    "        fs_none_val[dict_models[i]].append(f1_no)\n",
    "        accs_val[dict_models[i]].append(acc)\n",
    "        confusions_val[dict_models[i]].append(matriz)\n",
    "        \n",
    "    predicciones_all_val=[]\n",
    "    for i in dict_trainedModel.keys(): \n",
    "        a_evaluar=dict_models[i]\n",
    "        predicciones_all_val.append(dict_trainedModel[i].predict(x_val, batch_size=bs))\n",
    "    predichos_all_val=np.concatenate(np.asarray(predicciones_all_val),axis=-1)\n",
    "    \n",
    "    print (\"Agregando predicciones Test set\", dict_models[i])    \n",
    "    for i in dict_trainedModel.keys(): \n",
    "        a_evaluar=dict_models[i]\n",
    "        predicho=dict_trainedModel[i].predict(x_test, batch_size=bs)\n",
    "        predicho=[np.argmax(pred) for pred in predicho]\n",
    "        acc= accuracy_score(etiq_t, predicho)  \n",
    "        f1_ma=f1_score(etiq_t, predicho, average='macro')\n",
    "        f1_no=f1_score(etiq_t, predicho, average=None)\n",
    "        matriz=normalize(confusion_matrix(etiq_t, predicho))\n",
    "        fs_macro_test[dict_models[i]].append(f1_ma)\n",
    "        fs_none_test[dict_models[i]].append(f1_no)\n",
    "        accs_test[dict_models[i]].append(acc)\n",
    "        confusions_test[dict_models[i]].append(matriz)\n",
    "\n",
    "    predicciones_all_test=[]\n",
    "    bs=32\n",
    "    for i in dict_trainedModel.keys(): \n",
    "        a_evaluar=dict_models[i]\n",
    "        predicciones_all_test.append(dict_trainedModel[i].predict(x_test, batch_size=bs))\n",
    "    predichos_all_test=np.concatenate(np.asarray(predicciones_all_test),axis=-1)\n",
    "    \n",
    "    print (\"--------VALIDATION SET--------\")\n",
    "    \n",
    "    print (\"\\nCommittee Best Fit\")\n",
    "    trainPredicho, maxPredicho, confianzas = predecir_modelos(bs,predichos_all_val)\n",
    "    acc_comite= accuracy_score(etiq_v, trainPredicho)  \n",
    "    print(\"Accuracy:\",acc_comite)\n",
    "    f1=f1_score(etiq_v, trainPredicho, average='macro') \n",
    "    print(\"F1-macro:\",f1)\n",
    "    f1_no=f1_score(etiq_v, trainPredicho, average=None)  \n",
    "    print(\"F1-score SDQC:\",f1)\n",
    "    best['acc'].append(acc_comite)\n",
    "    best['macro'].append(f1)\n",
    "    best['none'].append(f1_no)\n",
    "    \n",
    "    print (\"\\nCommittee Norm\")\n",
    "    trainPredicho, maxPredicho, confianzas = predecir_modelos_norm(bs,predichos_all_val)\n",
    "    acc_comite= accuracy_score(etiq_v, trainPredicho)  \n",
    "    print(\"Accuracy:\",acc_comite)\n",
    "    f1=f1_score(etiq_v, trainPredicho, average='macro') \n",
    "    print(\"F1-macro:\",f1)\n",
    "    f1_no=f1_score(etiq_v, trainPredicho, average=None) \n",
    "    print(\"F1-score SDQC:\",f1)\n",
    "    norm['acc'].append(acc_comite)\n",
    "    norm['macro'].append(f1)\n",
    "    norm['none'].append(f1_no)\n",
    "\n",
    "    print (\"\\nCommittee Voting\")\n",
    "    trainPredicho, maxPredicho, confianzas = predecir_modelos_average(bs,predichos_all_val)\n",
    "    acc_comite= accuracy_score(etiq_v, trainPredicho)  \n",
    "    print(\"Accuracy:\",acc_comite)\n",
    "    f1=f1_score(etiq_v, trainPredicho, average='macro') \n",
    "    print(\"F1-macro:\",f1)\n",
    "    f1_no=f1_score(etiq_v, trainPredicho, average=None)  \n",
    "    print(\"F1-score SDQC:\",f1)\n",
    "    voting['acc'].append(acc_comite)\n",
    "    voting['macro'].append(f1)\n",
    "    voting['none'].append(f1_no)\n",
    "    \n",
    "    print (\"--------TESTING SET--------\")\n",
    "    \n",
    "    print (\"\\nCommittee Best Fit\")\n",
    "    trainPredicho, maxPredicho, confianzas = predecir_modelos(bs,predichos_all_test)\n",
    "    acc_comite= accuracy_score(etiq_t, trainPredicho)  \n",
    "    print(\"Accuracy:\",acc_comite)\n",
    "    f1=f1_score(etiq_t, trainPredicho, average='macro') \n",
    "    print(\"F1-macro:\",f1)\n",
    "    f1_no=f1_score(etiq_t, trainPredicho, average=None)  \n",
    "    print(\"F1-score SDQC:\",f1)\n",
    "    best_test['acc'].append(acc_comite)\n",
    "    best_test['macro'].append(f1)\n",
    "    best_test['none'].append(f1_no)\n",
    "\n",
    "    print (\"\\nCommittee Norm\")\n",
    "    trainPredicho, maxPredicho, confianzas = predecir_modelos_norm(bs,predichos_all_test)\n",
    "    acc_comite= accuracy_score(etiq_t, trainPredicho)  \n",
    "    print(\"Accuracy:\",acc_comite)\n",
    "    f1=f1_score(etiq_t, trainPredicho, average='macro') \n",
    "    print(\"F1-macro:\",f1)\n",
    "    f1_no=f1_score(etiq_t, trainPredicho, average=None) \n",
    "    print(\"F1-score SDQC:\",f1)\n",
    "    norm_test['acc'].append(acc_comite)\n",
    "    norm_test['macro'].append(f1)\n",
    "    norm_test['none'].append(f1_no)\n",
    "\n",
    "    print (\"\\nCommittee Voting\")\n",
    "    trainPredicho, maxPredicho, confianzas = predecir_modelos_average(bs,predichos_all_test)\n",
    "    acc_comite= accuracy_score(etiq_t, trainPredicho)  \n",
    "    print(\"Accuracy:\",acc_comite)\n",
    "    f1=f1_score(etiq_t, trainPredicho, average='macro') \n",
    "    print(\"F1-macro:\",f1)\n",
    "    f1_no=f1_score(etiq_t, trainPredicho, average=None)  \n",
    "    print(\"F1-score SDQC:\",f1)\n",
    "    voting_test['acc'].append(acc_comite)\n",
    "    voting_test['macro'].append(f1)\n",
    "    voting_test['none'].append(f1_no)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TEST] Acc Best comité: 0.7894221351616062\n",
      "[TEST] F1-score SDQC Best comité: [0.         0.         0.64727441 0.87530453]\n",
      "[TEST] F1 macro Best comité: 0.3806447330365905\n",
      "-----------\n",
      "[TEST] Acc Norm comité: 0.7919686581782566\n",
      "[TEST] F1-score SDQC Norm comité: [0.         0.         0.64248672 0.87721125]\n",
      "[TEST] F1 macro Norm comité: 0.37992449170373244\n",
      "-----------\n",
      "[TEST] Acc Voting comité: 0.7902056807051909\n",
      "[TEST] F1-score SDQC Voting comité: [0.         0.         0.6423631  0.87642066]\n",
      "[TEST] F1 macro Voting comité: 0.3796959393067279\n"
     ]
    }
   ],
   "source": [
    "m=5.0\n",
    "\n",
    "print (\"[TEST] Acc Best comité:\",np.sum(np.asarray(best_test['acc'])/m))\n",
    "temp=np.zeros(4)\n",
    "for result in best_test['none']:\n",
    "    temp+=result\n",
    "print (\"[TEST] F1-score SDQC Best comité:\" ,temp/m)\n",
    "print (\"[TEST] F1 macro Best comité:\",np.sum(np.asarray(best_test['macro'])/m))\n",
    "print (\"-----------\")\n",
    "print (\"[TEST] Acc Norm comité:\",np.sum(np.asarray(norm_test['acc'])/m))\n",
    "temp=np.zeros(4)\n",
    "for result in norm_test['none']:\n",
    "    temp+=result\n",
    "print (\"[TEST] F1-score SDQC Norm comité:\" ,temp/m)\n",
    "print (\"[TEST] F1 macro Norm comité:\",np.sum(np.asarray(norm_test['macro'])/m))\n",
    "print (\"-----------\")\n",
    "print (\"[TEST] Acc Voting comité:\",np.sum(np.asarray(voting_test['acc'])/m))\n",
    "temp=np.zeros(4)\n",
    "for result in voting_test['none']:\n",
    "    temp+=result\n",
    "print (\"[TEST] F1-score SDQC Voting comité:\" ,temp/m)\n",
    "print (\"[TEST] F1 macro Voting comité:\",np.sum(np.asarray(voting_test['macro'])/m))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comité 0.15 classic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------- EJECUCIÓN #1----------------\n",
      "WARNING:tensorflow:From /home/casapanshop/anaconda2/envs/newpy3_tf1/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/casapanshop/anaconda2/envs/newpy3_tf1/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/casapanshop/anaconda2/envs/newpy3_tf1/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/casapanshop/anaconda2/envs/newpy3_tf1/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/casapanshop/anaconda2/envs/newpy3_tf1/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/casapanshop/anaconda2/envs/newpy3_tf1/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:Large dropout rate: 0.65 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.65 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:From /home/casapanshop/anaconda2/envs/newpy3_tf1/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/casapanshop/anaconda2/envs/newpy3_tf1/lib/python3.6/site-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/casapanshop/anaconda2/envs/newpy3_tf1/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1205: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:Large dropout rate: 0.65 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.65 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.65 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "Agregando predicciones Val set\n",
      "Agregando predicciones Test set rnn3\n",
      "--------VALIDATION SET--------\n",
      "\n",
      "Committee Best Fit\n",
      "Accuracy: 0.44483985765124556\n",
      "F1-macro: 0.41451222716853237\n",
      "F1-score SDQC: 0.41451222716853237\n",
      "\n",
      "Committee Norm\n",
      "Accuracy: 0.5195729537366548\n",
      "F1-macro: 0.46787306619842306\n",
      "F1-score SDQC: 0.46787306619842306\n",
      "\n",
      "Committee Voting\n",
      "Accuracy: 0.5266903914590747\n",
      "F1-macro: 0.47648327486746445\n",
      "F1-score SDQC: 0.47648327486746445\n",
      "--------TESTING SET--------\n",
      "\n",
      "Committee Best Fit\n",
      "Accuracy: 0.4583741429970617\n",
      "F1-macro: 0.3506961522832386\n",
      "F1-score SDQC: 0.3506961522832386\n",
      "\n",
      "Committee Norm\n",
      "Accuracy: 0.5318315377081293\n",
      "F1-macro: 0.39225996635345844\n",
      "F1-score SDQC: 0.39225996635345844\n",
      "\n",
      "Committee Voting\n",
      "Accuracy: 0.5396669931439765\n",
      "F1-macro: 0.3916009894048971\n",
      "F1-score SDQC: 0.3916009894048971\n",
      "\n",
      "---------------- EJECUCIÓN #2----------------\n",
      "Agregando predicciones Val set\n",
      "Agregando predicciones Test set rnn3\n",
      "--------VALIDATION SET--------\n",
      "\n",
      "Committee Best Fit\n",
      "Accuracy: 0.604982206405694\n",
      "F1-macro: 0.5018924244848096\n",
      "F1-score SDQC: 0.5018924244848096\n",
      "\n",
      "Committee Norm\n",
      "Accuracy: 0.6298932384341637\n",
      "F1-macro: 0.53179429416862\n",
      "F1-score SDQC: 0.53179429416862\n",
      "\n",
      "Committee Voting\n",
      "Accuracy: 0.6120996441281139\n",
      "F1-macro: 0.5207393562703473\n",
      "F1-score SDQC: 0.5207393562703473\n",
      "--------TESTING SET--------\n",
      "\n",
      "Committee Best Fit\n",
      "Accuracy: 0.6072477962781586\n",
      "F1-macro: 0.3621155138343932\n",
      "F1-score SDQC: 0.3621155138343932\n",
      "\n",
      "Committee Norm\n",
      "Accuracy: 0.6268364348677767\n",
      "F1-macro: 0.36777899300823313\n",
      "F1-score SDQC: 0.36777899300823313\n",
      "\n",
      "Committee Voting\n",
      "Accuracy: 0.6160626836434868\n",
      "F1-macro: 0.39190301047598025\n",
      "F1-score SDQC: 0.39190301047598025\n",
      "\n",
      "---------------- EJECUCIÓN #3----------------\n",
      "Agregando predicciones Val set\n",
      "Agregando predicciones Test set rnn3\n",
      "--------VALIDATION SET--------\n",
      "\n",
      "Committee Best Fit\n",
      "Accuracy: 0.6156583629893239\n",
      "F1-macro: 0.4670614793583687\n",
      "F1-score SDQC: 0.4670614793583687\n",
      "\n",
      "Committee Norm\n",
      "Accuracy: 0.6298932384341637\n",
      "F1-macro: 0.5132363620897729\n",
      "F1-score SDQC: 0.5132363620897729\n",
      "\n",
      "Committee Voting\n",
      "Accuracy: 0.6227758007117438\n",
      "F1-macro: 0.5172375164820884\n",
      "F1-score SDQC: 0.5172375164820884\n",
      "--------TESTING SET--------\n",
      "\n",
      "Committee Best Fit\n",
      "Accuracy: 0.6464250734573947\n",
      "F1-macro: 0.37382223788945035\n",
      "F1-score SDQC: 0.37382223788945035\n",
      "\n",
      "Committee Norm\n",
      "Accuracy: 0.6542605288932419\n",
      "F1-macro: 0.4009204238194051\n",
      "F1-score SDQC: 0.4009204238194051\n",
      "\n",
      "Committee Voting\n",
      "Accuracy: 0.6258570029382958\n",
      "F1-macro: 0.4139079789585686\n",
      "F1-score SDQC: 0.4139079789585686\n",
      "\n",
      "---------------- EJECUCIÓN #4----------------\n",
      "Agregando predicciones Val set\n",
      "Agregando predicciones Test set rnn3\n",
      "--------VALIDATION SET--------\n",
      "\n",
      "Committee Best Fit\n",
      "Accuracy: 0.5907473309608541\n",
      "F1-macro: 0.45828467398234846\n",
      "F1-score SDQC: 0.45828467398234846\n",
      "\n",
      "Committee Norm\n",
      "Accuracy: 0.6120996441281139\n",
      "F1-macro: 0.47609256618598317\n",
      "F1-score SDQC: 0.47609256618598317\n",
      "\n",
      "Committee Voting\n",
      "Accuracy: 0.6014234875444839\n",
      "F1-macro: 0.48448389592053354\n",
      "F1-score SDQC: 0.48448389592053354\n",
      "--------TESTING SET--------\n",
      "\n",
      "Committee Best Fit\n",
      "Accuracy: 0.5984329089128305\n",
      "F1-macro: 0.371234252536897\n",
      "F1-score SDQC: 0.371234252536897\n",
      "\n",
      "Committee Norm\n",
      "Accuracy: 0.6131243878550441\n",
      "F1-macro: 0.3928756329931023\n",
      "F1-score SDQC: 0.3928756329931023\n",
      "\n",
      "Committee Voting\n",
      "Accuracy: 0.594515181194907\n",
      "F1-macro: 0.40872740207778924\n",
      "F1-score SDQC: 0.40872740207778924\n",
      "\n",
      "---------------- EJECUCIÓN #5----------------\n",
      "Agregando predicciones Val set\n",
      "Agregando predicciones Test set rnn3\n",
      "--------VALIDATION SET--------\n",
      "\n",
      "Committee Best Fit\n",
      "Accuracy: 0.5338078291814946\n",
      "F1-macro: 0.4402326860334044\n",
      "F1-score SDQC: 0.4402326860334044\n",
      "\n",
      "Committee Norm\n",
      "Accuracy: 0.6014234875444839\n",
      "F1-macro: 0.49941511754077217\n",
      "F1-score SDQC: 0.49941511754077217\n",
      "\n",
      "Committee Voting\n",
      "Accuracy: 0.604982206405694\n",
      "F1-macro: 0.5263888888888888\n",
      "F1-score SDQC: 0.5263888888888888\n",
      "--------TESTING SET--------\n",
      "\n",
      "Committee Best Fit\n",
      "Accuracy: 0.5259549461312438\n",
      "F1-macro: 0.361764759537884\n",
      "F1-score SDQC: 0.361764759537884\n",
      "\n",
      "Committee Norm\n",
      "Accuracy: 0.5661116552399609\n",
      "F1-macro: 0.38030371470285596\n",
      "F1-score SDQC: 0.38030371470285596\n",
      "\n",
      "Committee Voting\n",
      "Accuracy: 0.5700293829578844\n",
      "F1-macro: 0.4035820778011146\n",
      "F1-score SDQC: 0.4035820778011146\n"
     ]
    }
   ],
   "source": [
    "prob=0.15\n",
    "\n",
    "path='../../../new_data_augmented/stance_baselines/'\n",
    "bs=32\n",
    "    \n",
    "fs_macro_val={'cnn1':[], 'cnn2':[], 'rnn1':[],'rnn2':[],'rnn3':[]}\n",
    "fs_none_val={'cnn1':[], 'cnn2':[], 'rnn1':[],'rnn2':[],'rnn3':[]}\n",
    "accs_val={'cnn1':[], 'cnn2':[], 'rnn1':[],'rnn2':[],'rnn3':[]}\n",
    "confusions_val={'cnn1':[], 'cnn2':[], 'rnn1':[],'rnn2':[],'rnn3':[]}\n",
    "\n",
    "fs_macro_test={'cnn1':[], 'cnn2':[], 'rnn1':[],'rnn2':[],'rnn3':[]}\n",
    "fs_none_test={'cnn1':[], 'cnn2':[], 'rnn1':[],'rnn2':[],'rnn3':[]}\n",
    "accs_test={'cnn1':[], 'cnn2':[], 'rnn1':[],'rnn2':[],'rnn3':[]}\n",
    "confusions_test={'cnn1':[], 'cnn2':[], 'rnn1':[],'rnn2':[],'rnn3':[]}\n",
    "\n",
    "best={'acc':[], 'none':[], 'macro':[]}\n",
    "norm={'acc':[], 'none':[], 'macro':[]}\n",
    "voting={'acc':[], 'none':[], 'macro':[]}\n",
    "best_test={'acc':[], 'none':[], 'macro':[]}\n",
    "norm_test={'acc':[], 'none':[], 'macro':[]}\n",
    "voting_test={'acc':[], 'none':[], 'macro':[]}\n",
    "\n",
    "for j in range(1,6):\n",
    "    print (\"\")\n",
    "    print (\"---------------- EJECUCIÓN #\"+str(j)+'----------------')\n",
    "    cnn1= load_model(path+str(prob)+'/cnn1_'+str(j)+'-exec_app1_Top_1.h5', custom_objects={'FocalLoss': focal_loss, 'focal_loss_fixed': focal_loss()})  \n",
    "    cnn2= load_model(path+str(prob)+'/cnn2_'+str(j)+'-exec_app1_Top_1.h5', custom_objects={'FocalLoss': focal_loss, 'focal_loss_fixed': focal_loss()}) \n",
    "    rnn1= load_model(path+str(prob)+'/rnn1_'+str(j)+'-exec_app1_Top_1.h5', custom_objects={'FocalLoss': focal_loss, 'focal_loss_fixed': focal_loss()}) \n",
    "    rnn2= load_model(path+str(prob)+'/rnn2_'+str(j)+'-exec_app1_Top_1.h5', custom_objects={'FocalLoss': focal_loss, 'focal_loss_fixed': focal_loss()}) \n",
    "    rnn3= load_model(path+str(prob)+'/rnn3_'+str(j)+'-exec_app1_Top_1.h5') \n",
    "    \n",
    "    list_models=['cnn1', 'cnn2', 'rnn1', 'rnn2', 'rnn3']\n",
    "    index_models=np.arange(5)\n",
    "    dict_models=dict((key, value) for (key, value) in zip(index_models,list_models))\n",
    "    modelos=[cnn1, cnn2, rnn1, rnn2, rnn3]\n",
    "    ind=np.arange(5)\n",
    "    dict_trainedModel=dict((key, value) for (key, value) in zip(ind,modelos))\n",
    "    \n",
    "    print (\"Agregando predicciones Val set\")\n",
    "    for i in dict_trainedModel.keys(): \n",
    "        a_evaluar=dict_models[i]\n",
    "        predicho=dict_trainedModel[i].predict(x_val, batch_size=bs)\n",
    "        predicho=[np.argmax(pred) for pred in predicho]\n",
    "        acc= accuracy_score(etiq_v, predicho)  \n",
    "        f1_ma=f1_score(etiq_v, predicho, average='macro')\n",
    "        f1_no=f1_score(etiq_v, predicho, average=None)\n",
    "        matriz=normalize(confusion_matrix(etiq_v, predicho))\n",
    "        fs_macro_val[dict_models[i]].append(f1_ma)\n",
    "        fs_none_val[dict_models[i]].append(f1_no)\n",
    "        accs_val[dict_models[i]].append(acc)\n",
    "        confusions_val[dict_models[i]].append(matriz)\n",
    "        \n",
    "    predicciones_all_val=[]\n",
    "    for i in dict_trainedModel.keys(): \n",
    "        a_evaluar=dict_models[i]\n",
    "        predicciones_all_val.append(dict_trainedModel[i].predict(x_val, batch_size=bs))\n",
    "    predichos_all_val=np.concatenate(np.asarray(predicciones_all_val),axis=-1)\n",
    "    \n",
    "    print (\"Agregando predicciones Test set\", dict_models[i])    \n",
    "    for i in dict_trainedModel.keys(): \n",
    "        a_evaluar=dict_models[i]\n",
    "        predicho=dict_trainedModel[i].predict(x_test, batch_size=bs)\n",
    "        predicho=[np.argmax(pred) for pred in predicho]\n",
    "        acc= accuracy_score(etiq_t, predicho)  \n",
    "        f1_ma=f1_score(etiq_t, predicho, average='macro')\n",
    "        f1_no=f1_score(etiq_t, predicho, average=None)\n",
    "        matriz=normalize(confusion_matrix(etiq_t, predicho))\n",
    "        fs_macro_test[dict_models[i]].append(f1_ma)\n",
    "        fs_none_test[dict_models[i]].append(f1_no)\n",
    "        accs_test[dict_models[i]].append(acc)\n",
    "        confusions_test[dict_models[i]].append(matriz)\n",
    "\n",
    "    predicciones_all_test=[]\n",
    "    bs=32\n",
    "    for i in dict_trainedModel.keys(): \n",
    "        a_evaluar=dict_models[i]\n",
    "        predicciones_all_test.append(dict_trainedModel[i].predict(x_test, batch_size=bs))\n",
    "    predichos_all_test=np.concatenate(np.asarray(predicciones_all_test),axis=-1)\n",
    "    \n",
    "    print (\"--------VALIDATION SET--------\")\n",
    "    \n",
    "    print (\"\\nCommittee Best Fit\")\n",
    "    trainPredicho, maxPredicho, confianzas = predecir_modelos(bs,predichos_all_val)\n",
    "    acc_comite= accuracy_score(etiq_v, trainPredicho)  \n",
    "    print(\"Accuracy:\",acc_comite)\n",
    "    f1=f1_score(etiq_v, trainPredicho, average='macro') \n",
    "    print(\"F1-macro:\",f1)\n",
    "    f1_no=f1_score(etiq_v, trainPredicho, average=None)  \n",
    "    print(\"F1-score SDQC:\",f1)\n",
    "    best['acc'].append(acc_comite)\n",
    "    best['macro'].append(f1)\n",
    "    best['none'].append(f1_no)\n",
    "    \n",
    "    print (\"\\nCommittee Norm\")\n",
    "    trainPredicho, maxPredicho, confianzas = predecir_modelos_norm(bs,predichos_all_val)\n",
    "    acc_comite= accuracy_score(etiq_v, trainPredicho)  \n",
    "    print(\"Accuracy:\",acc_comite)\n",
    "    f1=f1_score(etiq_v, trainPredicho, average='macro') \n",
    "    print(\"F1-macro:\",f1)\n",
    "    f1_no=f1_score(etiq_v, trainPredicho, average=None) \n",
    "    print(\"F1-score SDQC:\",f1)\n",
    "    norm['acc'].append(acc_comite)\n",
    "    norm['macro'].append(f1)\n",
    "    norm['none'].append(f1_no)\n",
    "\n",
    "    print (\"\\nCommittee Voting\")\n",
    "    trainPredicho, maxPredicho, confianzas = predecir_modelos_average(bs,predichos_all_val)\n",
    "    acc_comite= accuracy_score(etiq_v, trainPredicho)  \n",
    "    print(\"Accuracy:\",acc_comite)\n",
    "    f1=f1_score(etiq_v, trainPredicho, average='macro') \n",
    "    print(\"F1-macro:\",f1)\n",
    "    f1_no=f1_score(etiq_v, trainPredicho, average=None)  \n",
    "    print(\"F1-score SDQC:\",f1)\n",
    "    voting['acc'].append(acc_comite)\n",
    "    voting['macro'].append(f1)\n",
    "    voting['none'].append(f1_no)\n",
    "    \n",
    "    print (\"--------TESTING SET--------\")\n",
    "    \n",
    "    print (\"\\nCommittee Best Fit\")\n",
    "    trainPredicho, maxPredicho, confianzas = predecir_modelos(bs,predichos_all_test)\n",
    "    acc_comite= accuracy_score(etiq_t, trainPredicho)  \n",
    "    print(\"Accuracy:\",acc_comite)\n",
    "    f1=f1_score(etiq_t, trainPredicho, average='macro') \n",
    "    print(\"F1-macro:\",f1)\n",
    "    f1_no=f1_score(etiq_t, trainPredicho, average=None)  \n",
    "    print(\"F1-score SDQC:\",f1)\n",
    "    best_test['acc'].append(acc_comite)\n",
    "    best_test['macro'].append(f1)\n",
    "    best_test['none'].append(f1_no)\n",
    "\n",
    "    print (\"\\nCommittee Norm\")\n",
    "    trainPredicho, maxPredicho, confianzas = predecir_modelos_norm(bs,predichos_all_test)\n",
    "    acc_comite= accuracy_score(etiq_t, trainPredicho)  \n",
    "    print(\"Accuracy:\",acc_comite)\n",
    "    f1=f1_score(etiq_t, trainPredicho, average='macro') \n",
    "    print(\"F1-macro:\",f1)\n",
    "    f1_no=f1_score(etiq_t, trainPredicho, average=None) \n",
    "    print(\"F1-score SDQC:\",f1)\n",
    "    norm_test['acc'].append(acc_comite)\n",
    "    norm_test['macro'].append(f1)\n",
    "    norm_test['none'].append(f1_no)\n",
    "\n",
    "    print (\"\\nCommittee Voting\")\n",
    "    trainPredicho, maxPredicho, confianzas = predecir_modelos_average(bs,predichos_all_test)\n",
    "    acc_comite= accuracy_score(etiq_t, trainPredicho)  \n",
    "    print(\"Accuracy:\",acc_comite)\n",
    "    f1=f1_score(etiq_t, trainPredicho, average='macro') \n",
    "    print(\"F1-macro:\",f1)\n",
    "    f1_no=f1_score(etiq_t, trainPredicho, average=None)  \n",
    "    print(\"F1-score SDQC:\",f1)\n",
    "    voting_test['acc'].append(acc_comite)\n",
    "    voting_test['macro'].append(f1)\n",
    "    voting_test['none'].append(f1_no)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TEST] Acc Best comité: 0.567286973555338\n",
      "[TEST] F1-score SDQC Best comité: [0.11094545 0.13159355 0.50521489 0.70795244]\n",
      "[TEST] F1 macro Best comité: 0.3639265832163726\n",
      "-----------\n",
      "[TEST] Acc Norm comité: 0.5984329089128306\n",
      "[TEST] F1-score SDQC Norm comité: [0.1237259  0.15281428 0.53556442 0.73520639]\n",
      "[TEST] F1 macro Norm comité: 0.386827746175411\n",
      "-----------\n",
      "[TEST] Acc Voting comité: 0.5892262487757101\n",
      "[TEST] F1-score SDQC Voting comité: [0.14351754 0.19035002 0.54949058 0.72441903]\n",
      "[TEST] F1 macro Voting comité: 0.40194429174366997\n"
     ]
    }
   ],
   "source": [
    "m=5.0\n",
    "\n",
    "print (\"[TEST] Acc Best comité:\",np.sum(np.asarray(best_test['acc'])/m))\n",
    "temp=np.zeros(4)\n",
    "for result in best_test['none']:\n",
    "    temp+=result\n",
    "print (\"[TEST] F1-score SDQC Best comité:\" ,temp/m)\n",
    "print (\"[TEST] F1 macro Best comité:\",np.sum(np.asarray(best_test['macro'])/m))\n",
    "print (\"-----------\")\n",
    "print (\"[TEST] Acc Norm comité:\",np.sum(np.asarray(norm_test['acc'])/m))\n",
    "temp=np.zeros(4)\n",
    "for result in norm_test['none']:\n",
    "    temp+=result\n",
    "print (\"[TEST] F1-score SDQC Norm comité:\" ,temp/m)\n",
    "print (\"[TEST] F1 macro Norm comité:\",np.sum(np.asarray(norm_test['macro'])/m))\n",
    "print (\"-----------\")\n",
    "print (\"[TEST] Acc Voting comité:\",np.sum(np.asarray(voting_test['acc'])/m))\n",
    "temp=np.zeros(4)\n",
    "for result in voting_test['none']:\n",
    "    temp+=result\n",
    "print (\"[TEST] F1-score SDQC Voting comité:\" ,temp/m)\n",
    "print (\"[TEST] F1 macro Voting comité:\",np.sum(np.asarray(voting_test['macro'])/m))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comité 0.15 sólo positivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------- EJECUCIÓN #1----------------\n",
      "Agregando predicciones Val set\n",
      "Agregando predicciones Test set rnn3\n",
      "--------VALIDATION SET--------\n",
      "\n",
      "Committee Best Fit\n",
      "Accuracy: 0.6441281138790036\n",
      "F1-macro: 0.45533720515659304\n",
      "F1-score SDQC: 0.45533720515659304\n",
      "\n",
      "Committee Norm\n",
      "Accuracy: 0.6690391459074733\n",
      "F1-macro: 0.5168951531158504\n",
      "F1-score SDQC: 0.5168951531158504\n",
      "\n",
      "Committee Voting\n",
      "Accuracy: 0.6690391459074733\n",
      "F1-macro: 0.5056331228143431\n",
      "F1-score SDQC: 0.5056331228143431\n",
      "--------TESTING SET--------\n",
      "\n",
      "Committee Best Fit\n",
      "Accuracy: 0.6503428011753183\n",
      "F1-macro: 0.3786951047293269\n",
      "F1-score SDQC: 0.3786951047293269\n",
      "\n",
      "Committee Norm\n",
      "Accuracy: 0.6787463271302644\n",
      "F1-macro: 0.3783194373793767\n",
      "F1-score SDQC: 0.3783194373793767\n",
      "\n",
      "Committee Voting\n",
      "Accuracy: 0.683643486777669\n",
      "F1-macro: 0.3884706581732874\n",
      "F1-score SDQC: 0.3884706581732874\n",
      "\n",
      "---------------- EJECUCIÓN #2----------------\n",
      "Agregando predicciones Val set\n",
      "Agregando predicciones Test set rnn3\n",
      "--------VALIDATION SET--------\n",
      "\n",
      "Committee Best Fit\n",
      "Accuracy: 0.6370106761565836\n",
      "F1-macro: 0.467457956438169\n",
      "F1-score SDQC: 0.467457956438169\n",
      "\n",
      "Committee Norm\n",
      "Accuracy: 0.6654804270462633\n",
      "F1-macro: 0.4972131974376598\n",
      "F1-score SDQC: 0.4972131974376598\n",
      "\n",
      "Committee Voting\n",
      "Accuracy: 0.6476868327402135\n",
      "F1-macro: 0.5089823526922371\n",
      "F1-score SDQC: 0.5089823526922371\n",
      "--------TESTING SET--------\n",
      "\n",
      "Committee Best Fit\n",
      "Accuracy: 0.6562193927522038\n",
      "F1-macro: 0.34647090410957493\n",
      "F1-score SDQC: 0.34647090410957493\n",
      "\n",
      "Committee Norm\n",
      "Accuracy: 0.6816846229187071\n",
      "F1-macro: 0.376557669953285\n",
      "F1-score SDQC: 0.376557669953285\n",
      "\n",
      "Committee Voting\n",
      "Accuracy: 0.6767874632713027\n",
      "F1-macro: 0.4129293287089709\n",
      "F1-score SDQC: 0.4129293287089709\n",
      "\n",
      "---------------- EJECUCIÓN #3----------------\n",
      "Agregando predicciones Val set\n",
      "Agregando predicciones Test set rnn3\n",
      "--------VALIDATION SET--------\n",
      "\n",
      "Committee Best Fit\n",
      "Accuracy: 0.5338078291814946\n",
      "F1-macro: 0.4575864064403962\n",
      "F1-score SDQC: 0.4575864064403962\n",
      "\n",
      "Committee Norm\n",
      "Accuracy: 0.5765124555160143\n",
      "F1-macro: 0.4553143625460181\n",
      "F1-score SDQC: 0.4553143625460181\n",
      "\n",
      "Committee Voting\n",
      "Accuracy: 0.608540925266904\n",
      "F1-macro: 0.5010085042117657\n",
      "F1-score SDQC: 0.5010085042117657\n",
      "--------TESTING SET--------\n",
      "\n",
      "Committee Best Fit\n",
      "Accuracy: 0.5318315377081293\n",
      "F1-macro: 0.3666424331081707\n",
      "F1-score SDQC: 0.3666424331081707\n",
      "\n",
      "Committee Norm\n",
      "Accuracy: 0.5729676787463271\n",
      "F1-macro: 0.38656778498847166\n",
      "F1-score SDQC: 0.38656778498847166\n",
      "\n",
      "Committee Voting\n",
      "Accuracy: 0.5905974534769833\n",
      "F1-macro: 0.40604608213903387\n",
      "F1-score SDQC: 0.40604608213903387\n"
     ]
    }
   ],
   "source": [
    "prob=0.15\n",
    "\n",
    "path='../../../new_data_augmented/stance_baselines/'\n",
    "bs=32\n",
    "    \n",
    "fs_macro_val={'cnn1':[], 'cnn2':[], 'rnn1':[],'rnn2':[],'rnn3':[]}\n",
    "fs_none_val={'cnn1':[], 'cnn2':[], 'rnn1':[],'rnn2':[],'rnn3':[]}\n",
    "accs_val={'cnn1':[], 'cnn2':[], 'rnn1':[],'rnn2':[],'rnn3':[]}\n",
    "confusions_val={'cnn1':[], 'cnn2':[], 'rnn1':[],'rnn2':[],'rnn3':[]}\n",
    "\n",
    "fs_macro_test={'cnn1':[], 'cnn2':[], 'rnn1':[],'rnn2':[],'rnn3':[]}\n",
    "fs_none_test={'cnn1':[], 'cnn2':[], 'rnn1':[],'rnn2':[],'rnn3':[]}\n",
    "accs_test={'cnn1':[], 'cnn2':[], 'rnn1':[],'rnn2':[],'rnn3':[]}\n",
    "confusions_test={'cnn1':[], 'cnn2':[], 'rnn1':[],'rnn2':[],'rnn3':[]}\n",
    "\n",
    "best={'acc':[], 'none':[], 'macro':[]}\n",
    "norm={'acc':[], 'none':[], 'macro':[]}\n",
    "voting={'acc':[], 'none':[], 'macro':[]}\n",
    "best_test={'acc':[], 'none':[], 'macro':[]}\n",
    "norm_test={'acc':[], 'none':[], 'macro':[]}\n",
    "voting_test={'acc':[], 'none':[], 'macro':[]}\n",
    "\n",
    "for j in range(1,4):\n",
    "    print (\"\")\n",
    "    print (\"---------------- EJECUCIÓN #\"+str(j)+'----------------')\n",
    "    cnn1= load_model(path+str(prob)+'_SinNeg/cnn1_'+str(j)+'-exec_app1_Top_1.h5', custom_objects={'FocalLoss': focal_loss, 'focal_loss_fixed': focal_loss()})  \n",
    "    cnn2= load_model(path+str(prob)+'_SinNeg/cnn2_'+str(j)+'-exec_app1_Top_1.h5', custom_objects={'FocalLoss': focal_loss, 'focal_loss_fixed': focal_loss()}) \n",
    "    rnn1= load_model(path+str(prob)+'_SinNeg/rnn1_'+str(j)+'-exec_app1_Top_1.h5', custom_objects={'FocalLoss': focal_loss, 'focal_loss_fixed': focal_loss()}) \n",
    "    rnn2= load_model(path+str(prob)+'_SinNeg/rnn2_'+str(j)+'-exec_app1_Top_1.h5', custom_objects={'FocalLoss': focal_loss, 'focal_loss_fixed': focal_loss()}) \n",
    "    rnn3= load_model(path+str(prob)+'_SinNeg/rnn3_'+str(j)+'-exec_app1_Top_1.h5') \n",
    "    \n",
    "    list_models=['cnn1', 'cnn2', 'rnn1', 'rnn2', 'rnn3']\n",
    "    index_models=np.arange(5)\n",
    "    dict_models=dict((key, value) for (key, value) in zip(index_models,list_models))\n",
    "    modelos=[cnn1, cnn2, rnn1, rnn2, rnn3]\n",
    "    ind=np.arange(5)\n",
    "    dict_trainedModel=dict((key, value) for (key, value) in zip(ind,modelos))\n",
    "    \n",
    "    print (\"Agregando predicciones Val set\")\n",
    "    for i in dict_trainedModel.keys(): \n",
    "        a_evaluar=dict_models[i]\n",
    "        predicho=dict_trainedModel[i].predict(x_val, batch_size=bs)\n",
    "        predicho=[np.argmax(pred) for pred in predicho]\n",
    "        acc= accuracy_score(etiq_v, predicho)  \n",
    "        f1_ma=f1_score(etiq_v, predicho, average='macro')\n",
    "        f1_no=f1_score(etiq_v, predicho, average=None)\n",
    "        matriz=normalize(confusion_matrix(etiq_v, predicho))\n",
    "        fs_macro_val[dict_models[i]].append(f1_ma)\n",
    "        fs_none_val[dict_models[i]].append(f1_no)\n",
    "        accs_val[dict_models[i]].append(acc)\n",
    "        confusions_val[dict_models[i]].append(matriz)\n",
    "        \n",
    "    predicciones_all_val=[]\n",
    "    for i in dict_trainedModel.keys(): \n",
    "        a_evaluar=dict_models[i]\n",
    "        predicciones_all_val.append(dict_trainedModel[i].predict(x_val, batch_size=bs))\n",
    "    predichos_all_val=np.concatenate(np.asarray(predicciones_all_val),axis=-1)\n",
    "    \n",
    "    print (\"Agregando predicciones Test set\", dict_models[i])    \n",
    "    for i in dict_trainedModel.keys(): \n",
    "        a_evaluar=dict_models[i]\n",
    "        predicho=dict_trainedModel[i].predict(x_test, batch_size=bs)\n",
    "        predicho=[np.argmax(pred) for pred in predicho]\n",
    "        acc= accuracy_score(etiq_t, predicho)  \n",
    "        f1_ma=f1_score(etiq_t, predicho, average='macro')\n",
    "        f1_no=f1_score(etiq_t, predicho, average=None)\n",
    "        matriz=normalize(confusion_matrix(etiq_t, predicho))\n",
    "        fs_macro_test[dict_models[i]].append(f1_ma)\n",
    "        fs_none_test[dict_models[i]].append(f1_no)\n",
    "        accs_test[dict_models[i]].append(acc)\n",
    "        confusions_test[dict_models[i]].append(matriz)\n",
    "\n",
    "    predicciones_all_test=[]\n",
    "    bs=32\n",
    "    for i in dict_trainedModel.keys(): \n",
    "        a_evaluar=dict_models[i]\n",
    "        predicciones_all_test.append(dict_trainedModel[i].predict(x_test, batch_size=bs))\n",
    "    predichos_all_test=np.concatenate(np.asarray(predicciones_all_test),axis=-1)\n",
    "    \n",
    "    print (\"--------VALIDATION SET--------\")\n",
    "    \n",
    "    print (\"\\nCommittee Best Fit\")\n",
    "    trainPredicho, maxPredicho, confianzas = predecir_modelos(bs,predichos_all_val)\n",
    "    acc_comite= accuracy_score(etiq_v, trainPredicho)  \n",
    "    print(\"Accuracy:\",acc_comite)\n",
    "    f1=f1_score(etiq_v, trainPredicho, average='macro') \n",
    "    print(\"F1-macro:\",f1)\n",
    "    f1_no=f1_score(etiq_v, trainPredicho, average=None)  \n",
    "    print(\"F1-score SDQC:\",f1)\n",
    "    best['acc'].append(acc_comite)\n",
    "    best['macro'].append(f1)\n",
    "    best['none'].append(f1_no)\n",
    "    \n",
    "    print (\"\\nCommittee Norm\")\n",
    "    trainPredicho, maxPredicho, confianzas = predecir_modelos_norm(bs,predichos_all_val)\n",
    "    acc_comite= accuracy_score(etiq_v, trainPredicho)  \n",
    "    print(\"Accuracy:\",acc_comite)\n",
    "    f1=f1_score(etiq_v, trainPredicho, average='macro') \n",
    "    print(\"F1-macro:\",f1)\n",
    "    f1_no=f1_score(etiq_v, trainPredicho, average=None) \n",
    "    print(\"F1-score SDQC:\",f1)\n",
    "    norm['acc'].append(acc_comite)\n",
    "    norm['macro'].append(f1)\n",
    "    norm['none'].append(f1_no)\n",
    "\n",
    "    print (\"\\nCommittee Voting\")\n",
    "    trainPredicho, maxPredicho, confianzas = predecir_modelos_average(bs,predichos_all_val)\n",
    "    acc_comite= accuracy_score(etiq_v, trainPredicho)  \n",
    "    print(\"Accuracy:\",acc_comite)\n",
    "    f1=f1_score(etiq_v, trainPredicho, average='macro') \n",
    "    print(\"F1-macro:\",f1)\n",
    "    f1_no=f1_score(etiq_v, trainPredicho, average=None)  \n",
    "    print(\"F1-score SDQC:\",f1)\n",
    "    voting['acc'].append(acc_comite)\n",
    "    voting['macro'].append(f1)\n",
    "    voting['none'].append(f1_no)\n",
    "    \n",
    "    print (\"--------TESTING SET--------\")\n",
    "    \n",
    "    print (\"\\nCommittee Best Fit\")\n",
    "    trainPredicho, maxPredicho, confianzas = predecir_modelos(bs,predichos_all_test)\n",
    "    acc_comite= accuracy_score(etiq_t, trainPredicho)  \n",
    "    print(\"Accuracy:\",acc_comite)\n",
    "    f1=f1_score(etiq_t, trainPredicho, average='macro') \n",
    "    print(\"F1-macro:\",f1)\n",
    "    f1_no=f1_score(etiq_t, trainPredicho, average=None)  \n",
    "    print(\"F1-score SDQC:\",f1)\n",
    "    best_test['acc'].append(acc_comite)\n",
    "    best_test['macro'].append(f1)\n",
    "    best_test['none'].append(f1_no)\n",
    "\n",
    "    print (\"\\nCommittee Norm\")\n",
    "    trainPredicho, maxPredicho, confianzas = predecir_modelos_norm(bs,predichos_all_test)\n",
    "    acc_comite= accuracy_score(etiq_t, trainPredicho)  \n",
    "    print(\"Accuracy:\",acc_comite)\n",
    "    f1=f1_score(etiq_t, trainPredicho, average='macro') \n",
    "    print(\"F1-macro:\",f1)\n",
    "    f1_no=f1_score(etiq_t, trainPredicho, average=None) \n",
    "    print(\"F1-score SDQC:\",f1)\n",
    "    norm_test['acc'].append(acc_comite)\n",
    "    norm_test['macro'].append(f1)\n",
    "    norm_test['none'].append(f1_no)\n",
    "\n",
    "    print (\"\\nCommittee Voting\")\n",
    "    trainPredicho, maxPredicho, confianzas = predecir_modelos_average(bs,predichos_all_test)\n",
    "    acc_comite= accuracy_score(etiq_t, trainPredicho)  \n",
    "    print(\"Accuracy:\",acc_comite)\n",
    "    f1=f1_score(etiq_t, trainPredicho, average='macro') \n",
    "    print(\"F1-macro:\",f1)\n",
    "    f1_no=f1_score(etiq_t, trainPredicho, average=None)  \n",
    "    print(\"F1-score SDQC:\",f1)\n",
    "    voting_test['acc'].append(acc_comite)\n",
    "    voting_test['macro'].append(f1)\n",
    "    voting_test['none'].append(f1_no)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TEST] Acc Best comité: 0.6127979105452172\n",
      "[TEST] F1-score SDQC Best comité: [0.14345237 0.12074184 0.43608304 0.75546735]\n",
      "[TEST] F1 macro Best comité: 0.3639361473156908\n",
      "-----------\n",
      "[TEST] Acc Norm comité: 0.6444662095984329\n",
      "[TEST] F1-score SDQC Norm comité: [0.13510175 0.12031739 0.49074922 0.77575817]\n",
      "[TEST] F1 macro Norm comité: 0.3804816307737111\n",
      "-----------\n",
      "[TEST] Acc Voting comité: 0.6503428011753183\n",
      "[TEST] F1-score SDQC Voting comité: [0.12938678 0.16592935 0.53539333 0.77921864]\n",
      "[TEST] F1 macro Voting comité: 0.40248202300709746\n"
     ]
    }
   ],
   "source": [
    "m=3.0\n",
    "\n",
    "print (\"[TEST] Acc Best comité:\",np.sum(np.asarray(best_test['acc'])/m))\n",
    "temp=np.zeros(4)\n",
    "for result in best_test['none']:\n",
    "    temp+=result\n",
    "print (\"[TEST] F1-score SDQC Best comité:\" ,temp/m)\n",
    "print (\"[TEST] F1 macro Best comité:\",np.sum(np.asarray(best_test['macro'])/m))\n",
    "print (\"-----------\")\n",
    "print (\"[TEST] Acc Norm comité:\",np.sum(np.asarray(norm_test['acc'])/m))\n",
    "temp=np.zeros(4)\n",
    "for result in norm_test['none']:\n",
    "    temp+=result\n",
    "print (\"[TEST] F1-score SDQC Norm comité:\" ,temp/m)\n",
    "print (\"[TEST] F1 macro Norm comité:\",np.sum(np.asarray(norm_test['macro'])/m))\n",
    "print (\"-----------\")\n",
    "print (\"[TEST] Acc Voting comité:\",np.sum(np.asarray(voting_test['acc'])/m))\n",
    "temp=np.zeros(4)\n",
    "for result in voting_test['none']:\n",
    "    temp+=result\n",
    "print (\"[TEST] F1-score SDQC Voting comité:\" ,temp/m)\n",
    "print (\"[TEST] F1 macro Voting comité:\",np.sum(np.asarray(voting_test['macro'])/m))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comité 0.15 pos + neg, si es posible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------- EJECUCIÓN #1----------------\n",
      "Agregando predicciones Val set\n",
      "Agregando predicciones Test set rnn3\n",
      "--------VALIDATION SET--------\n",
      "\n",
      "Committee Best Fit\n",
      "Accuracy: 0.5302491103202847\n",
      "F1-macro: 0.4384379560885844\n",
      "F1-score SDQC: 0.4384379560885844\n",
      "\n",
      "Committee Norm\n",
      "Accuracy: 0.5765124555160143\n",
      "F1-macro: 0.4832567310270179\n",
      "F1-score SDQC: 0.4832567310270179\n",
      "\n",
      "Committee Voting\n",
      "Accuracy: 0.6014234875444839\n",
      "F1-macro: 0.5241914717064738\n",
      "F1-score SDQC: 0.5241914717064738\n",
      "--------TESTING SET--------\n",
      "\n",
      "Committee Best Fit\n",
      "Accuracy: 0.4622918707149853\n",
      "F1-macro: 0.3372228506204332\n",
      "F1-score SDQC: 0.3372228506204332\n",
      "\n",
      "Committee Norm\n",
      "Accuracy: 0.5200783545543585\n",
      "F1-macro: 0.37672603329063453\n",
      "F1-score SDQC: 0.37672603329063453\n",
      "\n",
      "Committee Voting\n",
      "Accuracy: 0.555337904015671\n",
      "F1-macro: 0.3826608008543214\n",
      "F1-score SDQC: 0.3826608008543214\n",
      "\n",
      "---------------- EJECUCIÓN #2----------------\n",
      "Agregando predicciones Val set\n",
      "Agregando predicciones Test set rnn3\n",
      "--------VALIDATION SET--------\n",
      "\n",
      "Committee Best Fit\n",
      "Accuracy: 0.49466192170818507\n",
      "F1-macro: 0.42823788009656055\n",
      "F1-score SDQC: 0.42823788009656055\n",
      "\n",
      "Committee Norm\n",
      "Accuracy: 0.5338078291814946\n",
      "F1-macro: 0.4537396095345998\n",
      "F1-score SDQC: 0.4537396095345998\n",
      "\n",
      "Committee Voting\n",
      "Accuracy: 0.5765124555160143\n",
      "F1-macro: 0.5075617494150081\n",
      "F1-score SDQC: 0.5075617494150081\n",
      "--------TESTING SET--------\n",
      "\n",
      "Committee Best Fit\n",
      "Accuracy: 0.48677766895200786\n",
      "F1-macro: 0.35544286870708713\n",
      "F1-score SDQC: 0.35544286870708713\n",
      "\n",
      "Committee Norm\n",
      "Accuracy: 0.5475024485798237\n",
      "F1-macro: 0.39323579254037533\n",
      "F1-score SDQC: 0.39323579254037533\n",
      "\n",
      "Committee Voting\n",
      "Accuracy: 0.555337904015671\n",
      "F1-macro: 0.40008788940677487\n",
      "F1-score SDQC: 0.40008788940677487\n",
      "\n",
      "---------------- EJECUCIÓN #3----------------\n",
      "Agregando predicciones Val set\n",
      "Agregando predicciones Test set rnn3\n",
      "--------VALIDATION SET--------\n",
      "\n",
      "Committee Best Fit\n",
      "Accuracy: 0.44483985765124556\n",
      "F1-macro: 0.41391716218618035\n",
      "F1-score SDQC: 0.41391716218618035\n",
      "\n",
      "Committee Norm\n",
      "Accuracy: 0.5800711743772242\n",
      "F1-macro: 0.5148904826543795\n",
      "F1-score SDQC: 0.5148904826543795\n",
      "\n",
      "Committee Voting\n",
      "Accuracy: 0.5836298932384342\n",
      "F1-macro: 0.5160091052240443\n",
      "F1-score SDQC: 0.5160091052240443\n",
      "--------TESTING SET--------\n",
      "\n",
      "Committee Best Fit\n",
      "Accuracy: 0.4534769833496572\n",
      "F1-macro: 0.32380755076970813\n",
      "F1-score SDQC: 0.32380755076970813\n",
      "\n",
      "Committee Norm\n",
      "Accuracy: 0.5475024485798237\n",
      "F1-macro: 0.3720535678565569\n",
      "F1-score SDQC: 0.3720535678565569\n",
      "\n",
      "Committee Voting\n",
      "Accuracy: 0.5465230166503428\n",
      "F1-macro: 0.38315934816721636\n",
      "F1-score SDQC: 0.38315934816721636\n"
     ]
    }
   ],
   "source": [
    "prob=0.15\n",
    "\n",
    "path='../../../new_data_augmented/stance_baselines/'\n",
    "bs=32\n",
    "    \n",
    "fs_macro_val={'cnn1':[], 'cnn2':[], 'rnn1':[],'rnn2':[],'rnn3':[]}\n",
    "fs_none_val={'cnn1':[], 'cnn2':[], 'rnn1':[],'rnn2':[],'rnn3':[]}\n",
    "accs_val={'cnn1':[], 'cnn2':[], 'rnn1':[],'rnn2':[],'rnn3':[]}\n",
    "confusions_val={'cnn1':[], 'cnn2':[], 'rnn1':[],'rnn2':[],'rnn3':[]}\n",
    "\n",
    "fs_macro_test={'cnn1':[], 'cnn2':[], 'rnn1':[],'rnn2':[],'rnn3':[]}\n",
    "fs_none_test={'cnn1':[], 'cnn2':[], 'rnn1':[],'rnn2':[],'rnn3':[]}\n",
    "accs_test={'cnn1':[], 'cnn2':[], 'rnn1':[],'rnn2':[],'rnn3':[]}\n",
    "confusions_test={'cnn1':[], 'cnn2':[], 'rnn1':[],'rnn2':[],'rnn3':[]}\n",
    "\n",
    "best={'acc':[], 'none':[], 'macro':[]}\n",
    "norm={'acc':[], 'none':[], 'macro':[]}\n",
    "voting={'acc':[], 'none':[], 'macro':[]}\n",
    "best_test={'acc':[], 'none':[], 'macro':[]}\n",
    "norm_test={'acc':[], 'none':[], 'macro':[]}\n",
    "voting_test={'acc':[], 'none':[], 'macro':[]}\n",
    "\n",
    "for j in range(1,4):\n",
    "    print (\"\")\n",
    "    print (\"---------------- EJECUCIÓN #\"+str(j)+'----------------')\n",
    "    cnn1= load_model(path+str(prob)+'_Posible/cnn1_'+str(j)+'-exec_app1_Top_1.h5', custom_objects={'FocalLoss': focal_loss, 'focal_loss_fixed': focal_loss()})  \n",
    "    cnn2= load_model(path+str(prob)+'_Posible/cnn2_'+str(j)+'-exec_app1_Top_1.h5', custom_objects={'FocalLoss': focal_loss, 'focal_loss_fixed': focal_loss()}) \n",
    "    rnn1= load_model(path+str(prob)+'_Posible/rnn1_'+str(j)+'-exec_app1_Top_1.h5', custom_objects={'FocalLoss': focal_loss, 'focal_loss_fixed': focal_loss()}) \n",
    "    rnn2= load_model(path+str(prob)+'_Posible/rnn2_'+str(j)+'-exec_app1_Top_1.h5', custom_objects={'FocalLoss': focal_loss, 'focal_loss_fixed': focal_loss()}) \n",
    "    rnn3= load_model(path+str(prob)+'_Posible/rnn3_'+str(j)+'-exec_app1_Top_1.h5') \n",
    "    \n",
    "    list_models=['cnn1', 'cnn2', 'rnn1', 'rnn2', 'rnn3']\n",
    "    index_models=np.arange(5)\n",
    "    dict_models=dict((key, value) for (key, value) in zip(index_models,list_models))\n",
    "    modelos=[cnn1, cnn2, rnn1, rnn2, rnn3]\n",
    "    ind=np.arange(5)\n",
    "    dict_trainedModel=dict((key, value) for (key, value) in zip(ind,modelos))\n",
    "    \n",
    "    print (\"Agregando predicciones Val set\")\n",
    "    for i in dict_trainedModel.keys(): \n",
    "        a_evaluar=dict_models[i]\n",
    "        predicho=dict_trainedModel[i].predict(x_val, batch_size=bs)\n",
    "        predicho=[np.argmax(pred) for pred in predicho]\n",
    "        acc= accuracy_score(etiq_v, predicho)  \n",
    "        f1_ma=f1_score(etiq_v, predicho, average='macro')\n",
    "        f1_no=f1_score(etiq_v, predicho, average=None)\n",
    "        matriz=normalize(confusion_matrix(etiq_v, predicho))\n",
    "        fs_macro_val[dict_models[i]].append(f1_ma)\n",
    "        fs_none_val[dict_models[i]].append(f1_no)\n",
    "        accs_val[dict_models[i]].append(acc)\n",
    "        confusions_val[dict_models[i]].append(matriz)\n",
    "        \n",
    "    predicciones_all_val=[]\n",
    "    for i in dict_trainedModel.keys(): \n",
    "        a_evaluar=dict_models[i]\n",
    "        predicciones_all_val.append(dict_trainedModel[i].predict(x_val, batch_size=bs))\n",
    "    predichos_all_val=np.concatenate(np.asarray(predicciones_all_val),axis=-1)\n",
    "    \n",
    "    print (\"Agregando predicciones Test set\", dict_models[i])    \n",
    "    for i in dict_trainedModel.keys(): \n",
    "        a_evaluar=dict_models[i]\n",
    "        predicho=dict_trainedModel[i].predict(x_test, batch_size=bs)\n",
    "        predicho=[np.argmax(pred) for pred in predicho]\n",
    "        acc= accuracy_score(etiq_t, predicho)  \n",
    "        f1_ma=f1_score(etiq_t, predicho, average='macro')\n",
    "        f1_no=f1_score(etiq_t, predicho, average=None)\n",
    "        matriz=normalize(confusion_matrix(etiq_t, predicho))\n",
    "        fs_macro_test[dict_models[i]].append(f1_ma)\n",
    "        fs_none_test[dict_models[i]].append(f1_no)\n",
    "        accs_test[dict_models[i]].append(acc)\n",
    "        confusions_test[dict_models[i]].append(matriz)\n",
    "\n",
    "    predicciones_all_test=[]\n",
    "    bs=32\n",
    "    for i in dict_trainedModel.keys(): \n",
    "        a_evaluar=dict_models[i]\n",
    "        predicciones_all_test.append(dict_trainedModel[i].predict(x_test, batch_size=bs))\n",
    "    predichos_all_test=np.concatenate(np.asarray(predicciones_all_test),axis=-1)\n",
    "    \n",
    "    print (\"--------VALIDATION SET--------\")\n",
    "    \n",
    "    print (\"\\nCommittee Best Fit\")\n",
    "    trainPredicho, maxPredicho, confianzas = predecir_modelos(bs,predichos_all_val)\n",
    "    acc_comite= accuracy_score(etiq_v, trainPredicho)  \n",
    "    print(\"Accuracy:\",acc_comite)\n",
    "    f1=f1_score(etiq_v, trainPredicho, average='macro') \n",
    "    print(\"F1-macro:\",f1)\n",
    "    f1_no=f1_score(etiq_v, trainPredicho, average=None)  \n",
    "    print(\"F1-score SDQC:\",f1)\n",
    "    best['acc'].append(acc_comite)\n",
    "    best['macro'].append(f1)\n",
    "    best['none'].append(f1_no)\n",
    "    \n",
    "    print (\"\\nCommittee Norm\")\n",
    "    trainPredicho, maxPredicho, confianzas = predecir_modelos_norm(bs,predichos_all_val)\n",
    "    acc_comite= accuracy_score(etiq_v, trainPredicho)  \n",
    "    print(\"Accuracy:\",acc_comite)\n",
    "    f1=f1_score(etiq_v, trainPredicho, average='macro') \n",
    "    print(\"F1-macro:\",f1)\n",
    "    f1_no=f1_score(etiq_v, trainPredicho, average=None) \n",
    "    print(\"F1-score SDQC:\",f1)\n",
    "    norm['acc'].append(acc_comite)\n",
    "    norm['macro'].append(f1)\n",
    "    norm['none'].append(f1_no)\n",
    "\n",
    "    print (\"\\nCommittee Voting\")\n",
    "    trainPredicho, maxPredicho, confianzas = predecir_modelos_average(bs,predichos_all_val)\n",
    "    acc_comite= accuracy_score(etiq_v, trainPredicho)  \n",
    "    print(\"Accuracy:\",acc_comite)\n",
    "    f1=f1_score(etiq_v, trainPredicho, average='macro') \n",
    "    print(\"F1-macro:\",f1)\n",
    "    f1_no=f1_score(etiq_v, trainPredicho, average=None)  \n",
    "    print(\"F1-score SDQC:\",f1)\n",
    "    voting['acc'].append(acc_comite)\n",
    "    voting['macro'].append(f1)\n",
    "    voting['none'].append(f1_no)\n",
    "    \n",
    "    print (\"--------TESTING SET--------\")\n",
    "    \n",
    "    print (\"\\nCommittee Best Fit\")\n",
    "    trainPredicho, maxPredicho, confianzas = predecir_modelos(bs,predichos_all_test)\n",
    "    acc_comite= accuracy_score(etiq_t, trainPredicho)  \n",
    "    print(\"Accuracy:\",acc_comite)\n",
    "    f1=f1_score(etiq_t, trainPredicho, average='macro') \n",
    "    print(\"F1-macro:\",f1)\n",
    "    f1_no=f1_score(etiq_t, trainPredicho, average=None)  \n",
    "    print(\"F1-score SDQC:\",f1)\n",
    "    best_test['acc'].append(acc_comite)\n",
    "    best_test['macro'].append(f1)\n",
    "    best_test['none'].append(f1_no)\n",
    "\n",
    "    print (\"\\nCommittee Norm\")\n",
    "    trainPredicho, maxPredicho, confianzas = predecir_modelos_norm(bs,predichos_all_test)\n",
    "    acc_comite= accuracy_score(etiq_t, trainPredicho)  \n",
    "    print(\"Accuracy:\",acc_comite)\n",
    "    f1=f1_score(etiq_t, trainPredicho, average='macro') \n",
    "    print(\"F1-macro:\",f1)\n",
    "    f1_no=f1_score(etiq_t, trainPredicho, average=None) \n",
    "    print(\"F1-score SDQC:\",f1)\n",
    "    norm_test['acc'].append(acc_comite)\n",
    "    norm_test['macro'].append(f1)\n",
    "    norm_test['none'].append(f1_no)\n",
    "\n",
    "    print (\"\\nCommittee Voting\")\n",
    "    trainPredicho, maxPredicho, confianzas = predecir_modelos_average(bs,predichos_all_test)\n",
    "    acc_comite= accuracy_score(etiq_t, trainPredicho)  \n",
    "    print(\"Accuracy:\",acc_comite)\n",
    "    f1=f1_score(etiq_t, trainPredicho, average='macro') \n",
    "    print(\"F1-macro:\",f1)\n",
    "    f1_no=f1_score(etiq_t, trainPredicho, average=None)  \n",
    "    print(\"F1-score SDQC:\",f1)\n",
    "    voting_test['acc'].append(acc_comite)\n",
    "    voting_test['macro'].append(f1)\n",
    "    voting_test['none'].append(f1_no)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TEST] Acc Best comité: 0.4675155076722168\n",
      "[TEST] F1-score SDQC Best comité: [0.14587998 0.1501818  0.44104826 0.61818765]\n",
      "[TEST] F1 macro Best comité: 0.3388244233657428\n",
      "-----------\n",
      "[TEST] Acc Norm comité: 0.5383610839046685\n",
      "[TEST] F1-score SDQC Norm comité: [0.14583747 0.18396295 0.50922599 0.68366078]\n",
      "[TEST] F1 macro Norm comité: 0.3806717978958556\n",
      "-----------\n",
      "[TEST] Acc Voting comité: 0.5523996082272282\n",
      "[TEST] F1-score SDQC Voting comité: [0.15557567 0.16744758 0.53563062 0.69589018]\n",
      "[TEST] F1 macro Voting comité: 0.3886360128094376\n"
     ]
    }
   ],
   "source": [
    "m=3.0\n",
    "\n",
    "print (\"[TEST] Acc Best comité:\",np.sum(np.asarray(best_test['acc'])/m))\n",
    "temp=np.zeros(4)\n",
    "for result in best_test['none']:\n",
    "    temp+=result\n",
    "print (\"[TEST] F1-score SDQC Best comité:\" ,temp/m)\n",
    "print (\"[TEST] F1 macro Best comité:\",np.sum(np.asarray(best_test['macro'])/m))\n",
    "print (\"-----------\")\n",
    "print (\"[TEST] Acc Norm comité:\",np.sum(np.asarray(norm_test['acc'])/m))\n",
    "temp=np.zeros(4)\n",
    "for result in norm_test['none']:\n",
    "    temp+=result\n",
    "print (\"[TEST] F1-score SDQC Norm comité:\" ,temp/m)\n",
    "print (\"[TEST] F1 macro Norm comité:\",np.sum(np.asarray(norm_test['macro'])/m))\n",
    "print (\"-----------\")\n",
    "print (\"[TEST] Acc Voting comité:\",np.sum(np.asarray(voting_test['acc'])/m))\n",
    "temp=np.zeros(4)\n",
    "for result in voting_test['none']:\n",
    "    temp+=result\n",
    "print (\"[TEST] F1-score SDQC Voting comité:\" ,temp/m)\n",
    "print (\"[TEST] F1 macro Voting comité:\",np.sum(np.asarray(voting_test['macro'])/m))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:newpy3_tf1]",
   "language": "python",
   "name": "conda-env-newpy3_tf1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
