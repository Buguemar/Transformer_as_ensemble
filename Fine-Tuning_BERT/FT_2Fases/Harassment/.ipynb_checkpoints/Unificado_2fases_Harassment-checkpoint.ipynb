{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import os\n",
    "from sc_ft import *\n",
    "from lm_ft import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: GeForce GTX 1060 6GB\n"
     ]
    }
   ],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "epoch_macro= 1\n",
    "\n",
    "MODEL_CLASSES = {\n",
    "    \"openai-gpt\": (OpenAIGPTConfig, OpenAIGPTLMHeadModel, OpenAIGPTTokenizer),\n",
    "    \"bert\": (BertConfig, BertForMaskedLM, BertTokenizer),\n",
    "    \"roberta\": (RobertaConfig, RobertaForMaskedLM, RobertaTokenizer),\n",
    "}\n",
    "\n",
    "if torch.cuda.is_available():       \n",
    "    device = torch.device(\"cuda\")\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args_LM:\n",
    "    adam_epsilon=1e-8\n",
    "    block_size=-1 \n",
    "    cache_dir=\"\"\n",
    "    config_name=\"\"\n",
    "    do_train=True        \n",
    "    do_eval=True \n",
    "    do_lower_case=True\n",
    "    eval_all_checkpoints=False\n",
    "    evaluate_during_training=False\n",
    "    eval_data_file=None\n",
    "    fp16= False\n",
    "    fp16_opt_level=\"O1\"\n",
    "    gradient_accumulation_steps=1\n",
    "    learning_rate=5e-5\n",
    "    logging_steps=100\n",
    "    local_rank=-1\n",
    "    model_type= default=\"bert\"\n",
    "    model_name_or_path= \"bert-base-cased\" \n",
    "    mlm=True\n",
    "    mlm_probability=0.15\n",
    "    max_grad_norm=1.0\n",
    "    max_steps=-1\n",
    "    num_train_epochs=1.0\n",
    "    n_gpu=1\n",
    "    no_cuda=False\n",
    "    overwrite_output_dir=True\n",
    "    overwrite_cache=False\n",
    "    output_dir=None\n",
    "    per_gpu_train_batch_size=4\n",
    "    per_gpu_eval_batch_size=4\n",
    "    save_steps=100\n",
    "    save_total_limit=None    \n",
    "    seed=42   \n",
    "    server_ip=\"\"\n",
    "    server_port=\"\"\n",
    "    train_data_file=None #str\n",
    "    tokenizer_name=\"\" \n",
    "    weight_decay=0.0   \n",
    "    warmup_steps=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "out= '../../../../model_save/Dos-Fases-all_Harassment/'\n",
    "\n",
    "my_args = Args_LM()\n",
    "my_args.output_dir = out\n",
    "my_args.model_type = 'bert'\n",
    "my_args.train_data_file = '../../CSV_Harassment/Train_data_harassment.txt'\n",
    "my_args.block_size = 64\n",
    "my_args.eval_data_file = '../../CSV_Harassment/Test_data_harassment.txt'\n",
    "my_args.per_gpu_train_batch_size = 16\n",
    "my_args.per_gpu_eval_batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fase_LM(from_pre, my_args, logger):\n",
    "    if from_pre:\n",
    "        model_dir_ft = 'bert-base-uncased'\n",
    "        my_args.model_name_or_path = model_dir_ft\n",
    "    else:\n",
    "        model_dir_ft = out\n",
    "        my_args.model_name_or_path = model_dir_ft\n",
    "    \n",
    "    if my_args.model_type in [\"bert\", \"roberta\"] and not my_args.mlm:\n",
    "        raise ValueError(\n",
    "            \"BERT and RoBERTa do not have LM heads but masked LM heads. They must be run using the --mlm \"\n",
    "            \"flag (masked language modeling).\"\n",
    "        )\n",
    "    if my_args.eval_data_file is None and my_args.do_eval:\n",
    "        raise ValueError(\n",
    "            \"Cannot do evaluation without an evaluation data file. Either supply a file to --eval_data_file \"\n",
    "            \"or remove the --do_eval argument.\"\n",
    "        )\n",
    "\n",
    "    if (\n",
    "        os.path.exists(my_args.output_dir)\n",
    "        and os.listdir(my_args.output_dir)\n",
    "        and my_args.do_train\n",
    "        and not my_args.overwrite_output_dir\n",
    "    ):\n",
    "        raise ValueError(\n",
    "            \"Output directory ({}) already exists and is not empty. Use --overwrite_output_dir to overcome.\".format(\n",
    "                my_args.output_dir\n",
    "            )\n",
    "        )\n",
    "\n",
    "    if my_args.server_ip and my_args.server_port:\n",
    "        import ptvsd\n",
    "        #print(\"Waiting for debugger attach\")\n",
    "        ptvsd.enable_attach(address=(my_args.server_ip, my_args.server_port), redirect_output=True)\n",
    "        ptvsd.wait_for_attach()\n",
    "    if my_args.local_rank == -1 or my_args.no_cuda:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() and not my_args.no_cuda else \"cpu\")\n",
    "        my_args.n_gpu = torch.cuda.device_count()\n",
    "    else:  # Initializes the distributed backend which will take care of sychronizing nodes/GPUs\n",
    "        torch.cuda.set_device(my_args.local_rank)\n",
    "        device = torch.device(\"cuda\", my_args.local_rank)\n",
    "        torch.distributed.init_process_group(backend=\"nccl\")\n",
    "        my_args.n_gpu = 1\n",
    "    my_args.device = device\n",
    "    logging.basicConfig(\n",
    "        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
    "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "        level=logging.INFO if my_args.local_rank in [-1, 0] else logging.WARN,\n",
    "    )\n",
    "    logger.warning(\n",
    "        \"Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s\",\n",
    "        my_args.local_rank,\n",
    "        device,\n",
    "        my_args.n_gpu,\n",
    "        bool(my_args.local_rank != -1),\n",
    "        my_args.fp16,\n",
    "    )\n",
    "    set_seed(my_args)\n",
    "    if my_args.local_rank not in [-1, 0]:\n",
    "        torch.distributed.barrier()  # Barrier to make sure only the first process in distributed training download model & vocab\n",
    "    config_class, model_class, tokenizer_class = MODEL_CLASSES[my_args.model_type]\n",
    "    #print (\"\\nLoadding model \", model_class)\n",
    "    config = config_class.from_pretrained(\n",
    "        my_args.config_name if my_args.config_name else my_args.model_name_or_path,\n",
    "        cache_dir=my_args.cache_dir if my_args.cache_dir else None,\n",
    "    )\n",
    "    tokenizer = tokenizer_class.from_pretrained(\n",
    "        my_args.tokenizer_name if my_args.tokenizer_name else my_args.model_name_or_path,\n",
    "        do_lower_case=my_args.do_lower_case,\n",
    "        cache_dir=my_args.cache_dir if my_args.cache_dir else None,\n",
    "    )\n",
    "    if my_args.block_size <= 0:\n",
    "        my_args.block_size = (\n",
    "            tokenizer.max_len_single_sentence\n",
    "        )  # Our input block size will be the max possible for the model\n",
    "    my_args.block_size = min(my_args.block_size, tokenizer.max_len_single_sentence)\n",
    "    model = model_class.from_pretrained(\n",
    "        my_args.model_name_or_path,\n",
    "        from_tf=bool(\".ckpt\" in my_args.model_name_or_path),\n",
    "        config=config,\n",
    "        cache_dir=my_args.cache_dir if my_args.cache_dir else None,\n",
    "    )\n",
    "    model.to(my_args.device)\n",
    "    if my_args.local_rank == 0:\n",
    "        torch.distributed.barrier()  # End of barrier to make sure only the first process in distributed training download model & vocab\n",
    "    logger.info(\"Training/evaluation parameters %s\", my_args)\n",
    "####################################################################################################\n",
    "    # Training\n",
    "    if my_args.do_train:\n",
    "        if my_args.local_rank not in [-1, 0]:\n",
    "            torch.distributed.barrier()\n",
    "        train_dataset = load_and_cache_examples(my_args, tokenizer, logger, evaluate=False)\n",
    "        if my_args.local_rank == 0:\n",
    "            torch.distributed.barrier()\n",
    "        global_step, tr_loss = train(my_args, train_dataset, model, tokenizer, logger)\n",
    "        logger.info(\" global_step = %s, average loss = %s\", global_step, tr_loss)\n",
    "        \n",
    "    if my_args.do_train and (my_args.local_rank == -1 or torch.distributed.get_rank() == 0):\n",
    "        if not os.path.exists(my_args.output_dir) and my_args.local_rank in [-1, 0]:\n",
    "            os.makedirs(my_args.output_dir)\n",
    "        logger.info(\"Saving model checkpoint to %s\", my_args.output_dir)\n",
    "        model_to_save = (\n",
    "            model.module if hasattr(model, \"module\") else model\n",
    "        )  # Take care of distributed/parallel training\n",
    "        model_to_save.save_pretrained(my_args.output_dir)\n",
    "        tokenizer.save_pretrained(my_args.output_dir)\n",
    "        torch.save(my_args, os.path.join(my_args.output_dir, \"training_args.bin\"))\n",
    "\n",
    "        model = model_class.from_pretrained(my_args.output_dir)\n",
    "        tokenizer = tokenizer_class.from_pretrained(my_args.output_dir, do_lower_case=my_args.do_lower_case)\n",
    "        model.to(my_args.device)\n",
    "\n",
    "    # Evaluation\n",
    "    results = {}\n",
    "    if my_args.do_eval and my_args.local_rank in [-1, 0]:\n",
    "        checkpoints = [my_args.output_dir]\n",
    "        if my_args.eval_all_checkpoints:\n",
    "            checkpoints = list(\n",
    "                os.path.dirname(c) for c in sorted(glob.glob(my_args.output_dir + \"/**/\" + WEIGHTS_NAME, recursive=True))\n",
    "            )\n",
    "            logging.getLogger(\"transformers.modeling_utils\").setLevel(logging.WARN)  # Reduce logging\n",
    "        logger.info(\"Evaluate the following checkpoints: %s\", checkpoints)\n",
    "        for checkpoint in checkpoints:\n",
    "            global_step = checkpoint.split(\"-\")[-1] if len(checkpoints) > 1 else \"\"\n",
    "            prefix = checkpoint.split(\"/\")[-1] if checkpoint.find(\"checkpoint\") != -1 else \"\"\n",
    "\n",
    "            model = model_class.from_pretrained(checkpoint)\n",
    "            model.to(my_args.device)\n",
    "            result = evaluate(my_args, model, tokenizer, logger, prefix=prefix)\n",
    "            result = dict((k + \"_{}\".format(global_step), v) for k, v in result.items())\n",
    "            results.update(result)\n",
    "            \n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fase_SC(from_pre, batch_si, d_lab, max_len):\n",
    "    if from_pre:\n",
    "        model_dir_ft = 'bert-base-uncased'\n",
    "    else:\n",
    "        model_dir_ft = '../../../../model_save/Dos-Fases-all_Harassment/'\n",
    "    print('Loading BERT tokenizer...')\n",
    "    \n",
    "    tokenizer = BertTokenizer.from_pretrained(model_dir_ft, do_lower_case=True)\n",
    "    print('Loading BERT Seq Class from ', model_dir_ft, '...' )\n",
    "    model_loaded = BertForSequenceClassification.from_pretrained(model_dir_ft, num_labels=4) \n",
    "    model_loaded.cuda()\n",
    "\n",
    "    print (\"Modelo cargado correctamente desde \", model_dir_ft)\n",
    "    df_read=pd.read_csv(\"../../CSV_Harassment/Train_data_compeition.csv\")\n",
    "    \n",
    "    #etiq=[]\n",
    "    #for et in df_read['Label'].values:\n",
    "    #    etiq.append(d_lab[et])\n",
    "    \n",
    "    ys=df_read.values[:,2:]\n",
    "    #y_train_mul=np.zeros((tr.shape[0],4))\n",
    "    y_train=[]\n",
    "    i=0\n",
    "    for label in ys:\n",
    "        if np.sum(label)==0:\n",
    "            #y_train_mul[i][0]=1\n",
    "            y_train.append(0)\n",
    "        else: \n",
    "            #y_train_mul[i]=np.concatenate([[0],label[1:]])\n",
    "            y_train.append(np.argmax(np.concatenate([[0],label[1:]])))\n",
    "        i+=1\n",
    "    \n",
    "    sentences = df_read['tweet_content'].values\n",
    "    n_labels = np.array(y_train)\n",
    "    \n",
    "    input_ids = []\n",
    "    for sent in sentences:\n",
    "        myc, myc_list= my_cleaner(sent)\n",
    "        encoded_sent = tokenizer.encode(myc,add_special_tokens = True )    \n",
    "        input_ids.append(encoded_sent)\n",
    "        \n",
    "    input_ids, attention_masks = make_padding_and_masks(max_len, tokenizer, input_ids)\n",
    "    train_all, val_all = data_batches(input_ids, attention_masks, n_labels, test_size=0.1, batch_size=batch_si, mode='train')\n",
    "    train_data, train_sampler, train_dataloader = train_all\n",
    "    validation_data, validation_sampler, validation_dataloader = val_all\n",
    "\n",
    "    optimizer = AdamW(model_loaded.parameters(), lr = 2e-5, eps = 1e-8)\n",
    "    epochs = 1\n",
    "    total_steps = len(train_dataloader) * epochs\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps = 0, num_training_steps = total_steps)\n",
    "\n",
    "    seed_val = 42\n",
    "    random.seed(seed_val)\n",
    "    np.random.seed(seed_val)\n",
    "    torch.manual_seed(seed_val)\n",
    "    torch.cuda.manual_seed_all(seed_val)\n",
    "    loss_values = []\n",
    "    model_loaded.zero_grad()    \n",
    "    \n",
    "    total_loss = 0\n",
    "    model_loaded.train()        \n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        model_loaded.train()\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)                \n",
    "        outputs = model_loaded(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
    "        loss = outputs[0]\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model_loaded.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        model_loaded.zero_grad()\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_dataloader)       \n",
    "    loss_values.append(avg_train_loss)\n",
    "    #print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    #print(\"Running Validation...\")\n",
    "    model_loaded.eval()\n",
    "    eval_loss, eval_accuracy, eval_fscore = 0, 0, 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "    for batch in validation_dataloader:\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        with torch.no_grad():              \n",
    "            outputs = model_loaded(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)        \n",
    "        logits = outputs[0]\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()     \n",
    "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "        eval_accuracy += tmp_eval_accuracy\n",
    "        tmp_eval_fscore = flat_fscore(logits, label_ids)\n",
    "        eval_fscore += tmp_eval_fscore\n",
    "        nb_eval_steps += 1\n",
    "    #print(\"  F-score macro: {0:.2f}\".format(eval_fscore/nb_eval_steps))\n",
    "    #print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
    "    #df_read=pd.read_csv(\"../CSV_Stance/dev_semeval_raw.csv\")\n",
    "    df_read=pd.read_csv(\"../../CSV_Harassment/testset-competition.csv\")\n",
    "    \n",
    "    #etiq=[]\n",
    "    #for et in df_read['Label'].values:\n",
    "    #    etiq.append(d_lab[et])\n",
    "    \n",
    "    ys=df_read.values[:,2:]\n",
    "    y_test=[]\n",
    "    i=0\n",
    "    for label in ys:\n",
    "        if np.sum(label)==0:\n",
    "            y_test.append(0)\n",
    "        else: \n",
    "            y_test.append(np.argmax(np.concatenate([[0],label[1:]])))\n",
    "        i+=1\n",
    "  \n",
    "    sentences = df_read['tweet_content'].values\n",
    "    n_labels = np.array(y_test)\n",
    "    \n",
    "    input_ids = []\n",
    "    for sent in sentences:\n",
    "        myc, myc_list= my_cleaner(sent)\n",
    "        encoded_sent = tokenizer.encode(myc,add_special_tokens = True )   \n",
    "        input_ids.append(encoded_sent)\n",
    "    input_ids, attention_masks = make_padding_and_masks(max_len, tokenizer, input_ids)\n",
    "    pred_all = data_batches(input_ids, attention_masks, n_labels, test_size=0.1, batch_size=batch_si, mode='eval')\n",
    "    prediction_data, prediction_sampler, prediction_dataloader = pred_all\n",
    "    \n",
    "    model_loaded.eval()\n",
    "    predictions , true_labels = [], []\n",
    "    for batch in prediction_dataloader:\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        with torch.no_grad():\n",
    "            outputs = model_loaded(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "        logits = outputs[0]\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "        predictions.append(logits)\n",
    "        true_labels.append(label_ids)\n",
    "    f_macros = []\n",
    "    print('Calculating F-macro...')\n",
    "    flat_predictions = [item for sublist in predictions for item in sublist]\n",
    "    flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
    "    flat_true_labels = [item for sublist in true_labels for item in sublist]\n",
    "    fma = f1_score(flat_true_labels, flat_predictions, average='macro')\n",
    "    fno = f1_score(flat_true_labels, flat_predictions, average=None)\n",
    "    acc = accuracy_score(flat_true_labels, flat_predictions)\n",
    "    print('F macro: %.3f' % fma)\n",
    "    print('F macro none average: ', fno)\n",
    "    print('Accuracy: %.3f' % acc)\n",
    "    \n",
    "    state = {\n",
    "            'device': device,\n",
    "            'f_macro': fma,\n",
    "            'F_none': fno,\n",
    "            'Accuracy': acc,\n",
    "            'epoch': epoch_macro,\n",
    "            'batch_size': batch_si, \n",
    "            'max_len': max_len,\n",
    "            'optimizer': optimizer ,\n",
    "            'orden_epoch': 'primero'\n",
    "        }\n",
    "    save_model_to(out, model_loaded, tokenizer, state)\n",
    "    \n",
    "    del model_loaded\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    return fma, fno, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import resource\n",
    "\n",
    "def mem():\n",
    "    print('Memory usage         : % 2.2f MB' % round(\n",
    "        resource.getrusage(resource.RUSAGE_SELF).ru_maxrss/1024.0/1024.0,1)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/05/2020 01:17:07 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "03/05/2020 01:17:08 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /home/mbugueno/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.8f56353af4a709bf5ff0fbc915d8f5b42bfff892cbb6ac98c3c45f481a03c685\n",
      "03/05/2020 01:17:08 - INFO - transformers.configuration_utils -   Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": null,\n",
      "  \"do_sample\": false,\n",
      "  \"eos_token_ids\": null,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": null,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "03/05/2020 01:17:09 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/mbugueno/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "03/05/2020 01:17:09 - INFO - transformers.modeling_utils -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /home/mbugueno/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
      "03/05/2020 01:17:12 - INFO - transformers.modeling_utils -   Weights of BertForMaskedLM not initialized from pretrained model: ['cls.predictions.decoder.bias']\n",
      "03/05/2020 01:17:12 - INFO - transformers.modeling_utils -   Weights from pretrained model not used in BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "03/05/2020 01:17:13 - INFO - __main__ -   Training/evaluation parameters <__main__.Args_LM object at 0x7f7ea377dd30>\n",
      "03/05/2020 01:17:13 - INFO - __main__ -   ***** Running training *****\n",
      "03/05/2020 01:17:13 - INFO - __main__ -     Num examples = 2458\n",
      "03/05/2020 01:17:13 - INFO - __main__ -     Num Epochs = 1\n",
      "03/05/2020 01:17:13 - INFO - __main__ -     Instantaneous batch size per GPU = 16\n",
      "03/05/2020 01:17:13 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "03/05/2020 01:17:13 - INFO - __main__ -     Gradient Accumulation steps = 1\n",
      "03/05/2020 01:17:13 - INFO - __main__ -     Total optimization steps = 154\n",
      "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Iteration:   0%|          | 0/154 [00:00<?, ?it/s]\u001b[A\n",
      "Iteration:   1%|          | 1/154 [00:00<01:20,  1.90it/s]\u001b[A\n",
      "Iteration:   1%|▏         | 2/154 [00:00<01:11,  2.12it/s]\u001b[A\n",
      "Iteration:   2%|▏         | 3/154 [00:01<01:05,  2.30it/s]\u001b[A\n",
      "Iteration:   3%|▎         | 4/154 [00:01<01:01,  2.45it/s]\u001b[A\n",
      "Iteration:   3%|▎         | 5/154 [00:01<00:58,  2.56it/s]\u001b[A\n",
      "Iteration:   4%|▍         | 6/154 [00:02<00:55,  2.66it/s]\u001b[A\n",
      "Iteration:   5%|▍         | 7/154 [00:02<00:53,  2.72it/s]\u001b[A\n",
      "Iteration:   5%|▌         | 8/154 [00:02<00:52,  2.77it/s]\u001b[A\n",
      "Iteration:   6%|▌         | 9/154 [00:03<00:51,  2.81it/s]\u001b[A\n",
      "Iteration:   6%|▋         | 10/154 [00:03<00:50,  2.84it/s]\u001b[A\n",
      "Iteration:   7%|▋         | 11/154 [00:03<00:50,  2.85it/s]\u001b[A\n",
      "Iteration:   8%|▊         | 12/154 [00:04<00:49,  2.87it/s]\u001b[A\n",
      "Iteration:   8%|▊         | 13/154 [00:04<00:49,  2.87it/s]\u001b[A\n",
      "Iteration:   9%|▉         | 14/154 [00:05<00:48,  2.88it/s]\u001b[A\n",
      "Iteration:  10%|▉         | 15/154 [00:05<00:48,  2.88it/s]\u001b[A\n",
      "Iteration:  10%|█         | 16/154 [00:05<00:47,  2.88it/s]\u001b[A\n",
      "Iteration:  11%|█         | 17/154 [00:06<00:47,  2.88it/s]\u001b[A\n",
      "Iteration:  12%|█▏        | 18/154 [00:06<00:47,  2.89it/s]\u001b[A\n",
      "Iteration:  12%|█▏        | 19/154 [00:06<00:46,  2.89it/s]\u001b[A\n",
      "Iteration:  13%|█▎        | 20/154 [00:07<00:48,  2.78it/s]\u001b[A\n",
      "Iteration:  14%|█▎        | 21/154 [00:07<00:47,  2.81it/s]\u001b[A\n",
      "Iteration:  14%|█▍        | 22/154 [00:07<00:46,  2.81it/s]\u001b[A\n",
      "Iteration:  15%|█▍        | 23/154 [00:08<00:46,  2.84it/s]\u001b[A\n",
      "Iteration:  16%|█▌        | 24/154 [00:08<00:46,  2.80it/s]\u001b[A\n",
      "Iteration:  16%|█▌        | 25/154 [00:08<00:45,  2.83it/s]\u001b[A\n",
      "Iteration:  17%|█▋        | 26/154 [00:09<00:44,  2.85it/s]\u001b[A\n",
      "Iteration:  18%|█▊        | 27/154 [00:09<00:44,  2.84it/s]\u001b[A\n",
      "Iteration:  18%|█▊        | 28/154 [00:09<00:44,  2.85it/s]\u001b[A\n",
      "Iteration:  19%|█▉        | 29/154 [00:10<00:44,  2.79it/s]\u001b[A\n",
      "Iteration:  19%|█▉        | 30/154 [00:10<00:45,  2.74it/s]\u001b[A\n",
      "Iteration:  20%|██        | 31/154 [00:11<00:45,  2.72it/s]\u001b[A\n",
      "Iteration:  21%|██        | 32/154 [00:11<00:44,  2.75it/s]\u001b[A\n",
      "Iteration:  21%|██▏       | 33/154 [00:11<00:43,  2.79it/s]\u001b[A\n",
      "Iteration:  22%|██▏       | 34/154 [00:12<00:42,  2.81it/s]\u001b[A\n",
      "Iteration:  23%|██▎       | 35/154 [00:12<00:42,  2.83it/s]\u001b[A\n",
      "Iteration:  23%|██▎       | 36/154 [00:12<00:41,  2.85it/s]\u001b[A\n",
      "Iteration:  24%|██▍       | 37/154 [00:13<00:41,  2.85it/s]\u001b[A\n",
      "Iteration:  25%|██▍       | 38/154 [00:13<00:41,  2.82it/s]\u001b[A\n",
      "Iteration:  25%|██▌       | 39/154 [00:13<00:41,  2.80it/s]\u001b[A\n",
      "Iteration:  26%|██▌       | 40/154 [00:14<00:40,  2.81it/s]\u001b[A\n",
      "Iteration:  27%|██▋       | 41/154 [00:14<00:40,  2.81it/s]\u001b[A\n",
      "Iteration:  27%|██▋       | 42/154 [00:14<00:39,  2.83it/s]\u001b[A\n",
      "Iteration:  28%|██▊       | 43/154 [00:15<00:39,  2.83it/s]\u001b[A\n",
      "Iteration:  29%|██▊       | 44/154 [00:15<00:38,  2.85it/s]\u001b[A\n",
      "Iteration:  29%|██▉       | 45/154 [00:16<00:38,  2.83it/s]\u001b[A\n",
      "Iteration:  30%|██▉       | 46/154 [00:16<00:37,  2.85it/s]\u001b[A\n",
      "Iteration:  31%|███       | 47/154 [00:16<00:37,  2.86it/s]\u001b[A\n",
      "Iteration:  31%|███       | 48/154 [00:17<00:36,  2.87it/s]\u001b[A\n",
      "Iteration:  32%|███▏      | 49/154 [00:17<00:36,  2.86it/s]\u001b[A\n",
      "Iteration:  32%|███▏      | 50/154 [00:17<00:36,  2.86it/s]\u001b[A\n",
      "Iteration:  33%|███▎      | 51/154 [00:18<00:35,  2.86it/s]\u001b[A\n",
      "Iteration:  34%|███▍      | 52/154 [00:18<00:35,  2.87it/s]\u001b[A\n",
      "Iteration:  34%|███▍      | 53/154 [00:18<00:35,  2.86it/s]\u001b[A\n",
      "Iteration:  35%|███▌      | 54/154 [00:19<00:35,  2.84it/s]\u001b[A\n",
      "Iteration:  36%|███▌      | 55/154 [00:19<00:34,  2.85it/s]\u001b[A\n",
      "Iteration:  36%|███▋      | 56/154 [00:19<00:34,  2.85it/s]\u001b[A\n",
      "Iteration:  37%|███▋      | 57/154 [00:20<00:34,  2.85it/s]\u001b[A\n",
      "Iteration:  38%|███▊      | 58/154 [00:20<00:33,  2.83it/s]\u001b[A\n",
      "Iteration:  38%|███▊      | 59/154 [00:20<00:33,  2.83it/s]\u001b[A\n",
      "Iteration:  39%|███▉      | 60/154 [00:21<00:33,  2.85it/s]\u001b[A\n",
      "Iteration:  40%|███▉      | 61/154 [00:21<00:32,  2.85it/s]\u001b[A\n",
      "Iteration:  40%|████      | 62/154 [00:21<00:32,  2.84it/s]\u001b[A\n",
      "Iteration:  41%|████      | 63/154 [00:22<00:31,  2.86it/s]\u001b[A\n",
      "Iteration:  42%|████▏     | 64/154 [00:22<00:31,  2.86it/s]\u001b[A\n",
      "Iteration:  42%|████▏     | 65/154 [00:23<00:31,  2.86it/s]\u001b[A\n",
      "Iteration:  43%|████▎     | 66/154 [00:23<00:30,  2.85it/s]\u001b[A\n",
      "Iteration:  44%|████▎     | 67/154 [00:23<00:30,  2.86it/s]\u001b[A\n",
      "Iteration:  44%|████▍     | 68/154 [00:24<00:30,  2.86it/s]\u001b[A\n",
      "Iteration:  45%|████▍     | 69/154 [00:24<00:29,  2.86it/s]\u001b[A\n",
      "Iteration:  45%|████▌     | 70/154 [00:24<00:29,  2.85it/s]\u001b[A\n",
      "Iteration:  46%|████▌     | 71/154 [00:25<00:29,  2.80it/s]\u001b[A\n",
      "Iteration:  47%|████▋     | 72/154 [00:25<00:29,  2.82it/s]\u001b[A\n",
      "Iteration:  47%|████▋     | 73/154 [00:25<00:28,  2.83it/s]\u001b[A\n",
      "Iteration:  48%|████▊     | 74/154 [00:26<00:28,  2.84it/s]\u001b[A\n",
      "Iteration:  49%|████▊     | 75/154 [00:26<00:27,  2.84it/s]\u001b[A\n",
      "Iteration:  49%|████▉     | 76/154 [00:26<00:27,  2.84it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:  50%|█████     | 77/154 [00:27<00:26,  2.86it/s]\u001b[A\n",
      "Iteration:  51%|█████     | 78/154 [00:27<00:26,  2.86it/s]\u001b[A\n",
      "Iteration:  51%|█████▏    | 79/154 [00:27<00:26,  2.86it/s]\u001b[A\n",
      "Iteration:  52%|█████▏    | 80/154 [00:28<00:25,  2.87it/s]\u001b[A\n",
      "Iteration:  53%|█████▎    | 81/154 [00:28<00:25,  2.84it/s]\u001b[A\n",
      "Iteration:  53%|█████▎    | 82/154 [00:29<00:25,  2.82it/s]\u001b[A\n",
      "Iteration:  54%|█████▍    | 83/154 [00:29<00:25,  2.83it/s]\u001b[A\n",
      "Iteration:  55%|█████▍    | 84/154 [00:29<00:25,  2.78it/s]\u001b[A\n",
      "Iteration:  55%|█████▌    | 85/154 [00:30<00:24,  2.78it/s]\u001b[A\n",
      "Iteration:  56%|█████▌    | 86/154 [00:30<00:24,  2.78it/s]\u001b[A\n",
      "Iteration:  56%|█████▋    | 87/154 [00:30<00:25,  2.68it/s]\u001b[A\n",
      "Iteration:  57%|█████▋    | 88/154 [00:31<00:24,  2.64it/s]\u001b[A\n",
      "Iteration:  58%|█████▊    | 89/154 [00:31<00:25,  2.56it/s]\u001b[A\n",
      "Iteration:  58%|█████▊    | 90/154 [00:32<00:25,  2.56it/s]\u001b[A\n",
      "Iteration:  59%|█████▉    | 91/154 [00:32<00:24,  2.62it/s]\u001b[A\n",
      "Iteration:  60%|█████▉    | 92/154 [00:32<00:23,  2.59it/s]\u001b[A\n",
      "Iteration:  60%|██████    | 93/154 [00:33<00:23,  2.64it/s]\u001b[A\n",
      "Iteration:  61%|██████    | 94/154 [00:33<00:22,  2.68it/s]\u001b[A\n",
      "Iteration:  62%|██████▏   | 95/154 [00:33<00:21,  2.73it/s]\u001b[A\n",
      "Iteration:  62%|██████▏   | 96/154 [00:34<00:21,  2.75it/s]\u001b[A\n",
      "Iteration:  63%|██████▎   | 97/154 [00:34<00:20,  2.77it/s]\u001b[A\n",
      "Iteration:  64%|██████▎   | 98/154 [00:34<00:20,  2.79it/s]\u001b[A\n",
      "Iteration:  64%|██████▍   | 99/154 [00:35<00:19,  2.78it/s]\u001b[A/home/casapanshop/anaconda2/envs/newpy3/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:224: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "03/05/2020 01:17:49 - INFO - transformers.configuration_utils -   Configuration saved in ../../../../model_save/Dos-Fases-all_Harassment/checkpoint-100/config.json\n",
      "03/05/2020 01:17:51 - INFO - transformers.modeling_utils -   Model weights saved in ../../../../model_save/Dos-Fases-all_Harassment/checkpoint-100/pytorch_model.bin\n",
      "03/05/2020 01:17:51 - INFO - __main__ -   Saving model checkpoint to ../../../../model_save/Dos-Fases-all_Harassment/checkpoint-100\n",
      "03/05/2020 01:17:58 - INFO - __main__ -   Saving optimizer and scheduler states to ../../../../model_save/Dos-Fases-all_Harassment/checkpoint-100\n",
      "\n",
      "Iteration:  65%|██████▍   | 100/154 [00:45<02:55,  3.26s/it]\u001b[A\n",
      "Iteration:  66%|██████▌   | 101/154 [00:45<02:07,  2.40s/it]\u001b[A\n",
      "Iteration:  66%|██████▌   | 102/154 [00:46<01:32,  1.78s/it]\u001b[A\n",
      "Iteration:  67%|██████▋   | 103/154 [00:46<01:08,  1.35s/it]\u001b[A\n",
      "Iteration:  68%|██████▊   | 104/154 [00:46<00:52,  1.05s/it]\u001b[A\n",
      "Iteration:  68%|██████▊   | 105/154 [00:47<00:41,  1.19it/s]\u001b[A\n",
      "Iteration:  69%|██████▉   | 106/154 [00:47<00:33,  1.44it/s]\u001b[A\n",
      "Iteration:  69%|██████▉   | 107/154 [00:47<00:27,  1.70it/s]\u001b[A\n",
      "Iteration:  70%|███████   | 108/154 [00:48<00:23,  1.94it/s]\u001b[A\n",
      "Iteration:  71%|███████   | 109/154 [00:48<00:20,  2.14it/s]\u001b[A\n",
      "Iteration:  71%|███████▏  | 110/154 [00:48<00:18,  2.33it/s]\u001b[A\n",
      "Iteration:  72%|███████▏  | 111/154 [00:49<00:17,  2.47it/s]\u001b[A\n",
      "Iteration:  73%|███████▎  | 112/154 [00:49<00:16,  2.58it/s]\u001b[A\n",
      "Iteration:  73%|███████▎  | 113/154 [00:49<00:15,  2.66it/s]\u001b[A\n",
      "Iteration:  74%|███████▍  | 114/154 [00:50<00:14,  2.73it/s]\u001b[A\n",
      "Iteration:  75%|███████▍  | 115/154 [00:50<00:14,  2.77it/s]\u001b[A\n",
      "Iteration:  75%|███████▌  | 116/154 [00:50<00:13,  2.80it/s]\u001b[A\n",
      "Iteration:  76%|███████▌  | 117/154 [00:51<00:13,  2.83it/s]\u001b[A\n",
      "Iteration:  77%|███████▋  | 118/154 [00:51<00:12,  2.85it/s]\u001b[A\n",
      "Iteration:  77%|███████▋  | 119/154 [00:51<00:12,  2.87it/s]\u001b[A\n",
      "Iteration:  78%|███████▊  | 120/154 [00:52<00:11,  2.87it/s]\u001b[A\n",
      "Iteration:  79%|███████▊  | 121/154 [00:52<00:11,  2.87it/s]\u001b[A\n",
      "Iteration:  79%|███████▉  | 122/154 [00:53<00:11,  2.87it/s]\u001b[A\n",
      "Iteration:  80%|███████▉  | 123/154 [00:53<00:10,  2.88it/s]\u001b[A\n",
      "Iteration:  81%|████████  | 124/154 [00:53<00:10,  2.88it/s]\u001b[A\n",
      "Iteration:  81%|████████  | 125/154 [00:54<00:10,  2.88it/s]\u001b[A\n",
      "Iteration:  82%|████████▏ | 126/154 [00:54<00:09,  2.89it/s]\u001b[A\n",
      "Iteration:  82%|████████▏ | 127/154 [00:54<00:09,  2.88it/s]\u001b[A\n",
      "Iteration:  83%|████████▎ | 128/154 [00:55<00:09,  2.88it/s]\u001b[A\n",
      "Iteration:  84%|████████▍ | 129/154 [00:55<00:08,  2.88it/s]\u001b[A\n",
      "Iteration:  84%|████████▍ | 130/154 [00:55<00:08,  2.89it/s]\u001b[A\n",
      "Iteration:  85%|████████▌ | 131/154 [00:56<00:07,  2.89it/s]\u001b[A\n",
      "Iteration:  86%|████████▌ | 132/154 [00:56<00:07,  2.89it/s]\u001b[A\n",
      "Iteration:  86%|████████▋ | 133/154 [00:56<00:07,  2.88it/s]\u001b[A\n",
      "Iteration:  87%|████████▋ | 134/154 [00:57<00:06,  2.89it/s]\u001b[A\n",
      "Iteration:  88%|████████▊ | 135/154 [00:57<00:06,  2.89it/s]\u001b[A\n",
      "Iteration:  88%|████████▊ | 136/154 [00:57<00:06,  2.89it/s]\u001b[A\n",
      "Iteration:  89%|████████▉ | 137/154 [00:58<00:05,  2.88it/s]\u001b[A\n",
      "Iteration:  90%|████████▉ | 138/154 [00:58<00:05,  2.89it/s]\u001b[A\n",
      "Iteration:  90%|█████████ | 139/154 [00:58<00:05,  2.89it/s]\u001b[A\n",
      "Iteration:  91%|█████████ | 140/154 [00:59<00:04,  2.89it/s]\u001b[A\n",
      "Iteration:  92%|█████████▏| 141/154 [00:59<00:04,  2.88it/s]\u001b[A\n",
      "Iteration:  92%|█████████▏| 142/154 [00:59<00:04,  2.89it/s]\u001b[A\n",
      "Iteration:  93%|█████████▎| 143/154 [01:00<00:03,  2.89it/s]\u001b[A\n",
      "Iteration:  94%|█████████▎| 144/154 [01:00<00:03,  2.89it/s]\u001b[A\n",
      "Iteration:  94%|█████████▍| 145/154 [01:00<00:03,  2.88it/s]\u001b[A\n",
      "Iteration:  95%|█████████▍| 146/154 [01:01<00:02,  2.88it/s]\u001b[A\n",
      "Iteration:  95%|█████████▌| 147/154 [01:01<00:02,  2.89it/s]\u001b[A\n",
      "Iteration:  96%|█████████▌| 148/154 [01:02<00:02,  2.89it/s]\u001b[A\n",
      "Iteration:  97%|█████████▋| 149/154 [01:02<00:01,  2.89it/s]\u001b[A\n",
      "Iteration:  97%|█████████▋| 150/154 [01:02<00:01,  2.88it/s]\u001b[A\n",
      "Iteration:  98%|█████████▊| 151/154 [01:03<00:01,  2.88it/s]\u001b[A\n",
      "Iteration:  99%|█████████▊| 152/154 [01:03<00:00,  2.88it/s]\u001b[A\n",
      "Iteration:  99%|█████████▉| 153/154 [01:03<00:00,  2.89it/s]\u001b[A\n",
      "Iteration: 100%|██████████| 154/154 [01:04<00:00,  2.41it/s]\u001b[A\n",
      "Epoch: 100%|██████████| 1/1 [01:04<00:00, 64.01s/it]\n",
      "03/05/2020 01:18:17 - INFO - __main__ -    global_step = 154, average loss = 4.440967804425723\n",
      "03/05/2020 01:18:17 - INFO - __main__ -   Saving model checkpoint to ../../../../model_save/Dos-Fases-all_Harassment/\n",
      "03/05/2020 01:18:17 - INFO - transformers.configuration_utils -   Configuration saved in ../../../../model_save/Dos-Fases-all_Harassment/config.json\n",
      "03/05/2020 01:18:19 - INFO - transformers.modeling_utils -   Model weights saved in ../../../../model_save/Dos-Fases-all_Harassment/pytorch_model.bin\n",
      "03/05/2020 01:18:20 - INFO - transformers.configuration_utils -   loading configuration file ../../../../model_save/Dos-Fases-all_Harassment/config.json\n",
      "03/05/2020 01:18:20 - INFO - transformers.configuration_utils -   Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": null,\n",
      "  \"do_sample\": false,\n",
      "  \"eos_token_ids\": null,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": null,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "03/05/2020 01:18:20 - INFO - transformers.modeling_utils -   loading weights file ../../../../model_save/Dos-Fases-all_Harassment/pytorch_model.bin\n",
      "03/05/2020 01:18:22 - INFO - transformers.tokenization_utils -   Model name '../../../../model_save/Dos-Fases-all_Harassment/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming '../../../../model_save/Dos-Fases-all_Harassment/' is a path, a model identifier, or url to a directory containing tokenizer files.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/05/2020 01:18:22 - INFO - transformers.tokenization_utils -   Didn't find file ../../../../model_save/Dos-Fases-all_Harassment/added_tokens.json. We won't load it.\n",
      "03/05/2020 01:18:22 - INFO - transformers.tokenization_utils -   loading file ../../../../model_save/Dos-Fases-all_Harassment/vocab.txt\n",
      "03/05/2020 01:18:22 - INFO - transformers.tokenization_utils -   loading file None\n",
      "03/05/2020 01:18:22 - INFO - transformers.tokenization_utils -   loading file ../../../../model_save/Dos-Fases-all_Harassment/special_tokens_map.json\n",
      "03/05/2020 01:18:22 - INFO - transformers.tokenization_utils -   loading file ../../../../model_save/Dos-Fases-all_Harassment/tokenizer_config.json\n",
      "03/05/2020 01:18:22 - INFO - __main__ -   Evaluate the following checkpoints: ['../../../../model_save/Dos-Fases-all_Harassment/']\n",
      "03/05/2020 01:18:22 - INFO - transformers.configuration_utils -   loading configuration file ../../../../model_save/Dos-Fases-all_Harassment/config.json\n",
      "03/05/2020 01:18:22 - INFO - transformers.configuration_utils -   Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": null,\n",
      "  \"do_sample\": false,\n",
      "  \"eos_token_ids\": null,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": null,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "03/05/2020 01:18:22 - INFO - transformers.modeling_utils -   loading weights file ../../../../model_save/Dos-Fases-all_Harassment/pytorch_model.bin\n",
      "03/05/2020 01:18:24 - INFO - __main__ -   ***** Running evaluation  *****\n",
      "03/05/2020 01:18:24 - INFO - __main__ -     Num examples = 503\n",
      "03/05/2020 01:18:24 - INFO - __main__ -     Batch size = 16\n",
      "Evaluating: 100%|██████████| 32/32 [00:03<00:00, 10.64it/s]\n",
      "03/05/2020 01:18:27 - INFO - __main__ -   ***** Eval results  *****\n",
      "03/05/2020 01:18:27 - INFO - __main__ -     perplexity = tensor(51.8094)\n",
      "03/05/2020 01:18:27 - INFO - transformers.tokenization_utils -   Model name '../../../../model_save/Dos-Fases-all_Harassment/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming '../../../../model_save/Dos-Fases-all_Harassment/' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "03/05/2020 01:18:27 - INFO - transformers.tokenization_utils -   Didn't find file ../../../../model_save/Dos-Fases-all_Harassment/added_tokens.json. We won't load it.\n",
      "03/05/2020 01:18:27 - INFO - transformers.tokenization_utils -   loading file ../../../../model_save/Dos-Fases-all_Harassment/vocab.txt\n",
      "03/05/2020 01:18:27 - INFO - transformers.tokenization_utils -   loading file None\n",
      "03/05/2020 01:18:27 - INFO - transformers.tokenization_utils -   loading file ../../../../model_save/Dos-Fases-all_Harassment/special_tokens_map.json\n",
      "03/05/2020 01:18:27 - INFO - transformers.tokenization_utils -   loading file ../../../../model_save/Dos-Fases-all_Harassment/tokenizer_config.json\n",
      "03/05/2020 01:18:27 - INFO - transformers.configuration_utils -   loading configuration file ../../../../model_save/Dos-Fases-all_Harassment/config.json\n",
      "03/05/2020 01:18:27 - INFO - transformers.configuration_utils -   Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": null,\n",
      "  \"do_sample\": false,\n",
      "  \"eos_token_ids\": null,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 4,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": null,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "03/05/2020 01:18:27 - INFO - transformers.modeling_utils -   loading weights file ../../../../model_save/Dos-Fases-all_Harassment/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity epoch: 51.809357\n",
      "\n",
      "Memory usage         :  3.50 MB\n",
      "GC collected objects : 427\n",
      "Memory usage         :  3.50 MB\n",
      "GC collected objects : 0\n",
      "Memory usage         :  3.50 MB\n",
      "Loading BERT tokenizer...\n",
      "Loading BERT Seq Class from  ../../../../model_save/Dos-Fases-all_Harassment/ ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/05/2020 01:18:29 - INFO - transformers.modeling_utils -   Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n",
      "03/05/2020 01:18:29 - INFO - transformers.modeling_utils -   Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo cargado correctamente desde  ../../../../model_save/Dos-Fases-all_Harassment/\n",
      "\n",
      "Padding/truncating all sentences to 50 values...\n",
      "\n",
      "Padding token: \"[PAD]\", ID: 0\n",
      "Completado.\n",
      "\n",
      "Padding/truncating all sentences to 50 values...\n",
      "\n",
      "Padding token: \"[PAD]\", ID: 0\n",
      "Completado.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/05/2020 01:20:11 - INFO - transformers.configuration_utils -   Configuration saved in ../../../../model_save/Dos-Fases-all_Harassment/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating F-macro...\n",
      "F macro: 0.422\n",
      "F macro none average:  [0.91869187 0.         0.         0.76880985]\n",
      "Accuracy: 0.854\n",
      "Saving model to ../../../../model_save/Dos-Fases-all_Harassment/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/05/2020 01:20:13 - INFO - transformers.modeling_utils -   Model weights saved in ../../../../model_save/Dos-Fases-all_Harassment/pytorch_model.bin\n",
      "03/05/2020 01:20:24 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "03/05/2020 01:20:24 - INFO - transformers.configuration_utils -   loading configuration file ../../../../model_save/Dos-Fases-all_Harassment/config.json\n",
      "03/05/2020 01:20:24 - INFO - transformers.configuration_utils -   Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": null,\n",
      "  \"do_sample\": false,\n",
      "  \"eos_token_ids\": null,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 4,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": null,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "03/05/2020 01:20:24 - INFO - transformers.tokenization_utils -   Model name '../../../../model_save/Dos-Fases-all_Harassment/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming '../../../../model_save/Dos-Fases-all_Harassment/' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "03/05/2020 01:20:24 - INFO - transformers.tokenization_utils -   Didn't find file ../../../../model_save/Dos-Fases-all_Harassment/added_tokens.json. We won't load it.\n",
      "03/05/2020 01:20:24 - INFO - transformers.tokenization_utils -   loading file ../../../../model_save/Dos-Fases-all_Harassment/vocab.txt\n",
      "03/05/2020 01:20:24 - INFO - transformers.tokenization_utils -   loading file None\n",
      "03/05/2020 01:20:24 - INFO - transformers.tokenization_utils -   loading file ../../../../model_save/Dos-Fases-all_Harassment/special_tokens_map.json\n",
      "03/05/2020 01:20:24 - INFO - transformers.tokenization_utils -   loading file ../../../../model_save/Dos-Fases-all_Harassment/tokenizer_config.json\n",
      "03/05/2020 01:20:24 - INFO - transformers.modeling_utils -   loading weights file ../../../../model_save/Dos-Fases-all_Harassment/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage         :  3.50 MB\n",
      "GC collected objects : 427\n",
      "Memory usage         :  3.50 MB\n",
      "GC collected objects : 0\n",
      "Memory usage         :  3.50 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/05/2020 01:20:26 - INFO - transformers.modeling_utils -   Weights of BertForMaskedLM not initialized from pretrained model: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']\n",
      "03/05/2020 01:20:26 - INFO - transformers.modeling_utils -   Weights from pretrained model not used in BertForMaskedLM: ['classifier.weight', 'classifier.bias']\n",
      "03/05/2020 01:20:26 - INFO - __main__ -   Training/evaluation parameters <__main__.Args_LM object at 0x7f7ea377dd30>\n",
      "03/05/2020 01:20:26 - INFO - __main__ -   ***** Running training *****\n",
      "03/05/2020 01:20:26 - INFO - __main__ -     Num examples = 2458\n",
      "03/05/2020 01:20:26 - INFO - __main__ -     Num Epochs = 1\n",
      "03/05/2020 01:20:26 - INFO - __main__ -     Instantaneous batch size per GPU = 16\n",
      "03/05/2020 01:20:26 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "03/05/2020 01:20:26 - INFO - __main__ -     Gradient Accumulation steps = 1\n",
      "03/05/2020 01:20:26 - INFO - __main__ -     Total optimization steps = 154\n",
      "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Iteration:   0%|          | 0/154 [00:00<?, ?it/s]\u001b[A\n",
      "Iteration:   1%|          | 1/154 [00:00<00:57,  2.68it/s]\u001b[A\n",
      "Iteration:   1%|▏         | 2/154 [00:00<00:55,  2.74it/s]\u001b[A\n",
      "Iteration:   2%|▏         | 3/154 [00:01<00:54,  2.78it/s]\u001b[A\n",
      "Iteration:   3%|▎         | 4/154 [00:01<00:53,  2.81it/s]\u001b[A\n",
      "Iteration:   3%|▎         | 5/154 [00:01<00:52,  2.83it/s]\u001b[A\n",
      "Iteration:   4%|▍         | 6/154 [00:02<00:52,  2.84it/s]\u001b[A\n",
      "Iteration:   5%|▍         | 7/154 [00:02<00:51,  2.85it/s]\u001b[A\n",
      "Iteration:   5%|▌         | 8/154 [00:02<00:51,  2.86it/s]\u001b[A\n",
      "Iteration:   6%|▌         | 9/154 [00:03<00:50,  2.86it/s]\u001b[A\n",
      "Iteration:   6%|▋         | 10/154 [00:03<00:50,  2.87it/s]\u001b[A\n",
      "Iteration:   7%|▋         | 11/154 [00:03<00:49,  2.87it/s]\u001b[A\n",
      "Iteration:   8%|▊         | 12/154 [00:04<00:49,  2.88it/s]\u001b[A\n",
      "Iteration:   8%|▊         | 13/154 [00:04<00:49,  2.87it/s]\u001b[A\n",
      "Iteration:   9%|▉         | 14/154 [00:04<00:48,  2.88it/s]\u001b[A\n",
      "Iteration:  10%|▉         | 15/154 [00:05<00:48,  2.88it/s]\u001b[A\n",
      "Iteration:  10%|█         | 16/154 [00:05<00:47,  2.88it/s]\u001b[A\n",
      "Iteration:  11%|█         | 17/154 [00:05<00:47,  2.88it/s]\u001b[A\n",
      "Iteration:  12%|█▏        | 18/154 [00:06<00:47,  2.88it/s]\u001b[A\n",
      "Iteration:  12%|█▏        | 19/154 [00:06<00:46,  2.88it/s]\u001b[A\n",
      "Iteration:  13%|█▎        | 20/154 [00:06<00:46,  2.87it/s]\u001b[A\n",
      "Iteration:  14%|█▎        | 21/154 [00:07<00:46,  2.88it/s]\u001b[A\n",
      "Iteration:  14%|█▍        | 22/154 [00:07<00:45,  2.88it/s]\u001b[A\n",
      "Iteration:  15%|█▍        | 23/154 [00:08<00:45,  2.88it/s]\u001b[A\n",
      "Iteration:  16%|█▌        | 24/154 [00:08<00:45,  2.88it/s]\u001b[A\n",
      "Iteration:  16%|█▌        | 25/154 [00:08<00:44,  2.88it/s]\u001b[A\n",
      "Iteration:  17%|█▋        | 26/154 [00:09<00:44,  2.88it/s]\u001b[A\n",
      "Iteration:  18%|█▊        | 27/154 [00:09<00:44,  2.89it/s]\u001b[A\n",
      "Iteration:  18%|█▊        | 28/154 [00:09<00:43,  2.89it/s]\u001b[A\n",
      "Iteration:  19%|█▉        | 29/154 [00:10<00:43,  2.89it/s]\u001b[A\n",
      "Iteration:  19%|█▉        | 30/154 [00:10<00:42,  2.88it/s]\u001b[A\n",
      "Iteration:  20%|██        | 31/154 [00:10<00:42,  2.88it/s]\u001b[A\n",
      "Iteration:  21%|██        | 32/154 [00:11<00:42,  2.87it/s]\u001b[A\n",
      "Iteration:  21%|██▏       | 33/154 [00:11<00:42,  2.87it/s]\u001b[A\n",
      "Iteration:  22%|██▏       | 34/154 [00:11<00:41,  2.88it/s]\u001b[A\n",
      "Iteration:  23%|██▎       | 35/154 [00:12<00:41,  2.88it/s]\u001b[A\n",
      "Iteration:  23%|██▎       | 36/154 [00:12<00:41,  2.88it/s]\u001b[A\n",
      "Iteration:  24%|██▍       | 37/154 [00:12<00:40,  2.88it/s]\u001b[A\n",
      "Iteration:  25%|██▍       | 38/154 [00:13<00:40,  2.88it/s]\u001b[A\n",
      "Iteration:  25%|██▌       | 39/154 [00:13<00:39,  2.89it/s]\u001b[A\n",
      "Iteration:  26%|██▌       | 40/154 [00:13<00:39,  2.88it/s]\u001b[A\n",
      "Iteration:  27%|██▋       | 41/154 [00:14<00:39,  2.88it/s]\u001b[A\n",
      "Iteration:  27%|██▋       | 42/154 [00:14<00:38,  2.87it/s]\u001b[A\n",
      "Iteration:  28%|██▊       | 43/154 [00:14<00:38,  2.87it/s]\u001b[A\n",
      "Iteration:  29%|██▊       | 44/154 [00:15<00:38,  2.88it/s]\u001b[A\n",
      "Iteration:  29%|██▉       | 45/154 [00:15<00:37,  2.87it/s]\u001b[A\n",
      "Iteration:  30%|██▉       | 46/154 [00:16<00:37,  2.87it/s]\u001b[A\n",
      "Iteration:  31%|███       | 47/154 [00:16<00:37,  2.88it/s]\u001b[A\n",
      "Iteration:  31%|███       | 48/154 [00:16<00:36,  2.87it/s]\u001b[A\n",
      "Iteration:  32%|███▏      | 49/154 [00:17<00:36,  2.88it/s]\u001b[A\n",
      "Iteration:  32%|███▏      | 50/154 [00:17<00:36,  2.88it/s]\u001b[A\n",
      "Iteration:  33%|███▎      | 51/154 [00:17<00:35,  2.88it/s]\u001b[A\n",
      "Iteration:  34%|███▍      | 52/154 [00:18<00:35,  2.88it/s]\u001b[A\n",
      "Iteration:  34%|███▍      | 53/154 [00:18<00:35,  2.88it/s]\u001b[A\n",
      "Iteration:  35%|███▌      | 54/154 [00:18<00:34,  2.88it/s]\u001b[A\n",
      "Iteration:  36%|███▌      | 55/154 [00:19<00:34,  2.88it/s]\u001b[A\n",
      "Iteration:  36%|███▋      | 56/154 [00:19<00:34,  2.88it/s]\u001b[A\n",
      "Iteration:  37%|███▋      | 57/154 [00:19<00:33,  2.87it/s]\u001b[A\n",
      "Iteration:  38%|███▊      | 58/154 [00:20<00:33,  2.88it/s]\u001b[A\n",
      "Iteration:  38%|███▊      | 59/154 [00:20<00:33,  2.87it/s]\u001b[A\n",
      "Iteration:  39%|███▉      | 60/154 [00:20<00:32,  2.87it/s]\u001b[A\n",
      "Iteration:  40%|███▉      | 61/154 [00:21<00:32,  2.87it/s]\u001b[A\n",
      "Iteration:  40%|████      | 62/154 [00:21<00:31,  2.88it/s]\u001b[A\n",
      "Iteration:  41%|████      | 63/154 [00:21<00:31,  2.87it/s]\u001b[A\n",
      "Iteration:  42%|████▏     | 64/154 [00:22<00:31,  2.88it/s]\u001b[A\n",
      "Iteration:  42%|████▏     | 65/154 [00:22<00:30,  2.88it/s]\u001b[A\n",
      "Iteration:  43%|████▎     | 66/154 [00:22<00:30,  2.88it/s]\u001b[A\n",
      "Iteration:  44%|████▎     | 67/154 [00:23<00:30,  2.88it/s]\u001b[A\n",
      "Iteration:  44%|████▍     | 68/154 [00:23<00:29,  2.88it/s]\u001b[A\n",
      "Iteration:  45%|████▍     | 69/154 [00:23<00:29,  2.88it/s]\u001b[A\n",
      "Iteration:  45%|████▌     | 70/154 [00:24<00:29,  2.88it/s]\u001b[A\n",
      "Iteration:  46%|████▌     | 71/154 [00:24<00:28,  2.88it/s]\u001b[A\n",
      "Iteration:  47%|████▋     | 72/154 [00:25<00:28,  2.87it/s]\u001b[A\n",
      "Iteration:  47%|████▋     | 73/154 [00:25<00:28,  2.87it/s]\u001b[A\n",
      "Iteration:  48%|████▊     | 74/154 [00:25<00:27,  2.88it/s]\u001b[A\n",
      "Iteration:  49%|████▊     | 75/154 [00:26<00:27,  2.87it/s]\u001b[A\n",
      "Iteration:  49%|████▉     | 76/154 [00:26<00:27,  2.87it/s]\u001b[A\n",
      "Iteration:  50%|█████     | 77/154 [00:26<00:26,  2.87it/s]\u001b[A\n",
      "Iteration:  51%|█████     | 78/154 [00:27<00:26,  2.88it/s]\u001b[A\n",
      "Iteration:  51%|█████▏    | 79/154 [00:27<00:26,  2.88it/s]\u001b[A\n",
      "Iteration:  52%|█████▏    | 80/154 [00:27<00:25,  2.88it/s]\u001b[A\n",
      "Iteration:  53%|█████▎    | 81/154 [00:28<00:25,  2.88it/s]\u001b[A\n",
      "Iteration:  53%|█████▎    | 82/154 [00:28<00:25,  2.88it/s]\u001b[A\n",
      "Iteration:  54%|█████▍    | 83/154 [00:28<00:24,  2.88it/s]\u001b[A\n",
      "Iteration:  55%|█████▍    | 84/154 [00:29<00:24,  2.88it/s]\u001b[A\n",
      "Iteration:  55%|█████▌    | 85/154 [00:29<00:24,  2.87it/s]\u001b[A\n",
      "Iteration:  56%|█████▌    | 86/154 [00:29<00:23,  2.87it/s]\u001b[A\n",
      "Iteration:  56%|█████▋    | 87/154 [00:30<00:23,  2.87it/s]\u001b[A\n",
      "Iteration:  57%|█████▋    | 88/154 [00:30<00:23,  2.87it/s]\u001b[A\n",
      "Iteration:  58%|█████▊    | 89/154 [00:30<00:22,  2.87it/s]\u001b[A\n",
      "Iteration:  58%|█████▊    | 90/154 [00:31<00:22,  2.86it/s]\u001b[A\n",
      "Iteration:  59%|█████▉    | 91/154 [00:31<00:21,  2.87it/s]\u001b[A\n",
      "Iteration:  60%|█████▉    | 92/154 [00:32<00:21,  2.87it/s]\u001b[A\n",
      "Iteration:  60%|██████    | 93/154 [00:32<00:21,  2.87it/s]\u001b[A\n",
      "Iteration:  61%|██████    | 94/154 [00:32<00:20,  2.86it/s]\u001b[A\n",
      "Iteration:  62%|██████▏   | 95/154 [00:33<00:20,  2.86it/s]\u001b[A\n",
      "Iteration:  62%|██████▏   | 96/154 [00:33<00:20,  2.86it/s]\u001b[A\n",
      "Iteration:  63%|██████▎   | 97/154 [00:33<00:19,  2.87it/s]\u001b[A\n",
      "Iteration:  64%|██████▎   | 98/154 [00:34<00:19,  2.85it/s]\u001b[A\n",
      "Iteration:  64%|██████▍   | 99/154 [00:34<00:19,  2.86it/s]\u001b[A03/05/2020 01:21:01 - INFO - transformers.configuration_utils -   Configuration saved in ../../../../model_save/Dos-Fases-all_Harassment/checkpoint-100/config.json\n",
      "03/05/2020 01:21:03 - INFO - transformers.modeling_utils -   Model weights saved in ../../../../model_save/Dos-Fases-all_Harassment/checkpoint-100/pytorch_model.bin\n",
      "03/05/2020 01:21:03 - INFO - __main__ -   Saving model checkpoint to ../../../../model_save/Dos-Fases-all_Harassment/checkpoint-100\n",
      "03/05/2020 01:21:10 - INFO - __main__ -   Saving optimizer and scheduler states to ../../../../model_save/Dos-Fases-all_Harassment/checkpoint-100\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:  65%|██████▍   | 100/154 [00:44<02:47,  3.11s/it]\u001b[A\n",
      "Iteration:  66%|██████▌   | 101/154 [00:44<02:01,  2.29s/it]\u001b[A\n",
      "Iteration:  66%|██████▌   | 102/154 [00:44<01:28,  1.71s/it]\u001b[A\n",
      "Iteration:  67%|██████▋   | 103/154 [00:45<01:06,  1.30s/it]\u001b[A\n",
      "Iteration:  68%|██████▊   | 104/154 [00:45<00:50,  1.02s/it]\u001b[A\n",
      "Iteration:  68%|██████▊   | 105/154 [00:45<00:39,  1.23it/s]\u001b[A\n",
      "Iteration:  69%|██████▉   | 106/154 [00:46<00:32,  1.48it/s]\u001b[A\n",
      "Iteration:  69%|██████▉   | 107/154 [00:46<00:27,  1.73it/s]\u001b[A\n",
      "Iteration:  70%|███████   | 108/154 [00:46<00:23,  1.97it/s]\u001b[A\n",
      "Iteration:  71%|███████   | 109/154 [00:47<00:20,  2.17it/s]\u001b[A\n",
      "Iteration:  71%|███████▏  | 110/154 [00:47<00:18,  2.34it/s]\u001b[A\n",
      "Iteration:  72%|███████▏  | 111/154 [00:47<00:17,  2.48it/s]\u001b[A\n",
      "Iteration:  73%|███████▎  | 112/154 [00:48<00:16,  2.58it/s]\u001b[A\n",
      "Iteration:  73%|███████▎  | 113/154 [00:48<00:15,  2.67it/s]\u001b[A\n",
      "Iteration:  74%|███████▍  | 114/154 [00:48<00:14,  2.72it/s]\u001b[A\n",
      "Iteration:  75%|███████▍  | 115/154 [00:49<00:14,  2.77it/s]\u001b[A\n",
      "Iteration:  75%|███████▌  | 116/154 [00:49<00:13,  2.81it/s]\u001b[A\n",
      "Iteration:  76%|███████▌  | 117/154 [00:49<00:13,  2.82it/s]\u001b[A\n",
      "Iteration:  77%|███████▋  | 118/154 [00:50<00:12,  2.84it/s]\u001b[A\n",
      "Iteration:  77%|███████▋  | 119/154 [00:50<00:12,  2.85it/s]\u001b[A\n",
      "Iteration:  78%|███████▊  | 120/154 [00:51<00:11,  2.86it/s]\u001b[A\n",
      "Iteration:  79%|███████▊  | 121/154 [00:51<00:11,  2.87it/s]\u001b[A\n",
      "Iteration:  79%|███████▉  | 122/154 [00:51<00:11,  2.87it/s]\u001b[A\n",
      "Iteration:  80%|███████▉  | 123/154 [00:52<00:10,  2.87it/s]\u001b[A\n",
      "Iteration:  81%|████████  | 124/154 [00:52<00:10,  2.87it/s]\u001b[A\n",
      "Iteration:  81%|████████  | 125/154 [00:52<00:10,  2.87it/s]\u001b[A\n",
      "Iteration:  82%|████████▏ | 126/154 [00:53<00:09,  2.88it/s]\u001b[A\n",
      "Iteration:  82%|████████▏ | 127/154 [00:53<00:09,  2.88it/s]\u001b[A\n",
      "Iteration:  83%|████████▎ | 128/154 [00:53<00:09,  2.88it/s]\u001b[A\n",
      "Iteration:  84%|████████▍ | 129/154 [00:54<00:08,  2.88it/s]\u001b[A\n",
      "Iteration:  84%|████████▍ | 130/154 [00:54<00:08,  2.89it/s]\u001b[A\n",
      "Iteration:  85%|████████▌ | 131/154 [00:54<00:07,  2.88it/s]\u001b[A\n",
      "Iteration:  86%|████████▌ | 132/154 [00:55<00:07,  2.88it/s]\u001b[A\n",
      "Iteration:  86%|████████▋ | 133/154 [00:55<00:07,  2.89it/s]\u001b[A\n",
      "Iteration:  87%|████████▋ | 134/154 [00:55<00:06,  2.88it/s]\u001b[A\n",
      "Iteration:  88%|████████▊ | 135/154 [00:56<00:06,  2.88it/s]\u001b[A\n",
      "Iteration:  88%|████████▊ | 136/154 [00:56<00:06,  2.87it/s]\u001b[A\n",
      "Iteration:  89%|████████▉ | 137/154 [00:56<00:05,  2.87it/s]\u001b[A\n",
      "Iteration:  90%|████████▉ | 138/154 [00:57<00:05,  2.88it/s]\u001b[A\n",
      "Iteration:  90%|█████████ | 139/154 [00:57<00:05,  2.88it/s]\u001b[A\n",
      "Iteration:  91%|█████████ | 140/154 [00:57<00:04,  2.87it/s]\u001b[A\n",
      "Iteration:  92%|█████████▏| 141/154 [00:58<00:04,  2.88it/s]\u001b[A\n",
      "Iteration:  92%|█████████▏| 142/154 [00:58<00:04,  2.87it/s]\u001b[A\n",
      "Iteration:  93%|█████████▎| 143/154 [00:58<00:03,  2.87it/s]\u001b[A\n",
      "Iteration:  94%|█████████▎| 144/154 [00:59<00:03,  2.87it/s]\u001b[A\n",
      "Iteration:  94%|█████████▍| 145/154 [00:59<00:03,  2.86it/s]\u001b[A\n",
      "Iteration:  95%|█████████▍| 146/154 [01:00<00:02,  2.86it/s]\u001b[A\n",
      "Iteration:  95%|█████████▌| 147/154 [01:00<00:02,  2.87it/s]\u001b[A\n",
      "Iteration:  96%|█████████▌| 148/154 [01:00<00:02,  2.86it/s]\u001b[A\n",
      "Iteration:  97%|█████████▋| 149/154 [01:01<00:01,  2.87it/s]\u001b[A\n",
      "Iteration:  97%|█████████▋| 150/154 [01:01<00:01,  2.87it/s]\u001b[A\n",
      "Iteration:  98%|█████████▊| 151/154 [01:01<00:01,  2.87it/s]\u001b[A\n",
      "Iteration:  99%|█████████▊| 152/154 [01:02<00:00,  2.87it/s]\u001b[A\n",
      "Iteration:  99%|█████████▉| 153/154 [01:02<00:00,  2.87it/s]\u001b[A\n",
      "Iteration: 100%|██████████| 154/154 [01:02<00:00,  2.45it/s]\u001b[A\n",
      "Epoch: 100%|██████████| 1/1 [01:02<00:00, 62.74s/it]\n",
      "03/05/2020 01:21:29 - INFO - __main__ -    global_step = 154, average loss = 5.827671865364174\n",
      "03/05/2020 01:21:29 - INFO - __main__ -   Saving model checkpoint to ../../../../model_save/Dos-Fases-all_Harassment/\n",
      "03/05/2020 01:21:29 - INFO - transformers.configuration_utils -   Configuration saved in ../../../../model_save/Dos-Fases-all_Harassment/config.json\n",
      "03/05/2020 01:21:31 - INFO - transformers.modeling_utils -   Model weights saved in ../../../../model_save/Dos-Fases-all_Harassment/pytorch_model.bin\n",
      "03/05/2020 01:21:31 - INFO - transformers.configuration_utils -   loading configuration file ../../../../model_save/Dos-Fases-all_Harassment/config.json\n",
      "03/05/2020 01:21:31 - INFO - transformers.configuration_utils -   Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": null,\n",
      "  \"do_sample\": false,\n",
      "  \"eos_token_ids\": null,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 4,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": null,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "03/05/2020 01:21:31 - INFO - transformers.modeling_utils -   loading weights file ../../../../model_save/Dos-Fases-all_Harassment/pytorch_model.bin\n",
      "03/05/2020 01:21:33 - INFO - transformers.tokenization_utils -   Model name '../../../../model_save/Dos-Fases-all_Harassment/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming '../../../../model_save/Dos-Fases-all_Harassment/' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "03/05/2020 01:21:33 - INFO - transformers.tokenization_utils -   Didn't find file ../../../../model_save/Dos-Fases-all_Harassment/added_tokens.json. We won't load it.\n",
      "03/05/2020 01:21:33 - INFO - transformers.tokenization_utils -   loading file ../../../../model_save/Dos-Fases-all_Harassment/vocab.txt\n",
      "03/05/2020 01:21:33 - INFO - transformers.tokenization_utils -   loading file None\n",
      "03/05/2020 01:21:33 - INFO - transformers.tokenization_utils -   loading file ../../../../model_save/Dos-Fases-all_Harassment/special_tokens_map.json\n",
      "03/05/2020 01:21:33 - INFO - transformers.tokenization_utils -   loading file ../../../../model_save/Dos-Fases-all_Harassment/tokenizer_config.json\n",
      "03/05/2020 01:21:33 - INFO - __main__ -   Evaluate the following checkpoints: ['../../../../model_save/Dos-Fases-all_Harassment/']\n",
      "03/05/2020 01:21:33 - INFO - transformers.configuration_utils -   loading configuration file ../../../../model_save/Dos-Fases-all_Harassment/config.json\n",
      "03/05/2020 01:21:33 - INFO - transformers.configuration_utils -   Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": null,\n",
      "  \"do_sample\": false,\n",
      "  \"eos_token_ids\": null,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 4,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": null,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/05/2020 01:21:33 - INFO - transformers.modeling_utils -   loading weights file ../../../../model_save/Dos-Fases-all_Harassment/pytorch_model.bin\n",
      "03/05/2020 01:21:35 - INFO - __main__ -   ***** Running evaluation  *****\n",
      "03/05/2020 01:21:35 - INFO - __main__ -     Num examples = 503\n",
      "03/05/2020 01:21:35 - INFO - __main__ -     Batch size = 16\n",
      "Evaluating: 100%|██████████| 32/32 [00:03<00:00, 10.53it/s]\n",
      "03/05/2020 01:21:38 - INFO - __main__ -   ***** Eval results  *****\n",
      "03/05/2020 01:21:38 - INFO - __main__ -     perplexity = tensor(349.7055)\n",
      "03/05/2020 01:21:38 - INFO - transformers.tokenization_utils -   Model name '../../../../model_save/Dos-Fases-all_Harassment/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming '../../../../model_save/Dos-Fases-all_Harassment/' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "03/05/2020 01:21:38 - INFO - transformers.tokenization_utils -   Didn't find file ../../../../model_save/Dos-Fases-all_Harassment/added_tokens.json. We won't load it.\n",
      "03/05/2020 01:21:38 - INFO - transformers.tokenization_utils -   loading file ../../../../model_save/Dos-Fases-all_Harassment/vocab.txt\n",
      "03/05/2020 01:21:38 - INFO - transformers.tokenization_utils -   loading file None\n",
      "03/05/2020 01:21:38 - INFO - transformers.tokenization_utils -   loading file ../../../../model_save/Dos-Fases-all_Harassment/special_tokens_map.json\n",
      "03/05/2020 01:21:38 - INFO - transformers.tokenization_utils -   loading file ../../../../model_save/Dos-Fases-all_Harassment/tokenizer_config.json\n",
      "03/05/2020 01:21:38 - INFO - transformers.configuration_utils -   loading configuration file ../../../../model_save/Dos-Fases-all_Harassment/config.json\n",
      "03/05/2020 01:21:38 - INFO - transformers.configuration_utils -   Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": null,\n",
      "  \"do_sample\": false,\n",
      "  \"eos_token_ids\": null,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 4,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": null,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "03/05/2020 01:21:38 - INFO - transformers.modeling_utils -   loading weights file ../../../../model_save/Dos-Fases-all_Harassment/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity epoch: 349.7055\n",
      "\n",
      "Memory usage         :  3.80 MB\n",
      "GC collected objects : 844\n",
      "Memory usage         :  3.80 MB\n",
      "GC collected objects : 0\n",
      "Memory usage         :  3.80 MB\n",
      "Loading BERT tokenizer...\n",
      "Loading BERT Seq Class from  ../../../../model_save/Dos-Fases-all_Harassment/ ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/05/2020 01:21:40 - INFO - transformers.modeling_utils -   Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n",
      "03/05/2020 01:21:40 - INFO - transformers.modeling_utils -   Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo cargado correctamente desde  ../../../../model_save/Dos-Fases-all_Harassment/\n",
      "\n",
      "Padding/truncating all sentences to 50 values...\n",
      "\n",
      "Padding token: \"[PAD]\", ID: 0\n",
      "Completado.\n",
      "\n",
      "Padding/truncating all sentences to 50 values...\n",
      "\n",
      "Padding token: \"[PAD]\", ID: 0\n",
      "Completado.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/05/2020 01:23:24 - INFO - transformers.configuration_utils -   Configuration saved in ../../../../model_save/Dos-Fases-all_Harassment/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating F-macro...\n",
      "F macro: 0.422\n",
      "F macro none average:  [0.91878933 0.         0.         0.77028886]\n",
      "Accuracy: 0.854\n",
      "Saving model to ../../../../model_save/Dos-Fases-all_Harassment/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/05/2020 01:23:26 - INFO - transformers.modeling_utils -   Model weights saved in ../../../../model_save/Dos-Fases-all_Harassment/pytorch_model.bin\n",
      "03/05/2020 01:23:36 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "03/05/2020 01:23:36 - INFO - transformers.configuration_utils -   loading configuration file ../../../../model_save/Dos-Fases-all_Harassment/config.json\n",
      "03/05/2020 01:23:36 - INFO - transformers.configuration_utils -   Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": null,\n",
      "  \"do_sample\": false,\n",
      "  \"eos_token_ids\": null,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 4,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": null,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "03/05/2020 01:23:36 - INFO - transformers.tokenization_utils -   Model name '../../../../model_save/Dos-Fases-all_Harassment/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming '../../../../model_save/Dos-Fases-all_Harassment/' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "03/05/2020 01:23:36 - INFO - transformers.tokenization_utils -   Didn't find file ../../../../model_save/Dos-Fases-all_Harassment/added_tokens.json. We won't load it.\n",
      "03/05/2020 01:23:36 - INFO - transformers.tokenization_utils -   loading file ../../../../model_save/Dos-Fases-all_Harassment/vocab.txt\n",
      "03/05/2020 01:23:36 - INFO - transformers.tokenization_utils -   loading file None\n",
      "03/05/2020 01:23:36 - INFO - transformers.tokenization_utils -   loading file ../../../../model_save/Dos-Fases-all_Harassment/special_tokens_map.json\n",
      "03/05/2020 01:23:36 - INFO - transformers.tokenization_utils -   loading file ../../../../model_save/Dos-Fases-all_Harassment/tokenizer_config.json\n",
      "03/05/2020 01:23:36 - INFO - transformers.modeling_utils -   loading weights file ../../../../model_save/Dos-Fases-all_Harassment/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage         :  3.80 MB\n",
      "GC collected objects : 427\n",
      "Memory usage         :  3.80 MB\n",
      "GC collected objects : 0\n",
      "Memory usage         :  3.80 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/05/2020 01:23:38 - INFO - transformers.modeling_utils -   Weights of BertForMaskedLM not initialized from pretrained model: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']\n",
      "03/05/2020 01:23:38 - INFO - transformers.modeling_utils -   Weights from pretrained model not used in BertForMaskedLM: ['classifier.weight', 'classifier.bias']\n",
      "03/05/2020 01:23:38 - INFO - __main__ -   Training/evaluation parameters <__main__.Args_LM object at 0x7f7ea377dd30>\n",
      "03/05/2020 01:23:38 - INFO - __main__ -   ***** Running training *****\n",
      "03/05/2020 01:23:38 - INFO - __main__ -     Num examples = 2458\n",
      "03/05/2020 01:23:38 - INFO - __main__ -     Num Epochs = 1\n",
      "03/05/2020 01:23:38 - INFO - __main__ -     Instantaneous batch size per GPU = 16\n",
      "03/05/2020 01:23:38 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "03/05/2020 01:23:38 - INFO - __main__ -     Gradient Accumulation steps = 1\n",
      "03/05/2020 01:23:38 - INFO - __main__ -     Total optimization steps = 154\n",
      "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Iteration:   0%|          | 0/154 [00:00<?, ?it/s]\u001b[A\n",
      "Iteration:   1%|          | 1/154 [00:00<00:58,  2.63it/s]\u001b[A\n",
      "Iteration:   1%|▏         | 2/154 [00:00<00:56,  2.70it/s]\u001b[A\n",
      "Iteration:   2%|▏         | 3/154 [00:01<00:55,  2.75it/s]\u001b[A\n",
      "Iteration:   3%|▎         | 4/154 [00:01<00:53,  2.78it/s]\u001b[A\n",
      "Iteration:   3%|▎         | 5/154 [00:01<00:52,  2.81it/s]\u001b[A\n",
      "Iteration:   4%|▍         | 6/154 [00:02<00:52,  2.83it/s]\u001b[A\n",
      "Iteration:   5%|▍         | 7/154 [00:02<00:51,  2.84it/s]\u001b[A\n",
      "Iteration:   5%|▌         | 8/154 [00:02<00:51,  2.86it/s]\u001b[A\n",
      "Iteration:   6%|▌         | 9/154 [00:03<00:50,  2.86it/s]\u001b[A\n",
      "Iteration:   6%|▋         | 10/154 [00:03<00:50,  2.87it/s]\u001b[A\n",
      "Iteration:   7%|▋         | 11/154 [00:03<00:49,  2.87it/s]\u001b[A\n",
      "Iteration:   8%|▊         | 12/154 [00:04<00:49,  2.87it/s]\u001b[A\n",
      "Iteration:   8%|▊         | 13/154 [00:04<00:49,  2.86it/s]\u001b[A\n",
      "Iteration:   9%|▉         | 14/154 [00:04<00:48,  2.87it/s]\u001b[A\n",
      "Iteration:  10%|▉         | 15/154 [00:05<00:48,  2.87it/s]\u001b[A\n",
      "Iteration:  10%|█         | 16/154 [00:05<00:48,  2.87it/s]\u001b[A\n",
      "Iteration:  11%|█         | 17/154 [00:05<00:47,  2.88it/s]\u001b[A\n",
      "Iteration:  12%|█▏        | 18/154 [00:06<00:47,  2.88it/s]\u001b[A\n",
      "Iteration:  12%|█▏        | 19/154 [00:06<00:46,  2.88it/s]\u001b[A\n",
      "Iteration:  13%|█▎        | 20/154 [00:06<00:46,  2.86it/s]\u001b[A\n",
      "Iteration:  14%|█▎        | 21/154 [00:07<00:46,  2.87it/s]\u001b[A\n",
      "Iteration:  14%|█▍        | 22/154 [00:07<00:46,  2.87it/s]\u001b[A\n",
      "Iteration:  15%|█▍        | 23/154 [00:08<00:45,  2.87it/s]\u001b[A\n",
      "Iteration:  16%|█▌        | 24/154 [00:08<00:45,  2.87it/s]\u001b[A\n",
      "Iteration:  16%|█▌        | 25/154 [00:08<00:44,  2.88it/s]\u001b[A\n",
      "Iteration:  17%|█▋        | 26/154 [00:09<00:44,  2.88it/s]\u001b[A\n",
      "Iteration:  18%|█▊        | 27/154 [00:09<00:44,  2.88it/s]\u001b[A\n",
      "Iteration:  18%|█▊        | 28/154 [00:09<00:43,  2.87it/s]\u001b[A\n",
      "Iteration:  19%|█▉        | 29/154 [00:10<00:43,  2.87it/s]\u001b[A\n",
      "Iteration:  19%|█▉        | 30/154 [00:10<00:43,  2.86it/s]\u001b[A\n",
      "Iteration:  20%|██        | 31/154 [00:10<00:42,  2.86it/s]\u001b[A\n",
      "Iteration:  21%|██        | 32/154 [00:11<00:42,  2.86it/s]\u001b[A\n",
      "Iteration:  21%|██▏       | 33/154 [00:11<00:42,  2.86it/s]\u001b[A\n",
      "Iteration:  22%|██▏       | 34/154 [00:11<00:41,  2.87it/s]\u001b[A\n",
      "Iteration:  23%|██▎       | 35/154 [00:12<00:41,  2.88it/s]\u001b[A\n",
      "Iteration:  23%|██▎       | 36/154 [00:12<00:41,  2.88it/s]\u001b[A\n",
      "Iteration:  24%|██▍       | 37/154 [00:12<00:40,  2.88it/s]\u001b[A\n",
      "Iteration:  25%|██▍       | 38/154 [00:13<00:40,  2.87it/s]\u001b[A\n",
      "Iteration:  25%|██▌       | 39/154 [00:13<00:40,  2.86it/s]\u001b[A\n",
      "Iteration:  26%|██▌       | 40/154 [00:13<00:39,  2.87it/s]\u001b[A\n",
      "Iteration:  27%|██▋       | 41/154 [00:14<00:39,  2.87it/s]\u001b[A\n",
      "Iteration:  27%|██▋       | 42/154 [00:14<00:39,  2.87it/s]\u001b[A\n",
      "Iteration:  28%|██▊       | 43/154 [00:15<00:38,  2.87it/s]\u001b[A\n",
      "Iteration:  29%|██▊       | 44/154 [00:15<00:38,  2.87it/s]\u001b[A\n",
      "Iteration:  29%|██▉       | 45/154 [00:15<00:38,  2.87it/s]\u001b[A\n",
      "Iteration:  30%|██▉       | 46/154 [00:16<00:37,  2.86it/s]\u001b[A\n",
      "Iteration:  31%|███       | 47/154 [00:16<00:37,  2.86it/s]\u001b[A\n",
      "Iteration:  31%|███       | 48/154 [00:16<00:36,  2.87it/s]\u001b[A\n",
      "Iteration:  32%|███▏      | 49/154 [00:17<00:36,  2.87it/s]\u001b[A\n",
      "Iteration:  32%|███▏      | 50/154 [00:17<00:36,  2.87it/s]\u001b[A\n",
      "Iteration:  33%|███▎      | 51/154 [00:17<00:35,  2.87it/s]\u001b[A\n",
      "Iteration:  34%|███▍      | 52/154 [00:18<00:35,  2.87it/s]\u001b[A\n",
      "Iteration:  34%|███▍      | 53/154 [00:18<00:35,  2.87it/s]\u001b[A\n",
      "Iteration:  35%|███▌      | 54/154 [00:18<00:34,  2.87it/s]\u001b[A\n",
      "Iteration:  36%|███▌      | 55/154 [00:19<00:34,  2.87it/s]\u001b[A\n",
      "Iteration:  36%|███▋      | 56/154 [00:19<00:34,  2.87it/s]\u001b[A\n",
      "Iteration:  37%|███▋      | 57/154 [00:19<00:33,  2.87it/s]\u001b[A\n",
      "Iteration:  38%|███▊      | 58/154 [00:20<00:33,  2.87it/s]\u001b[A\n",
      "Iteration:  38%|███▊      | 59/154 [00:20<00:33,  2.87it/s]\u001b[A\n",
      "Iteration:  39%|███▉      | 60/154 [00:20<00:32,  2.87it/s]\u001b[A\n",
      "Iteration:  40%|███▉      | 61/154 [00:21<00:32,  2.86it/s]\u001b[A\n",
      "Iteration:  40%|████      | 62/154 [00:21<00:32,  2.86it/s]\u001b[A\n",
      "Iteration:  41%|████      | 63/154 [00:21<00:31,  2.86it/s]\u001b[A\n",
      "Iteration:  42%|████▏     | 64/154 [00:22<00:31,  2.86it/s]\u001b[A\n",
      "Iteration:  42%|████▏     | 65/154 [00:22<00:31,  2.86it/s]\u001b[A\n",
      "Iteration:  43%|████▎     | 66/154 [00:23<00:30,  2.87it/s]\u001b[A\n",
      "Iteration:  44%|████▎     | 67/154 [00:23<00:30,  2.86it/s]\u001b[A\n",
      "Iteration:  44%|████▍     | 68/154 [00:23<00:30,  2.87it/s]\u001b[A\n",
      "Iteration:  45%|████▍     | 69/154 [00:24<00:29,  2.87it/s]\u001b[A\n",
      "Iteration:  45%|████▌     | 70/154 [00:24<00:29,  2.87it/s]\u001b[A\n",
      "Iteration:  46%|████▌     | 71/154 [00:24<00:28,  2.88it/s]\u001b[A\n",
      "Iteration:  47%|████▋     | 72/154 [00:25<00:28,  2.87it/s]\u001b[A\n",
      "Iteration:  47%|████▋     | 73/154 [00:25<00:28,  2.87it/s]\u001b[A\n",
      "Iteration:  48%|████▊     | 74/154 [00:25<00:27,  2.87it/s]\u001b[A\n",
      "Iteration:  49%|████▊     | 75/154 [00:26<00:27,  2.87it/s]\u001b[A\n",
      "Iteration:  49%|████▉     | 76/154 [00:26<00:27,  2.87it/s]\u001b[A\n",
      "Iteration:  50%|█████     | 77/154 [00:26<00:26,  2.86it/s]\u001b[A\n",
      "Iteration:  51%|█████     | 78/154 [00:27<00:26,  2.87it/s]\u001b[A\n",
      "Iteration:  51%|█████▏    | 79/154 [00:27<00:26,  2.86it/s]\u001b[A\n",
      "Iteration:  52%|█████▏    | 80/154 [00:27<00:25,  2.87it/s]\u001b[A\n",
      "Iteration:  53%|█████▎    | 81/154 [00:28<00:25,  2.87it/s]\u001b[A\n",
      "Iteration:  53%|█████▎    | 82/154 [00:28<00:25,  2.87it/s]\u001b[A\n",
      "Iteration:  54%|█████▍    | 83/154 [00:28<00:24,  2.88it/s]\u001b[A\n",
      "Iteration:  55%|█████▍    | 84/154 [00:29<00:24,  2.87it/s]\u001b[A\n",
      "Iteration:  55%|█████▌    | 85/154 [00:29<00:24,  2.87it/s]\u001b[A\n",
      "Iteration:  56%|█████▌    | 86/154 [00:29<00:23,  2.88it/s]\u001b[A\n",
      "Iteration:  56%|█████▋    | 87/154 [00:30<00:23,  2.88it/s]\u001b[A\n",
      "Iteration:  57%|█████▋    | 88/154 [00:30<00:22,  2.88it/s]\u001b[A\n",
      "Iteration:  58%|█████▊    | 89/154 [00:31<00:22,  2.87it/s]\u001b[A\n",
      "Iteration:  58%|█████▊    | 90/154 [00:31<00:22,  2.87it/s]\u001b[A\n",
      "Iteration:  59%|█████▉    | 91/154 [00:31<00:21,  2.87it/s]\u001b[A\n",
      "Iteration:  60%|█████▉    | 92/154 [00:32<00:21,  2.86it/s]\u001b[A\n",
      "Iteration:  60%|██████    | 93/154 [00:32<00:21,  2.86it/s]\u001b[A\n",
      "Iteration:  61%|██████    | 94/154 [00:32<00:20,  2.86it/s]\u001b[A\n",
      "Iteration:  62%|██████▏   | 95/154 [00:33<00:20,  2.86it/s]\u001b[A\n",
      "Iteration:  62%|██████▏   | 96/154 [00:33<00:20,  2.86it/s]\u001b[A\n",
      "Iteration:  63%|██████▎   | 97/154 [00:33<00:19,  2.87it/s]\u001b[A\n",
      "Iteration:  64%|██████▎   | 98/154 [00:34<00:19,  2.86it/s]\u001b[A\n",
      "Iteration:  64%|██████▍   | 99/154 [00:34<00:19,  2.87it/s]\u001b[A03/05/2020 01:24:13 - INFO - transformers.configuration_utils -   Configuration saved in ../../../../model_save/Dos-Fases-all_Harassment/checkpoint-100/config.json\n",
      "03/05/2020 01:24:16 - INFO - transformers.modeling_utils -   Model weights saved in ../../../../model_save/Dos-Fases-all_Harassment/checkpoint-100/pytorch_model.bin\n",
      "03/05/2020 01:24:16 - INFO - __main__ -   Saving model checkpoint to ../../../../model_save/Dos-Fases-all_Harassment/checkpoint-100\n",
      "03/05/2020 01:24:23 - INFO - __main__ -   Saving optimizer and scheduler states to ../../../../model_save/Dos-Fases-all_Harassment/checkpoint-100\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:  65%|██████▍   | 100/154 [00:44<02:49,  3.14s/it]\u001b[A\n",
      "Iteration:  66%|██████▌   | 101/154 [00:44<02:02,  2.31s/it]\u001b[A\n",
      "Iteration:  66%|██████▌   | 102/154 [00:44<01:29,  1.72s/it]\u001b[A\n",
      "Iteration:  67%|██████▋   | 103/154 [00:45<01:06,  1.31s/it]\u001b[A\n",
      "Iteration:  68%|██████▊   | 104/154 [00:45<00:51,  1.02s/it]\u001b[A\n",
      "Iteration:  68%|██████▊   | 105/154 [00:45<00:40,  1.22it/s]\u001b[A\n",
      "Iteration:  69%|██████▉   | 106/154 [00:46<00:32,  1.47it/s]\u001b[A\n",
      "Iteration:  69%|██████▉   | 107/154 [00:46<00:27,  1.72it/s]\u001b[A\n",
      "Iteration:  70%|███████   | 108/154 [00:47<00:23,  1.96it/s]\u001b[A\n",
      "Iteration:  71%|███████   | 109/154 [00:47<00:20,  2.16it/s]\u001b[A\n",
      "Iteration:  71%|███████▏  | 110/154 [00:47<00:18,  2.33it/s]\u001b[A\n",
      "Iteration:  72%|███████▏  | 111/154 [00:48<00:17,  2.47it/s]\u001b[A\n",
      "Iteration:  73%|███████▎  | 112/154 [00:48<00:16,  2.58it/s]\u001b[A\n",
      "Iteration:  73%|███████▎  | 113/154 [00:48<00:15,  2.66it/s]\u001b[A\n",
      "Iteration:  74%|███████▍  | 114/154 [00:49<00:14,  2.71it/s]\u001b[A\n",
      "Iteration:  75%|███████▍  | 115/154 [00:49<00:14,  2.75it/s]\u001b[A\n",
      "Iteration:  75%|███████▌  | 116/154 [00:49<00:13,  2.79it/s]\u001b[A\n",
      "Iteration:  76%|███████▌  | 117/154 [00:50<00:13,  2.81it/s]\u001b[A\n",
      "Iteration:  77%|███████▋  | 118/154 [00:50<00:12,  2.82it/s]\u001b[A\n",
      "Iteration:  77%|███████▋  | 119/154 [00:50<00:12,  2.83it/s]\u001b[A\n",
      "Iteration:  78%|███████▊  | 120/154 [00:51<00:12,  2.83it/s]\u001b[A\n",
      "Iteration:  79%|███████▊  | 121/154 [00:51<00:11,  2.84it/s]\u001b[A\n",
      "Iteration:  79%|███████▉  | 122/154 [00:51<00:11,  2.85it/s]\u001b[A\n",
      "Iteration:  80%|███████▉  | 123/154 [00:52<00:10,  2.85it/s]\u001b[A\n",
      "Iteration:  81%|████████  | 124/154 [00:52<00:10,  2.86it/s]\u001b[A\n",
      "Iteration:  81%|████████  | 125/154 [00:52<00:10,  2.87it/s]\u001b[A\n",
      "Iteration:  82%|████████▏ | 126/154 [00:53<00:09,  2.87it/s]\u001b[A\n",
      "Iteration:  82%|████████▏ | 127/154 [00:53<00:09,  2.86it/s]\u001b[A\n",
      "Iteration:  83%|████████▎ | 128/154 [00:54<00:09,  2.87it/s]\u001b[A\n",
      "Iteration:  84%|████████▍ | 129/154 [00:54<00:08,  2.86it/s]\u001b[A\n",
      "Iteration:  84%|████████▍ | 130/154 [00:54<00:08,  2.85it/s]\u001b[A\n",
      "Iteration:  85%|████████▌ | 131/154 [00:55<00:08,  2.85it/s]\u001b[A\n",
      "Iteration:  86%|████████▌ | 132/154 [00:55<00:07,  2.86it/s]\u001b[A\n",
      "Iteration:  86%|████████▋ | 133/154 [00:55<00:07,  2.87it/s]\u001b[A\n",
      "Iteration:  87%|████████▋ | 134/154 [00:56<00:06,  2.87it/s]\u001b[A\n",
      "Iteration:  88%|████████▊ | 135/154 [00:56<00:06,  2.87it/s]\u001b[A\n",
      "Iteration:  88%|████████▊ | 136/154 [00:56<00:06,  2.88it/s]\u001b[A\n",
      "Iteration:  89%|████████▉ | 137/154 [00:57<00:05,  2.87it/s]\u001b[A\n",
      "Iteration:  90%|████████▉ | 138/154 [00:57<00:05,  2.87it/s]\u001b[A\n",
      "Iteration:  90%|█████████ | 139/154 [00:57<00:05,  2.87it/s]\u001b[A\n",
      "Iteration:  91%|█████████ | 140/154 [00:58<00:04,  2.86it/s]\u001b[A\n",
      "Iteration:  92%|█████████▏| 141/154 [00:58<00:04,  2.85it/s]\u001b[A\n",
      "Iteration:  92%|█████████▏| 142/154 [00:58<00:04,  2.86it/s]\u001b[A\n",
      "Iteration:  93%|█████████▎| 143/154 [00:59<00:03,  2.86it/s]\u001b[A\n",
      "Iteration:  94%|█████████▎| 144/154 [00:59<00:03,  2.87it/s]\u001b[A\n",
      "Iteration:  94%|█████████▍| 145/154 [00:59<00:03,  2.87it/s]\u001b[A\n",
      "Iteration:  95%|█████████▍| 146/154 [01:00<00:02,  2.87it/s]\u001b[A\n",
      "Iteration:  95%|█████████▌| 147/154 [01:00<00:02,  2.86it/s]\u001b[A\n",
      "Iteration:  96%|█████████▌| 148/154 [01:00<00:02,  2.86it/s]\u001b[A\n",
      "Iteration:  97%|█████████▋| 149/154 [01:01<00:01,  2.87it/s]\u001b[A\n",
      "Iteration:  97%|█████████▋| 150/154 [01:01<00:01,  2.86it/s]\u001b[A\n",
      "Iteration:  98%|█████████▊| 151/154 [01:02<00:01,  2.87it/s]\u001b[A\n",
      "Iteration:  99%|█████████▊| 152/154 [01:02<00:00,  2.87it/s]\u001b[A\n",
      "Iteration:  99%|█████████▉| 153/154 [01:02<00:00,  2.86it/s]\u001b[A\n",
      "Iteration: 100%|██████████| 154/154 [01:02<00:00,  2.44it/s]\u001b[A\n",
      "Epoch: 100%|██████████| 1/1 [01:02<00:00, 62.99s/it]\n",
      "03/05/2020 01:24:41 - INFO - __main__ -    global_step = 154, average loss = 4.97637170011347\n",
      "03/05/2020 01:24:41 - INFO - __main__ -   Saving model checkpoint to ../../../../model_save/Dos-Fases-all_Harassment/\n",
      "03/05/2020 01:24:41 - INFO - transformers.configuration_utils -   Configuration saved in ../../../../model_save/Dos-Fases-all_Harassment/config.json\n",
      "03/05/2020 01:24:44 - INFO - transformers.modeling_utils -   Model weights saved in ../../../../model_save/Dos-Fases-all_Harassment/pytorch_model.bin\n",
      "03/05/2020 01:24:44 - INFO - transformers.configuration_utils -   loading configuration file ../../../../model_save/Dos-Fases-all_Harassment/config.json\n",
      "03/05/2020 01:24:44 - INFO - transformers.configuration_utils -   Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": null,\n",
      "  \"do_sample\": false,\n",
      "  \"eos_token_ids\": null,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 4,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": null,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "03/05/2020 01:24:44 - INFO - transformers.modeling_utils -   loading weights file ../../../../model_save/Dos-Fases-all_Harassment/pytorch_model.bin\n",
      "03/05/2020 01:24:46 - INFO - transformers.tokenization_utils -   Model name '../../../../model_save/Dos-Fases-all_Harassment/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming '../../../../model_save/Dos-Fases-all_Harassment/' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "03/05/2020 01:24:46 - INFO - transformers.tokenization_utils -   Didn't find file ../../../../model_save/Dos-Fases-all_Harassment/added_tokens.json. We won't load it.\n",
      "03/05/2020 01:24:46 - INFO - transformers.tokenization_utils -   loading file ../../../../model_save/Dos-Fases-all_Harassment/vocab.txt\n",
      "03/05/2020 01:24:46 - INFO - transformers.tokenization_utils -   loading file None\n",
      "03/05/2020 01:24:46 - INFO - transformers.tokenization_utils -   loading file ../../../../model_save/Dos-Fases-all_Harassment/special_tokens_map.json\n",
      "03/05/2020 01:24:46 - INFO - transformers.tokenization_utils -   loading file ../../../../model_save/Dos-Fases-all_Harassment/tokenizer_config.json\n",
      "03/05/2020 01:24:46 - INFO - __main__ -   Evaluate the following checkpoints: ['../../../../model_save/Dos-Fases-all_Harassment/']\n",
      "03/05/2020 01:24:46 - INFO - transformers.configuration_utils -   loading configuration file ../../../../model_save/Dos-Fases-all_Harassment/config.json\n",
      "03/05/2020 01:24:46 - INFO - transformers.configuration_utils -   Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": null,\n",
      "  \"do_sample\": false,\n",
      "  \"eos_token_ids\": null,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 4,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": null,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/05/2020 01:24:46 - INFO - transformers.modeling_utils -   loading weights file ../../../../model_save/Dos-Fases-all_Harassment/pytorch_model.bin\n",
      "03/05/2020 01:24:48 - INFO - __main__ -   ***** Running evaluation  *****\n",
      "03/05/2020 01:24:48 - INFO - __main__ -     Num examples = 503\n",
      "03/05/2020 01:24:48 - INFO - __main__ -     Batch size = 16\n",
      "Evaluating: 100%|██████████| 32/32 [00:03<00:00, 10.54it/s]\n",
      "03/05/2020 01:24:51 - INFO - __main__ -   ***** Eval results  *****\n",
      "03/05/2020 01:24:51 - INFO - __main__ -     perplexity = tensor(315.0999)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity epoch: 315.09985\n",
      "\n",
      "Memory usage         :  3.80 MB\n",
      "GC collected objects : 844\n",
      "Memory usage         :  3.80 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/05/2020 01:24:52 - INFO - transformers.tokenization_utils -   Model name '../../../../model_save/Dos-Fases-all_Harassment/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming '../../../../model_save/Dos-Fases-all_Harassment/' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "03/05/2020 01:24:52 - INFO - transformers.tokenization_utils -   Didn't find file ../../../../model_save/Dos-Fases-all_Harassment/added_tokens.json. We won't load it.\n",
      "03/05/2020 01:24:52 - INFO - transformers.tokenization_utils -   loading file ../../../../model_save/Dos-Fases-all_Harassment/vocab.txt\n",
      "03/05/2020 01:24:52 - INFO - transformers.tokenization_utils -   loading file None\n",
      "03/05/2020 01:24:52 - INFO - transformers.tokenization_utils -   loading file ../../../../model_save/Dos-Fases-all_Harassment/special_tokens_map.json\n",
      "03/05/2020 01:24:52 - INFO - transformers.tokenization_utils -   loading file ../../../../model_save/Dos-Fases-all_Harassment/tokenizer_config.json\n",
      "03/05/2020 01:24:52 - INFO - transformers.configuration_utils -   loading configuration file ../../../../model_save/Dos-Fases-all_Harassment/config.json\n",
      "03/05/2020 01:24:52 - INFO - transformers.configuration_utils -   Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": null,\n",
      "  \"do_sample\": false,\n",
      "  \"eos_token_ids\": null,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 4,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": null,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "03/05/2020 01:24:52 - INFO - transformers.modeling_utils -   loading weights file ../../../../model_save/Dos-Fases-all_Harassment/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GC collected objects : 0\n",
      "Memory usage         :  3.80 MB\n",
      "Loading BERT tokenizer...\n",
      "Loading BERT Seq Class from  ../../../../model_save/Dos-Fases-all_Harassment/ ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/05/2020 01:24:53 - INFO - transformers.modeling_utils -   Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n",
      "03/05/2020 01:24:53 - INFO - transformers.modeling_utils -   Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo cargado correctamente desde  ../../../../model_save/Dos-Fases-all_Harassment/\n",
      "\n",
      "Padding/truncating all sentences to 50 values...\n",
      "\n",
      "Padding token: \"[PAD]\", ID: 0\n",
      "Completado.\n",
      "\n",
      "Padding/truncating all sentences to 50 values...\n",
      "\n",
      "Padding token: \"[PAD]\", ID: 0\n",
      "Completado.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/05/2020 01:26:36 - INFO - transformers.configuration_utils -   Configuration saved in ../../../../model_save/Dos-Fases-all_Harassment/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating F-macro...\n",
      "F macro: 0.416\n",
      "F macro none average:  [0.91544667 0.         0.         0.74755927]\n",
      "Accuracy: 0.848\n",
      "Saving model to ../../../../model_save/Dos-Fases-all_Harassment/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/05/2020 01:26:38 - INFO - transformers.modeling_utils -   Model weights saved in ../../../../model_save/Dos-Fases-all_Harassment/pytorch_model.bin\n",
      "03/05/2020 01:26:49 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "03/05/2020 01:26:49 - INFO - transformers.configuration_utils -   loading configuration file ../../../../model_save/Dos-Fases-all_Harassment/config.json\n",
      "03/05/2020 01:26:49 - INFO - transformers.configuration_utils -   Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": null,\n",
      "  \"do_sample\": false,\n",
      "  \"eos_token_ids\": null,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 4,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": null,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "03/05/2020 01:26:49 - INFO - transformers.tokenization_utils -   Model name '../../../../model_save/Dos-Fases-all_Harassment/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming '../../../../model_save/Dos-Fases-all_Harassment/' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "03/05/2020 01:26:49 - INFO - transformers.tokenization_utils -   Didn't find file ../../../../model_save/Dos-Fases-all_Harassment/added_tokens.json. We won't load it.\n",
      "03/05/2020 01:26:49 - INFO - transformers.tokenization_utils -   loading file ../../../../model_save/Dos-Fases-all_Harassment/vocab.txt\n",
      "03/05/2020 01:26:49 - INFO - transformers.tokenization_utils -   loading file None\n",
      "03/05/2020 01:26:49 - INFO - transformers.tokenization_utils -   loading file ../../../../model_save/Dos-Fases-all_Harassment/special_tokens_map.json\n",
      "03/05/2020 01:26:49 - INFO - transformers.tokenization_utils -   loading file ../../../../model_save/Dos-Fases-all_Harassment/tokenizer_config.json\n",
      "03/05/2020 01:26:49 - INFO - transformers.modeling_utils -   loading weights file ../../../../model_save/Dos-Fases-all_Harassment/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage         :  3.80 MB\n",
      "GC collected objects : 427\n",
      "Memory usage         :  3.80 MB\n",
      "GC collected objects : 0\n",
      "Memory usage         :  3.80 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/05/2020 01:26:51 - INFO - transformers.modeling_utils -   Weights of BertForMaskedLM not initialized from pretrained model: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']\n",
      "03/05/2020 01:26:51 - INFO - transformers.modeling_utils -   Weights from pretrained model not used in BertForMaskedLM: ['classifier.weight', 'classifier.bias']\n",
      "03/05/2020 01:26:51 - INFO - __main__ -   Training/evaluation parameters <__main__.Args_LM object at 0x7f7ea377dd30>\n",
      "03/05/2020 01:26:51 - INFO - __main__ -   ***** Running training *****\n",
      "03/05/2020 01:26:51 - INFO - __main__ -     Num examples = 2458\n",
      "03/05/2020 01:26:51 - INFO - __main__ -     Num Epochs = 1\n",
      "03/05/2020 01:26:51 - INFO - __main__ -     Instantaneous batch size per GPU = 16\n",
      "03/05/2020 01:26:51 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "03/05/2020 01:26:51 - INFO - __main__ -     Gradient Accumulation steps = 1\n",
      "03/05/2020 01:26:51 - INFO - __main__ -     Total optimization steps = 154\n",
      "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Iteration:   0%|          | 0/154 [00:00<?, ?it/s]\u001b[A\n",
      "Iteration:   1%|          | 1/154 [00:00<00:57,  2.65it/s]\u001b[A\n",
      "Iteration:   1%|▏         | 2/154 [00:00<00:56,  2.70it/s]\u001b[A\n",
      "Iteration:   2%|▏         | 3/154 [00:01<00:55,  2.74it/s]\u001b[A\n",
      "Iteration:   3%|▎         | 4/154 [00:01<00:53,  2.78it/s]\u001b[A\n",
      "Iteration:   3%|▎         | 5/154 [00:01<00:53,  2.80it/s]\u001b[A\n",
      "Iteration:   4%|▍         | 6/154 [00:02<00:52,  2.82it/s]\u001b[A\n",
      "Iteration:   5%|▍         | 7/154 [00:02<00:51,  2.84it/s]\u001b[A\n",
      "Iteration:   5%|▌         | 8/154 [00:02<00:51,  2.84it/s]\u001b[A\n",
      "Iteration:   6%|▌         | 9/154 [00:03<00:50,  2.85it/s]\u001b[A\n",
      "Iteration:   6%|▋         | 10/154 [00:03<00:50,  2.86it/s]\u001b[A\n",
      "Iteration:   7%|▋         | 11/154 [00:03<00:49,  2.86it/s]\u001b[A\n",
      "Iteration:   8%|▊         | 12/154 [00:04<00:49,  2.86it/s]\u001b[A\n",
      "Iteration:   8%|▊         | 13/154 [00:04<00:49,  2.85it/s]\u001b[A\n",
      "Iteration:   9%|▉         | 14/154 [00:04<00:49,  2.85it/s]\u001b[A\n",
      "Iteration:  10%|▉         | 15/154 [00:05<00:48,  2.87it/s]\u001b[A\n",
      "Iteration:  10%|█         | 16/154 [00:05<00:48,  2.87it/s]\u001b[A\n",
      "Iteration:  11%|█         | 17/154 [00:05<00:47,  2.87it/s]\u001b[A\n",
      "Iteration:  12%|█▏        | 18/154 [00:06<00:47,  2.87it/s]\u001b[A\n",
      "Iteration:  12%|█▏        | 19/154 [00:06<00:47,  2.86it/s]\u001b[A\n",
      "Iteration:  13%|█▎        | 20/154 [00:07<00:46,  2.87it/s]\u001b[A\n",
      "Iteration:  14%|█▎        | 21/154 [00:07<00:46,  2.87it/s]\u001b[A\n",
      "Iteration:  14%|█▍        | 22/154 [00:07<00:45,  2.87it/s]\u001b[A\n",
      "Iteration:  15%|█▍        | 23/154 [00:08<00:45,  2.87it/s]\u001b[A\n",
      "Iteration:  16%|█▌        | 24/154 [00:08<00:45,  2.88it/s]\u001b[A\n",
      "Iteration:  16%|█▌        | 25/154 [00:08<00:44,  2.87it/s]\u001b[A\n",
      "Iteration:  17%|█▋        | 26/154 [00:09<00:44,  2.86it/s]\u001b[A\n",
      "Iteration:  18%|█▊        | 27/154 [00:09<00:44,  2.86it/s]\u001b[A\n",
      "Iteration:  18%|█▊        | 28/154 [00:09<00:43,  2.87it/s]\u001b[A\n",
      "Iteration:  19%|█▉        | 29/154 [00:10<00:43,  2.87it/s]\u001b[A\n",
      "Iteration:  19%|█▉        | 30/154 [00:10<00:43,  2.86it/s]\u001b[A\n",
      "Iteration:  20%|██        | 31/154 [00:10<00:42,  2.87it/s]\u001b[A\n",
      "Iteration:  21%|██        | 32/154 [00:11<00:42,  2.87it/s]\u001b[A\n",
      "Iteration:  21%|██▏       | 33/154 [00:11<00:42,  2.87it/s]\u001b[A\n",
      "Iteration:  22%|██▏       | 34/154 [00:11<00:41,  2.87it/s]\u001b[A\n",
      "Iteration:  23%|██▎       | 35/154 [00:12<00:41,  2.87it/s]\u001b[A\n",
      "Iteration:  23%|██▎       | 36/154 [00:12<00:41,  2.87it/s]\u001b[A\n",
      "Iteration:  24%|██▍       | 37/154 [00:12<00:40,  2.86it/s]\u001b[A\n",
      "Iteration:  25%|██▍       | 38/154 [00:13<00:40,  2.86it/s]\u001b[A\n",
      "Iteration:  25%|██▌       | 39/154 [00:13<00:40,  2.86it/s]\u001b[A\n",
      "Iteration:  26%|██▌       | 40/154 [00:13<00:39,  2.87it/s]\u001b[A\n",
      "Iteration:  27%|██▋       | 41/154 [00:14<00:39,  2.86it/s]\u001b[A\n",
      "Iteration:  27%|██▋       | 42/154 [00:14<00:39,  2.87it/s]\u001b[A\n",
      "Iteration:  28%|██▊       | 43/154 [00:15<00:38,  2.86it/s]\u001b[A\n",
      "Iteration:  29%|██▊       | 44/154 [00:15<00:38,  2.86it/s]\u001b[A\n",
      "Iteration:  29%|██▉       | 45/154 [00:15<00:37,  2.87it/s]\u001b[A\n",
      "Iteration:  30%|██▉       | 46/154 [00:16<00:37,  2.87it/s]\u001b[A\n",
      "Iteration:  31%|███       | 47/154 [00:16<00:37,  2.86it/s]\u001b[A\n",
      "Iteration:  31%|███       | 48/154 [00:16<00:37,  2.86it/s]\u001b[A\n",
      "Iteration:  32%|███▏      | 49/154 [00:17<00:36,  2.86it/s]\u001b[A\n",
      "Iteration:  32%|███▏      | 50/154 [00:17<00:36,  2.86it/s]\u001b[A\n",
      "Iteration:  33%|███▎      | 51/154 [00:17<00:36,  2.86it/s]\u001b[A\n",
      "Iteration:  34%|███▍      | 52/154 [00:18<00:35,  2.86it/s]\u001b[A\n",
      "Iteration:  34%|███▍      | 53/154 [00:18<00:35,  2.86it/s]\u001b[A\n",
      "Iteration:  35%|███▌      | 54/154 [00:18<00:34,  2.87it/s]\u001b[A\n",
      "Iteration:  36%|███▌      | 55/154 [00:19<00:34,  2.86it/s]\u001b[A\n",
      "Iteration:  36%|███▋      | 56/154 [00:19<00:34,  2.87it/s]\u001b[A\n",
      "Iteration:  37%|███▋      | 57/154 [00:19<00:33,  2.87it/s]\u001b[A\n",
      "Iteration:  38%|███▊      | 58/154 [00:20<00:33,  2.87it/s]\u001b[A\n",
      "Iteration:  38%|███▊      | 59/154 [00:20<00:33,  2.88it/s]\u001b[A\n",
      "Iteration:  39%|███▉      | 60/154 [00:20<00:32,  2.87it/s]\u001b[A\n",
      "Iteration:  40%|███▉      | 61/154 [00:21<00:32,  2.86it/s]\u001b[A\n",
      "Iteration:  40%|████      | 62/154 [00:21<00:32,  2.87it/s]\u001b[A\n",
      "Iteration:  41%|████      | 63/154 [00:22<00:31,  2.87it/s]\u001b[A\n",
      "Iteration:  42%|████▏     | 64/154 [00:22<00:31,  2.87it/s]\u001b[A\n",
      "Iteration:  42%|████▏     | 65/154 [00:22<00:31,  2.87it/s]\u001b[A\n",
      "Iteration:  43%|████▎     | 66/154 [00:23<00:30,  2.87it/s]\u001b[A\n",
      "Iteration:  44%|████▎     | 67/154 [00:23<00:30,  2.87it/s]\u001b[A\n",
      "Iteration:  44%|████▍     | 68/154 [00:23<00:29,  2.87it/s]\u001b[A\n",
      "Iteration:  45%|████▍     | 69/154 [00:24<00:29,  2.87it/s]\u001b[A\n",
      "Iteration:  45%|████▌     | 70/154 [00:24<00:29,  2.87it/s]\u001b[A\n",
      "Iteration:  46%|████▌     | 71/154 [00:24<00:28,  2.87it/s]\u001b[A\n",
      "Iteration:  47%|████▋     | 72/154 [00:25<00:28,  2.87it/s]\u001b[A\n",
      "Iteration:  47%|████▋     | 73/154 [00:25<00:28,  2.87it/s]\u001b[A\n",
      "Iteration:  48%|████▊     | 74/154 [00:25<00:27,  2.87it/s]\u001b[A\n",
      "Iteration:  49%|████▊     | 75/154 [00:26<00:27,  2.86it/s]\u001b[A\n",
      "Iteration:  49%|████▉     | 76/154 [00:26<00:27,  2.87it/s]\u001b[A\n",
      "Iteration:  50%|█████     | 77/154 [00:26<00:26,  2.87it/s]\u001b[A\n",
      "Iteration:  51%|█████     | 78/154 [00:27<00:26,  2.87it/s]\u001b[A\n",
      "Iteration:  51%|█████▏    | 79/154 [00:27<00:26,  2.87it/s]\u001b[A\n",
      "Iteration:  52%|█████▏    | 80/154 [00:27<00:25,  2.87it/s]\u001b[A\n",
      "Iteration:  53%|█████▎    | 81/154 [00:28<00:25,  2.87it/s]\u001b[A\n",
      "Iteration:  53%|█████▎    | 82/154 [00:28<00:25,  2.87it/s]\u001b[A\n",
      "Iteration:  54%|█████▍    | 83/154 [00:28<00:24,  2.87it/s]\u001b[A\n",
      "Iteration:  55%|█████▍    | 84/154 [00:29<00:24,  2.87it/s]\u001b[A\n",
      "Iteration:  55%|█████▌    | 85/154 [00:29<00:24,  2.87it/s]\u001b[A\n",
      "Iteration:  56%|█████▌    | 86/154 [00:30<00:23,  2.87it/s]\u001b[A\n",
      "Iteration:  56%|█████▋    | 87/154 [00:30<00:23,  2.87it/s]\u001b[A\n",
      "Iteration:  57%|█████▋    | 88/154 [00:30<00:23,  2.86it/s]\u001b[A\n",
      "Iteration:  58%|█████▊    | 89/154 [00:31<00:22,  2.85it/s]\u001b[A\n",
      "Iteration:  58%|█████▊    | 90/154 [00:31<00:22,  2.86it/s]\u001b[A\n",
      "Iteration:  59%|█████▉    | 91/154 [00:31<00:22,  2.86it/s]\u001b[A\n",
      "Iteration:  60%|█████▉    | 92/154 [00:32<00:21,  2.86it/s]\u001b[A\n",
      "Iteration:  60%|██████    | 93/154 [00:32<00:21,  2.86it/s]\u001b[A\n",
      "Iteration:  61%|██████    | 94/154 [00:32<00:20,  2.86it/s]\u001b[A\n",
      "Iteration:  62%|██████▏   | 95/154 [00:33<00:20,  2.87it/s]\u001b[A\n",
      "Iteration:  62%|██████▏   | 96/154 [00:33<00:20,  2.87it/s]\u001b[A\n",
      "Iteration:  63%|██████▎   | 97/154 [00:33<00:19,  2.88it/s]\u001b[A\n",
      "Iteration:  64%|██████▎   | 98/154 [00:34<00:19,  2.88it/s]\u001b[A\n",
      "Iteration:  64%|██████▍   | 99/154 [00:34<00:19,  2.87it/s]\u001b[A03/05/2020 01:27:26 - INFO - transformers.configuration_utils -   Configuration saved in ../../../../model_save/Dos-Fases-all_Harassment/checkpoint-100/config.json\n",
      "03/05/2020 01:27:28 - INFO - transformers.modeling_utils -   Model weights saved in ../../../../model_save/Dos-Fases-all_Harassment/checkpoint-100/pytorch_model.bin\n",
      "03/05/2020 01:27:28 - INFO - __main__ -   Saving model checkpoint to ../../../../model_save/Dos-Fases-all_Harassment/checkpoint-100\n",
      "03/05/2020 01:27:35 - INFO - __main__ -   Saving optimizer and scheduler states to ../../../../model_save/Dos-Fases-all_Harassment/checkpoint-100\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:  65%|██████▍   | 100/154 [00:43<02:44,  3.04s/it]\u001b[A\n",
      "Iteration:  66%|██████▌   | 101/154 [00:44<01:58,  2.24s/it]\u001b[A\n",
      "Iteration:  66%|██████▌   | 102/154 [00:44<01:27,  1.68s/it]\u001b[A\n",
      "Iteration:  67%|██████▋   | 103/154 [00:44<01:05,  1.28s/it]\u001b[A\n",
      "Iteration:  68%|██████▊   | 104/154 [00:45<00:49,  1.00it/s]\u001b[A\n",
      "Iteration:  68%|██████▊   | 105/154 [00:45<00:39,  1.24it/s]\u001b[A\n",
      "Iteration:  69%|██████▉   | 106/154 [00:46<00:32,  1.50it/s]\u001b[A\n",
      "Iteration:  69%|██████▉   | 107/154 [00:46<00:26,  1.75it/s]\u001b[A\n",
      "Iteration:  70%|███████   | 108/154 [00:46<00:23,  1.98it/s]\u001b[A\n",
      "Iteration:  71%|███████   | 109/154 [00:47<00:20,  2.18it/s]\u001b[A\n",
      "Iteration:  71%|███████▏  | 110/154 [00:47<00:18,  2.35it/s]\u001b[A\n",
      "Iteration:  72%|███████▏  | 111/154 [00:47<00:17,  2.49it/s]\u001b[A\n",
      "Iteration:  73%|███████▎  | 112/154 [00:48<00:16,  2.60it/s]\u001b[A\n",
      "Iteration:  73%|███████▎  | 113/154 [00:48<00:15,  2.67it/s]\u001b[A\n",
      "Iteration:  74%|███████▍  | 114/154 [00:48<00:14,  2.72it/s]\u001b[A\n",
      "Iteration:  75%|███████▍  | 115/154 [00:49<00:14,  2.76it/s]\u001b[A\n",
      "Iteration:  75%|███████▌  | 116/154 [00:49<00:13,  2.78it/s]\u001b[A\n",
      "Iteration:  76%|███████▌  | 117/154 [00:49<00:13,  2.81it/s]\u001b[A\n",
      "Iteration:  77%|███████▋  | 118/154 [00:50<00:12,  2.82it/s]\u001b[A\n",
      "Iteration:  77%|███████▋  | 119/154 [00:50<00:12,  2.83it/s]\u001b[A\n",
      "Iteration:  78%|███████▊  | 120/154 [00:50<00:11,  2.84it/s]\u001b[A\n",
      "Iteration:  79%|███████▊  | 121/154 [00:51<00:11,  2.85it/s]\u001b[A\n",
      "Iteration:  79%|███████▉  | 122/154 [00:51<00:11,  2.85it/s]\u001b[A\n",
      "Iteration:  80%|███████▉  | 123/154 [00:51<00:10,  2.86it/s]\u001b[A\n",
      "Iteration:  81%|████████  | 124/154 [00:52<00:10,  2.86it/s]\u001b[A\n",
      "Iteration:  81%|████████  | 125/154 [00:52<00:10,  2.85it/s]\u001b[A\n",
      "Iteration:  82%|████████▏ | 126/154 [00:53<00:09,  2.85it/s]\u001b[A\n",
      "Iteration:  82%|████████▏ | 127/154 [00:53<00:09,  2.85it/s]\u001b[A\n",
      "Iteration:  83%|████████▎ | 128/154 [00:53<00:09,  2.86it/s]\u001b[A\n",
      "Iteration:  84%|████████▍ | 129/154 [00:54<00:08,  2.86it/s]\u001b[A\n",
      "Iteration:  84%|████████▍ | 130/154 [00:54<00:08,  2.86it/s]\u001b[A\n",
      "Iteration:  85%|████████▌ | 131/154 [00:54<00:08,  2.86it/s]\u001b[A\n",
      "Iteration:  86%|████████▌ | 132/154 [00:55<00:07,  2.86it/s]\u001b[A\n",
      "Iteration:  86%|████████▋ | 133/154 [00:55<00:07,  2.86it/s]\u001b[A\n",
      "Iteration:  87%|████████▋ | 134/154 [00:55<00:07,  2.85it/s]\u001b[A\n",
      "Iteration:  88%|████████▊ | 135/154 [00:56<00:06,  2.86it/s]\u001b[A\n",
      "Iteration:  88%|████████▊ | 136/154 [00:56<00:06,  2.86it/s]\u001b[A\n",
      "Iteration:  89%|████████▉ | 137/154 [00:56<00:05,  2.85it/s]\u001b[A\n",
      "Iteration:  90%|████████▉ | 138/154 [00:57<00:05,  2.84it/s]\u001b[A\n",
      "Iteration:  90%|█████████ | 139/154 [00:57<00:05,  2.84it/s]\u001b[A\n",
      "Iteration:  91%|█████████ | 140/154 [00:57<00:04,  2.85it/s]\u001b[A\n",
      "Iteration:  92%|█████████▏| 141/154 [00:58<00:04,  2.85it/s]\u001b[A\n",
      "Iteration:  92%|█████████▏| 142/154 [00:58<00:04,  2.84it/s]\u001b[A\n",
      "Iteration:  93%|█████████▎| 143/154 [00:58<00:03,  2.85it/s]\u001b[A\n",
      "Iteration:  94%|█████████▎| 144/154 [00:59<00:03,  2.85it/s]\u001b[A\n",
      "Iteration:  94%|█████████▍| 145/154 [00:59<00:03,  2.85it/s]\u001b[A\n",
      "Iteration:  95%|█████████▍| 146/154 [01:00<00:02,  2.85it/s]\u001b[A\n",
      "Iteration:  95%|█████████▌| 147/154 [01:00<00:02,  2.85it/s]\u001b[A\n",
      "Iteration:  96%|█████████▌| 148/154 [01:00<00:02,  2.85it/s]\u001b[A\n",
      "Iteration:  97%|█████████▋| 149/154 [01:01<00:01,  2.85it/s]\u001b[A\n",
      "Iteration:  97%|█████████▋| 150/154 [01:01<00:01,  2.85it/s]\u001b[A\n",
      "Iteration:  98%|█████████▊| 151/154 [01:01<00:01,  2.85it/s]\u001b[A\n",
      "Iteration:  99%|█████████▊| 152/154 [01:02<00:00,  2.85it/s]\u001b[A\n",
      "Iteration:  99%|█████████▉| 153/154 [01:02<00:00,  2.85it/s]\u001b[A\n",
      "Iteration: 100%|██████████| 154/154 [01:02<00:00,  2.45it/s]\u001b[A\n",
      "Epoch: 100%|██████████| 1/1 [01:02<00:00, 62.75s/it]\n",
      "03/05/2020 01:27:53 - INFO - __main__ -    global_step = 154, average loss = 4.336089587830878\n",
      "03/05/2020 01:27:53 - INFO - __main__ -   Saving model checkpoint to ../../../../model_save/Dos-Fases-all_Harassment/\n",
      "03/05/2020 01:27:53 - INFO - transformers.configuration_utils -   Configuration saved in ../../../../model_save/Dos-Fases-all_Harassment/config.json\n",
      "03/05/2020 01:27:56 - INFO - transformers.modeling_utils -   Model weights saved in ../../../../model_save/Dos-Fases-all_Harassment/pytorch_model.bin\n",
      "03/05/2020 01:27:56 - INFO - transformers.configuration_utils -   loading configuration file ../../../../model_save/Dos-Fases-all_Harassment/config.json\n",
      "03/05/2020 01:27:56 - INFO - transformers.configuration_utils -   Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": null,\n",
      "  \"do_sample\": false,\n",
      "  \"eos_token_ids\": null,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 4,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": null,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "03/05/2020 01:27:56 - INFO - transformers.modeling_utils -   loading weights file ../../../../model_save/Dos-Fases-all_Harassment/pytorch_model.bin\n",
      "03/05/2020 01:27:58 - INFO - transformers.tokenization_utils -   Model name '../../../../model_save/Dos-Fases-all_Harassment/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming '../../../../model_save/Dos-Fases-all_Harassment/' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "03/05/2020 01:27:58 - INFO - transformers.tokenization_utils -   Didn't find file ../../../../model_save/Dos-Fases-all_Harassment/added_tokens.json. We won't load it.\n",
      "03/05/2020 01:27:58 - INFO - transformers.tokenization_utils -   loading file ../../../../model_save/Dos-Fases-all_Harassment/vocab.txt\n",
      "03/05/2020 01:27:58 - INFO - transformers.tokenization_utils -   loading file None\n",
      "03/05/2020 01:27:58 - INFO - transformers.tokenization_utils -   loading file ../../../../model_save/Dos-Fases-all_Harassment/special_tokens_map.json\n",
      "03/05/2020 01:27:58 - INFO - transformers.tokenization_utils -   loading file ../../../../model_save/Dos-Fases-all_Harassment/tokenizer_config.json\n",
      "03/05/2020 01:27:58 - INFO - __main__ -   Evaluate the following checkpoints: ['../../../../model_save/Dos-Fases-all_Harassment/']\n",
      "03/05/2020 01:27:58 - INFO - transformers.configuration_utils -   loading configuration file ../../../../model_save/Dos-Fases-all_Harassment/config.json\n",
      "03/05/2020 01:27:58 - INFO - transformers.configuration_utils -   Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": null,\n",
      "  \"do_sample\": false,\n",
      "  \"eos_token_ids\": null,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 4,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": null,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/05/2020 01:27:58 - INFO - transformers.modeling_utils -   loading weights file ../../../../model_save/Dos-Fases-all_Harassment/pytorch_model.bin\n",
      "03/05/2020 01:28:00 - INFO - __main__ -   ***** Running evaluation  *****\n",
      "03/05/2020 01:28:00 - INFO - __main__ -     Num examples = 503\n",
      "03/05/2020 01:28:00 - INFO - __main__ -     Batch size = 16\n",
      "Evaluating: 100%|██████████| 32/32 [00:03<00:00, 10.57it/s]\n",
      "03/05/2020 01:28:03 - INFO - __main__ -   ***** Eval results  *****\n",
      "03/05/2020 01:28:03 - INFO - __main__ -     perplexity = tensor(297.5224)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity epoch: 297.52237\n",
      "\n",
      "Memory usage         :  3.80 MB\n",
      "GC collected objects : 844\n",
      "Memory usage         :  3.80 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/05/2020 01:28:04 - INFO - transformers.tokenization_utils -   Model name '../../../../model_save/Dos-Fases-all_Harassment/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming '../../../../model_save/Dos-Fases-all_Harassment/' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "03/05/2020 01:28:04 - INFO - transformers.tokenization_utils -   Didn't find file ../../../../model_save/Dos-Fases-all_Harassment/added_tokens.json. We won't load it.\n",
      "03/05/2020 01:28:04 - INFO - transformers.tokenization_utils -   loading file ../../../../model_save/Dos-Fases-all_Harassment/vocab.txt\n",
      "03/05/2020 01:28:04 - INFO - transformers.tokenization_utils -   loading file None\n",
      "03/05/2020 01:28:04 - INFO - transformers.tokenization_utils -   loading file ../../../../model_save/Dos-Fases-all_Harassment/special_tokens_map.json\n",
      "03/05/2020 01:28:04 - INFO - transformers.tokenization_utils -   loading file ../../../../model_save/Dos-Fases-all_Harassment/tokenizer_config.json\n",
      "03/05/2020 01:28:04 - INFO - transformers.configuration_utils -   loading configuration file ../../../../model_save/Dos-Fases-all_Harassment/config.json\n",
      "03/05/2020 01:28:04 - INFO - transformers.configuration_utils -   Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": null,\n",
      "  \"do_sample\": false,\n",
      "  \"eos_token_ids\": null,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 4,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": null,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "03/05/2020 01:28:04 - INFO - transformers.modeling_utils -   loading weights file ../../../../model_save/Dos-Fases-all_Harassment/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GC collected objects : 0\n",
      "Memory usage         :  3.80 MB\n",
      "Loading BERT tokenizer...\n",
      "Loading BERT Seq Class from  ../../../../model_save/Dos-Fases-all_Harassment/ ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/05/2020 01:28:05 - INFO - transformers.modeling_utils -   Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n",
      "03/05/2020 01:28:05 - INFO - transformers.modeling_utils -   Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo cargado correctamente desde  ../../../../model_save/Dos-Fases-all_Harassment/\n",
      "\n",
      "Padding/truncating all sentences to 50 values...\n",
      "\n",
      "Padding token: \"[PAD]\", ID: 0\n",
      "Completado.\n",
      "\n",
      "Padding/truncating all sentences to 50 values...\n",
      "\n",
      "Padding token: \"[PAD]\", ID: 0\n",
      "Completado.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/05/2020 01:29:48 - INFO - transformers.configuration_utils -   Configuration saved in ../../../../model_save/Dos-Fases-all_Harassment/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating F-macro...\n",
      "F macro: 0.417\n",
      "F macro none average:  [0.91651759 0.         0.         0.75211268]\n",
      "Accuracy: 0.850\n",
      "Saving model to ../../../../model_save/Dos-Fases-all_Harassment/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/05/2020 01:29:50 - INFO - transformers.modeling_utils -   Model weights saved in ../../../../model_save/Dos-Fases-all_Harassment/pytorch_model.bin\n",
      "03/05/2020 01:30:01 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "03/05/2020 01:30:01 - INFO - transformers.configuration_utils -   loading configuration file ../../../../model_save/Dos-Fases-all_Harassment/config.json\n",
      "03/05/2020 01:30:01 - INFO - transformers.configuration_utils -   Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": null,\n",
      "  \"do_sample\": false,\n",
      "  \"eos_token_ids\": null,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 4,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": null,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "03/05/2020 01:30:01 - INFO - transformers.tokenization_utils -   Model name '../../../../model_save/Dos-Fases-all_Harassment/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming '../../../../model_save/Dos-Fases-all_Harassment/' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "03/05/2020 01:30:01 - INFO - transformers.tokenization_utils -   Didn't find file ../../../../model_save/Dos-Fases-all_Harassment/added_tokens.json. We won't load it.\n",
      "03/05/2020 01:30:01 - INFO - transformers.tokenization_utils -   loading file ../../../../model_save/Dos-Fases-all_Harassment/vocab.txt\n",
      "03/05/2020 01:30:01 - INFO - transformers.tokenization_utils -   loading file None\n",
      "03/05/2020 01:30:01 - INFO - transformers.tokenization_utils -   loading file ../../../../model_save/Dos-Fases-all_Harassment/special_tokens_map.json\n",
      "03/05/2020 01:30:01 - INFO - transformers.tokenization_utils -   loading file ../../../../model_save/Dos-Fases-all_Harassment/tokenizer_config.json\n",
      "03/05/2020 01:30:01 - INFO - transformers.modeling_utils -   loading weights file ../../../../model_save/Dos-Fases-all_Harassment/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage         :  3.80 MB\n",
      "GC collected objects : 427\n",
      "Memory usage         :  3.80 MB\n",
      "GC collected objects : 0\n",
      "Memory usage         :  3.80 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/05/2020 01:30:03 - INFO - transformers.modeling_utils -   Weights of BertForMaskedLM not initialized from pretrained model: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']\n",
      "03/05/2020 01:30:03 - INFO - transformers.modeling_utils -   Weights from pretrained model not used in BertForMaskedLM: ['classifier.weight', 'classifier.bias']\n",
      "03/05/2020 01:30:03 - INFO - __main__ -   Training/evaluation parameters <__main__.Args_LM object at 0x7f7ea377dd30>\n",
      "03/05/2020 01:30:03 - INFO - __main__ -   ***** Running training *****\n",
      "03/05/2020 01:30:03 - INFO - __main__ -     Num examples = 2458\n",
      "03/05/2020 01:30:03 - INFO - __main__ -     Num Epochs = 1\n",
      "03/05/2020 01:30:03 - INFO - __main__ -     Instantaneous batch size per GPU = 16\n",
      "03/05/2020 01:30:03 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "03/05/2020 01:30:03 - INFO - __main__ -     Gradient Accumulation steps = 1\n",
      "03/05/2020 01:30:03 - INFO - __main__ -     Total optimization steps = 154\n",
      "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Iteration:   0%|          | 0/154 [00:00<?, ?it/s]\u001b[A\n",
      "Iteration:   1%|          | 1/154 [00:00<00:58,  2.63it/s]\u001b[A\n",
      "Iteration:   1%|▏         | 2/154 [00:00<00:56,  2.70it/s]\u001b[A\n",
      "Iteration:   2%|▏         | 3/154 [00:01<00:55,  2.74it/s]\u001b[A\n",
      "Iteration:   3%|▎         | 4/154 [00:01<00:53,  2.79it/s]\u001b[A\n",
      "Iteration:   3%|▎         | 5/154 [00:01<00:52,  2.82it/s]\u001b[A\n",
      "Iteration:   4%|▍         | 6/154 [00:02<00:52,  2.84it/s]\u001b[A\n",
      "Iteration:   5%|▍         | 7/154 [00:02<00:51,  2.85it/s]\u001b[A\n",
      "Iteration:   5%|▌         | 8/154 [00:02<00:51,  2.85it/s]\u001b[A\n",
      "Iteration:   6%|▌         | 9/154 [00:03<00:50,  2.86it/s]\u001b[A\n",
      "Iteration:   6%|▋         | 10/154 [00:03<00:50,  2.87it/s]\u001b[A\n",
      "Iteration:   7%|▋         | 11/154 [00:03<00:49,  2.87it/s]\u001b[A\n",
      "Iteration:   8%|▊         | 12/154 [00:04<00:49,  2.87it/s]\u001b[A\n",
      "Iteration:   8%|▊         | 13/154 [00:04<00:49,  2.87it/s]\u001b[A\n",
      "Iteration:   9%|▉         | 14/154 [00:04<00:48,  2.88it/s]\u001b[A\n",
      "Iteration:  10%|▉         | 15/154 [00:05<00:48,  2.87it/s]\u001b[A\n",
      "Iteration:  10%|█         | 16/154 [00:05<00:47,  2.88it/s]\u001b[A\n",
      "Iteration:  11%|█         | 17/154 [00:05<00:47,  2.88it/s]\u001b[A\n",
      "Iteration:  12%|█▏        | 18/154 [00:06<00:47,  2.88it/s]\u001b[A\n",
      "Iteration:  12%|█▏        | 19/154 [00:06<00:46,  2.88it/s]\u001b[A\n",
      "Iteration:  13%|█▎        | 20/154 [00:06<00:46,  2.87it/s]\u001b[A\n",
      "Iteration:  14%|█▎        | 21/154 [00:07<00:46,  2.87it/s]\u001b[A\n",
      "Iteration:  14%|█▍        | 22/154 [00:07<00:46,  2.87it/s]\u001b[A\n",
      "Iteration:  15%|█▍        | 23/154 [00:08<00:45,  2.87it/s]\u001b[A\n",
      "Iteration:  16%|█▌        | 24/154 [00:08<00:45,  2.87it/s]\u001b[A\n",
      "Iteration:  16%|█▌        | 25/154 [00:08<00:45,  2.86it/s]\u001b[A\n",
      "Iteration:  17%|█▋        | 26/154 [00:09<00:44,  2.86it/s]\u001b[A\n",
      "Iteration:  18%|█▊        | 27/154 [00:09<00:44,  2.86it/s]\u001b[A\n",
      "Iteration:  18%|█▊        | 28/154 [00:09<00:44,  2.86it/s]\u001b[A\n",
      "Iteration:  19%|█▉        | 29/154 [00:10<00:43,  2.86it/s]\u001b[A\n",
      "Iteration:  19%|█▉        | 30/154 [00:10<00:43,  2.86it/s]\u001b[A\n",
      "Iteration:  20%|██        | 31/154 [00:10<00:43,  2.86it/s]\u001b[A\n",
      "Iteration:  21%|██        | 32/154 [00:11<00:42,  2.86it/s]\u001b[A\n",
      "Iteration:  21%|██▏       | 33/154 [00:11<00:42,  2.87it/s]\u001b[A\n",
      "Iteration:  22%|██▏       | 34/154 [00:11<00:41,  2.86it/s]\u001b[A\n",
      "Iteration:  23%|██▎       | 35/154 [00:12<00:41,  2.86it/s]\u001b[A\n",
      "Iteration:  23%|██▎       | 36/154 [00:12<00:41,  2.86it/s]\u001b[A\n",
      "Iteration:  24%|██▍       | 37/154 [00:12<00:40,  2.86it/s]\u001b[A\n",
      "Iteration:  25%|██▍       | 38/154 [00:13<00:40,  2.86it/s]\u001b[A\n",
      "Iteration:  25%|██▌       | 39/154 [00:13<00:40,  2.86it/s]\u001b[A\n",
      "Iteration:  26%|██▌       | 40/154 [00:13<00:39,  2.87it/s]\u001b[A\n",
      "Iteration:  27%|██▋       | 41/154 [00:14<00:39,  2.87it/s]\u001b[A\n",
      "Iteration:  27%|██▋       | 42/154 [00:14<00:39,  2.87it/s]\u001b[A\n",
      "Iteration:  28%|██▊       | 43/154 [00:15<00:38,  2.87it/s]\u001b[A\n",
      "Iteration:  29%|██▊       | 44/154 [00:15<00:38,  2.87it/s]\u001b[A\n",
      "Iteration:  29%|██▉       | 45/154 [00:15<00:38,  2.87it/s]\u001b[A\n",
      "Iteration:  30%|██▉       | 46/154 [00:16<00:37,  2.87it/s]\u001b[A\n",
      "Iteration:  31%|███       | 47/154 [00:16<00:37,  2.86it/s]\u001b[A\n",
      "Iteration:  31%|███       | 48/154 [00:16<00:36,  2.87it/s]\u001b[A\n",
      "Iteration:  32%|███▏      | 49/154 [00:17<00:36,  2.86it/s]\u001b[A\n",
      "Iteration:  32%|███▏      | 50/154 [00:17<00:36,  2.87it/s]\u001b[A\n",
      "Iteration:  33%|███▎      | 51/154 [00:17<00:35,  2.88it/s]\u001b[A\n",
      "Iteration:  34%|███▍      | 52/154 [00:18<00:35,  2.88it/s]\u001b[A\n",
      "Iteration:  34%|███▍      | 53/154 [00:18<00:35,  2.88it/s]\u001b[A\n",
      "Iteration:  35%|███▌      | 54/154 [00:18<00:34,  2.88it/s]\u001b[A\n",
      "Iteration:  36%|███▌      | 55/154 [00:19<00:34,  2.87it/s]\u001b[A\n",
      "Iteration:  36%|███▋      | 56/154 [00:19<00:34,  2.87it/s]\u001b[A\n",
      "Iteration:  37%|███▋      | 57/154 [00:19<00:33,  2.87it/s]\u001b[A\n",
      "Iteration:  38%|███▊      | 58/154 [00:20<00:33,  2.86it/s]\u001b[A\n",
      "Iteration:  38%|███▊      | 59/154 [00:20<00:33,  2.87it/s]\u001b[A\n",
      "Iteration:  39%|███▉      | 60/154 [00:20<00:32,  2.87it/s]\u001b[A\n",
      "Iteration:  40%|███▉      | 61/154 [00:21<00:32,  2.87it/s]\u001b[A\n",
      "Iteration:  40%|████      | 62/154 [00:21<00:32,  2.87it/s]\u001b[A\n",
      "Iteration:  41%|████      | 63/154 [00:21<00:31,  2.87it/s]\u001b[A\n",
      "Iteration:  42%|████▏     | 64/154 [00:22<00:31,  2.87it/s]\u001b[A\n",
      "Iteration:  42%|████▏     | 65/154 [00:22<00:31,  2.87it/s]\u001b[A\n",
      "Iteration:  43%|████▎     | 66/154 [00:23<00:30,  2.87it/s]\u001b[A\n",
      "Iteration:  44%|████▎     | 67/154 [00:23<00:30,  2.86it/s]\u001b[A\n",
      "Iteration:  44%|████▍     | 68/154 [00:23<00:30,  2.86it/s]\u001b[A\n",
      "Iteration:  45%|████▍     | 69/154 [00:24<00:29,  2.86it/s]\u001b[A\n",
      "Iteration:  45%|████▌     | 70/154 [00:24<00:29,  2.86it/s]\u001b[A\n",
      "Iteration:  46%|████▌     | 71/154 [00:24<00:28,  2.86it/s]\u001b[A\n",
      "Iteration:  47%|████▋     | 72/154 [00:25<00:28,  2.86it/s]\u001b[A\n",
      "Iteration:  47%|████▋     | 73/154 [00:25<00:28,  2.86it/s]\u001b[A\n",
      "Iteration:  48%|████▊     | 74/154 [00:25<00:27,  2.87it/s]\u001b[A\n",
      "Iteration:  49%|████▊     | 75/154 [00:26<00:27,  2.87it/s]\u001b[A\n",
      "Iteration:  49%|████▉     | 76/154 [00:26<00:27,  2.87it/s]\u001b[A\n",
      "Iteration:  50%|█████     | 77/154 [00:26<00:26,  2.87it/s]\u001b[A\n",
      "Iteration:  51%|█████     | 78/154 [00:27<00:26,  2.87it/s]\u001b[A\n",
      "Iteration:  51%|█████▏    | 79/154 [00:27<00:26,  2.87it/s]\u001b[A\n",
      "Iteration:  52%|█████▏    | 80/154 [00:27<00:25,  2.86it/s]\u001b[A\n",
      "Iteration:  53%|█████▎    | 81/154 [00:28<00:25,  2.86it/s]\u001b[A\n",
      "Iteration:  53%|█████▎    | 82/154 [00:28<00:25,  2.87it/s]\u001b[A\n",
      "Iteration:  54%|█████▍    | 83/154 [00:28<00:24,  2.86it/s]\u001b[A\n",
      "Iteration:  55%|█████▍    | 84/154 [00:29<00:24,  2.86it/s]\u001b[A\n",
      "Iteration:  55%|█████▌    | 85/154 [00:29<00:24,  2.87it/s]\u001b[A\n",
      "Iteration:  56%|█████▌    | 86/154 [00:30<00:23,  2.87it/s]\u001b[A\n",
      "Iteration:  56%|█████▋    | 87/154 [00:30<00:23,  2.87it/s]\u001b[A\n",
      "Iteration:  57%|█████▋    | 88/154 [00:30<00:22,  2.87it/s]\u001b[A\n",
      "Iteration:  58%|█████▊    | 89/154 [00:31<00:22,  2.87it/s]\u001b[A\n",
      "Iteration:  58%|█████▊    | 90/154 [00:31<00:22,  2.87it/s]\u001b[A\n",
      "Iteration:  59%|█████▉    | 91/154 [00:31<00:21,  2.87it/s]\u001b[A\n",
      "Iteration:  60%|█████▉    | 92/154 [00:32<00:21,  2.87it/s]\u001b[A\n",
      "Iteration:  60%|██████    | 93/154 [00:32<00:21,  2.87it/s]\u001b[A\n",
      "Iteration:  61%|██████    | 94/154 [00:32<00:20,  2.87it/s]\u001b[A\n",
      "Iteration:  62%|██████▏   | 95/154 [00:33<00:20,  2.87it/s]\u001b[A\n",
      "Iteration:  62%|██████▏   | 96/154 [00:33<00:20,  2.87it/s]\u001b[A\n",
      "Iteration:  63%|██████▎   | 97/154 [00:33<00:19,  2.86it/s]\u001b[A\n",
      "Iteration:  64%|██████▎   | 98/154 [00:34<00:19,  2.87it/s]\u001b[A\n",
      "Iteration:  64%|██████▍   | 99/154 [00:34<00:19,  2.87it/s]\u001b[A03/05/2020 01:30:37 - INFO - transformers.configuration_utils -   Configuration saved in ../../../../model_save/Dos-Fases-all_Harassment/checkpoint-100/config.json\n",
      "03/05/2020 01:30:40 - INFO - transformers.modeling_utils -   Model weights saved in ../../../../model_save/Dos-Fases-all_Harassment/checkpoint-100/pytorch_model.bin\n",
      "03/05/2020 01:30:40 - INFO - __main__ -   Saving model checkpoint to ../../../../model_save/Dos-Fases-all_Harassment/checkpoint-100\n",
      "03/05/2020 01:30:47 - INFO - __main__ -   Saving optimizer and scheduler states to ../../../../model_save/Dos-Fases-all_Harassment/checkpoint-100\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:  65%|██████▍   | 100/154 [00:44<02:50,  3.15s/it]\u001b[A\n",
      "Iteration:  66%|██████▌   | 101/154 [00:44<02:02,  2.32s/it]\u001b[A\n",
      "Iteration:  66%|██████▌   | 102/154 [00:44<01:29,  1.73s/it]\u001b[A\n",
      "Iteration:  67%|██████▋   | 103/154 [00:45<01:06,  1.31s/it]\u001b[A\n",
      "Iteration:  68%|██████▊   | 104/154 [00:45<00:51,  1.02s/it]\u001b[A\n",
      "Iteration:  68%|██████▊   | 105/154 [00:46<00:40,  1.22it/s]\u001b[A\n",
      "Iteration:  69%|██████▉   | 106/154 [00:46<00:32,  1.47it/s]\u001b[A\n",
      "Iteration:  69%|██████▉   | 107/154 [00:46<00:27,  1.72it/s]\u001b[A\n",
      "Iteration:  70%|███████   | 108/154 [00:47<00:23,  1.96it/s]\u001b[A\n",
      "Iteration:  71%|███████   | 109/154 [00:47<00:20,  2.17it/s]\u001b[A\n",
      "Iteration:  71%|███████▏  | 110/154 [00:47<00:18,  2.34it/s]\u001b[A\n",
      "Iteration:  72%|███████▏  | 111/154 [00:48<00:17,  2.48it/s]\u001b[A\n",
      "Iteration:  73%|███████▎  | 112/154 [00:48<00:16,  2.59it/s]\u001b[A\n",
      "Iteration:  73%|███████▎  | 113/154 [00:48<00:15,  2.67it/s]\u001b[A\n",
      "Iteration:  74%|███████▍  | 114/154 [00:49<00:14,  2.73it/s]\u001b[A\n",
      "Iteration:  75%|███████▍  | 115/154 [00:49<00:14,  2.77it/s]\u001b[A\n",
      "Iteration:  75%|███████▌  | 116/154 [00:49<00:13,  2.81it/s]\u001b[A\n",
      "Iteration:  76%|███████▌  | 117/154 [00:50<00:13,  2.82it/s]\u001b[A\n",
      "Iteration:  77%|███████▋  | 118/154 [00:50<00:12,  2.84it/s]\u001b[A\n",
      "Iteration:  77%|███████▋  | 119/154 [00:50<00:12,  2.85it/s]\u001b[A\n",
      "Iteration:  78%|███████▊  | 120/154 [00:51<00:11,  2.84it/s]\u001b[A\n",
      "Iteration:  79%|███████▊  | 121/154 [00:51<00:11,  2.85it/s]\u001b[A\n",
      "Iteration:  79%|███████▉  | 122/154 [00:51<00:11,  2.85it/s]\u001b[A\n",
      "Iteration:  80%|███████▉  | 123/154 [00:52<00:10,  2.86it/s]\u001b[A\n",
      "Iteration:  81%|████████  | 124/154 [00:52<00:10,  2.87it/s]\u001b[A\n",
      "Iteration:  81%|████████  | 125/154 [00:52<00:10,  2.87it/s]\u001b[A\n",
      "Iteration:  82%|████████▏ | 126/154 [00:53<00:09,  2.86it/s]\u001b[A\n",
      "Iteration:  82%|████████▏ | 127/154 [00:53<00:09,  2.86it/s]\u001b[A\n",
      "Iteration:  83%|████████▎ | 128/154 [00:54<00:09,  2.86it/s]\u001b[A\n",
      "Iteration:  84%|████████▍ | 129/154 [00:54<00:08,  2.86it/s]\u001b[A\n",
      "Iteration:  84%|████████▍ | 130/154 [00:54<00:08,  2.86it/s]\u001b[A\n",
      "Iteration:  85%|████████▌ | 131/154 [00:55<00:08,  2.86it/s]\u001b[A\n",
      "Iteration:  86%|████████▌ | 132/154 [00:55<00:07,  2.86it/s]\u001b[A\n",
      "Iteration:  86%|████████▋ | 133/154 [00:55<00:07,  2.87it/s]\u001b[A\n",
      "Iteration:  87%|████████▋ | 134/154 [00:56<00:06,  2.87it/s]\u001b[A\n",
      "Iteration:  88%|████████▊ | 135/154 [00:56<00:06,  2.86it/s]\u001b[A\n",
      "Iteration:  88%|████████▊ | 136/154 [00:56<00:06,  2.86it/s]\u001b[A\n",
      "Iteration:  89%|████████▉ | 137/154 [00:57<00:05,  2.85it/s]\u001b[A\n",
      "Iteration:  90%|████████▉ | 138/154 [00:57<00:05,  2.86it/s]\u001b[A\n",
      "Iteration:  90%|█████████ | 139/154 [00:57<00:05,  2.86it/s]\u001b[A\n",
      "Iteration:  91%|█████████ | 140/154 [00:58<00:04,  2.86it/s]\u001b[A\n",
      "Iteration:  92%|█████████▏| 141/154 [00:58<00:04,  2.86it/s]\u001b[A\n",
      "Iteration:  92%|█████████▏| 142/154 [00:58<00:04,  2.85it/s]\u001b[A\n",
      "Iteration:  93%|█████████▎| 143/154 [00:59<00:03,  2.86it/s]\u001b[A\n",
      "Iteration:  94%|█████████▎| 144/154 [00:59<00:03,  2.85it/s]\u001b[A\n",
      "Iteration:  94%|█████████▍| 145/154 [00:59<00:03,  2.86it/s]\u001b[A\n",
      "Iteration:  95%|█████████▍| 146/154 [01:00<00:02,  2.86it/s]\u001b[A\n",
      "Iteration:  95%|█████████▌| 147/154 [01:00<00:02,  2.87it/s]\u001b[A\n",
      "Iteration:  96%|█████████▌| 148/154 [01:01<00:02,  2.86it/s]\u001b[A\n",
      "Iteration:  97%|█████████▋| 149/154 [01:01<00:01,  2.86it/s]\u001b[A\n",
      "Iteration:  97%|█████████▋| 150/154 [01:01<00:01,  2.86it/s]\u001b[A\n",
      "Iteration:  98%|█████████▊| 151/154 [01:02<00:01,  2.85it/s]\u001b[A\n",
      "Iteration:  99%|█████████▊| 152/154 [01:02<00:00,  2.85it/s]\u001b[A\n",
      "Iteration:  99%|█████████▉| 153/154 [01:02<00:00,  2.84it/s]\u001b[A\n",
      "Iteration: 100%|██████████| 154/154 [01:03<00:00,  2.44it/s]\u001b[A\n",
      "Epoch: 100%|██████████| 1/1 [01:03<00:00, 63.02s/it]\n",
      "03/05/2020 01:31:06 - INFO - __main__ -    global_step = 154, average loss = 3.7646767574471314\n",
      "03/05/2020 01:31:06 - INFO - __main__ -   Saving model checkpoint to ../../../../model_save/Dos-Fases-all_Harassment/\n",
      "03/05/2020 01:31:06 - INFO - transformers.configuration_utils -   Configuration saved in ../../../../model_save/Dos-Fases-all_Harassment/config.json\n",
      "03/05/2020 01:31:08 - INFO - transformers.modeling_utils -   Model weights saved in ../../../../model_save/Dos-Fases-all_Harassment/pytorch_model.bin\n",
      "03/05/2020 01:31:08 - INFO - transformers.configuration_utils -   loading configuration file ../../../../model_save/Dos-Fases-all_Harassment/config.json\n",
      "03/05/2020 01:31:08 - INFO - transformers.configuration_utils -   Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": null,\n",
      "  \"do_sample\": false,\n",
      "  \"eos_token_ids\": null,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 4,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": null,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "03/05/2020 01:31:08 - INFO - transformers.modeling_utils -   loading weights file ../../../../model_save/Dos-Fases-all_Harassment/pytorch_model.bin\n",
      "03/05/2020 01:31:10 - INFO - transformers.tokenization_utils -   Model name '../../../../model_save/Dos-Fases-all_Harassment/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming '../../../../model_save/Dos-Fases-all_Harassment/' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "03/05/2020 01:31:10 - INFO - transformers.tokenization_utils -   Didn't find file ../../../../model_save/Dos-Fases-all_Harassment/added_tokens.json. We won't load it.\n",
      "03/05/2020 01:31:10 - INFO - transformers.tokenization_utils -   loading file ../../../../model_save/Dos-Fases-all_Harassment/vocab.txt\n",
      "03/05/2020 01:31:10 - INFO - transformers.tokenization_utils -   loading file None\n",
      "03/05/2020 01:31:10 - INFO - transformers.tokenization_utils -   loading file ../../../../model_save/Dos-Fases-all_Harassment/special_tokens_map.json\n",
      "03/05/2020 01:31:10 - INFO - transformers.tokenization_utils -   loading file ../../../../model_save/Dos-Fases-all_Harassment/tokenizer_config.json\n",
      "03/05/2020 01:31:10 - INFO - __main__ -   Evaluate the following checkpoints: ['../../../../model_save/Dos-Fases-all_Harassment/']\n",
      "03/05/2020 01:31:10 - INFO - transformers.configuration_utils -   loading configuration file ../../../../model_save/Dos-Fases-all_Harassment/config.json\n",
      "03/05/2020 01:31:10 - INFO - transformers.configuration_utils -   Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": null,\n",
      "  \"do_sample\": false,\n",
      "  \"eos_token_ids\": null,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 4,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": null,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/05/2020 01:31:10 - INFO - transformers.modeling_utils -   loading weights file ../../../../model_save/Dos-Fases-all_Harassment/pytorch_model.bin\n",
      "03/05/2020 01:31:13 - INFO - __main__ -   ***** Running evaluation  *****\n",
      "03/05/2020 01:31:13 - INFO - __main__ -     Num examples = 503\n",
      "03/05/2020 01:31:13 - INFO - __main__ -     Batch size = 16\n",
      "Evaluating: 100%|██████████| 32/32 [00:03<00:00, 10.58it/s]\n",
      "03/05/2020 01:31:16 - INFO - __main__ -   ***** Eval results  *****\n",
      "03/05/2020 01:31:16 - INFO - __main__ -     perplexity = tensor(288.7885)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity epoch: 288.78854\n",
      "\n",
      "Memory usage         :  3.80 MB\n",
      "GC collected objects : 844\n",
      "Memory usage         :  3.80 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/05/2020 01:31:16 - INFO - transformers.tokenization_utils -   Model name '../../../../model_save/Dos-Fases-all_Harassment/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming '../../../../model_save/Dos-Fases-all_Harassment/' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "03/05/2020 01:31:16 - INFO - transformers.tokenization_utils -   Didn't find file ../../../../model_save/Dos-Fases-all_Harassment/added_tokens.json. We won't load it.\n",
      "03/05/2020 01:31:16 - INFO - transformers.tokenization_utils -   loading file ../../../../model_save/Dos-Fases-all_Harassment/vocab.txt\n",
      "03/05/2020 01:31:16 - INFO - transformers.tokenization_utils -   loading file None\n",
      "03/05/2020 01:31:16 - INFO - transformers.tokenization_utils -   loading file ../../../../model_save/Dos-Fases-all_Harassment/special_tokens_map.json\n",
      "03/05/2020 01:31:16 - INFO - transformers.tokenization_utils -   loading file ../../../../model_save/Dos-Fases-all_Harassment/tokenizer_config.json\n",
      "03/05/2020 01:31:16 - INFO - transformers.configuration_utils -   loading configuration file ../../../../model_save/Dos-Fases-all_Harassment/config.json\n",
      "03/05/2020 01:31:16 - INFO - transformers.configuration_utils -   Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": null,\n",
      "  \"do_sample\": false,\n",
      "  \"eos_token_ids\": null,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 4,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": null,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "03/05/2020 01:31:16 - INFO - transformers.modeling_utils -   loading weights file ../../../../model_save/Dos-Fases-all_Harassment/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GC collected objects : 0\n",
      "Memory usage         :  3.80 MB\n",
      "Loading BERT tokenizer...\n",
      "Loading BERT Seq Class from  ../../../../model_save/Dos-Fases-all_Harassment/ ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/05/2020 01:31:18 - INFO - transformers.modeling_utils -   Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n",
      "03/05/2020 01:31:18 - INFO - transformers.modeling_utils -   Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo cargado correctamente desde  ../../../../model_save/Dos-Fases-all_Harassment/\n",
      "\n",
      "Padding/truncating all sentences to 50 values...\n",
      "\n",
      "Padding token: \"[PAD]\", ID: 0\n",
      "Completado.\n",
      "\n",
      "Padding/truncating all sentences to 50 values...\n",
      "\n",
      "Padding token: \"[PAD]\", ID: 0\n",
      "Completado.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/05/2020 01:33:00 - INFO - transformers.configuration_utils -   Configuration saved in ../../../../model_save/Dos-Fases-all_Harassment/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating F-macro...\n",
      "F macro: 0.417\n",
      "F macro none average:  [0.9160168  0.         0.         0.75277778]\n",
      "Accuracy: 0.847\n",
      "Saving model to ../../../../model_save/Dos-Fases-all_Harassment/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/05/2020 01:33:02 - INFO - transformers.modeling_utils -   Model weights saved in ../../../../model_save/Dos-Fases-all_Harassment/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage         :  3.80 MB\n",
      "GC collected objects : 427\n",
      "Memory usage         :  3.80 MB\n",
      "GC collected objects : 0\n",
      "Memory usage         :  3.80 MB\n"
     ]
    }
   ],
   "source": [
    "Num_epochs= 5\n",
    "out= '../../../../model_save/Dos-Fases-all_Harassment/'\n",
    "from_pre = True\n",
    "\n",
    "f= open(out+\"/Summary_macro.txt\",\"w\")\n",
    "f.write(\"This is a summary of the running \\n\")\n",
    "f.close()\n",
    "\n",
    "while epoch_macro <= Num_epochs:\n",
    "    ####################################################\n",
    "    ################## LANGUAGE MODEL ##################\n",
    "    f=open(out+\"/Summary_macro.txt\", \"a+\")\n",
    "    f.write(\"------------------------------------\\n\")\n",
    "    f.write(\"Running Language Model epoch: \"+str(epoch_macro)+'\\n')\n",
    "    resultado_lm = fase_LM(from_pre, my_args, logger)\n",
    "    print (\"Perplexity epoch: \"+str(list(resultado_lm.values())[0].cpu().numpy())+'\\n')\n",
    "    f.write(\"Perplexity epoch: \"+str(list(resultado_lm.values())[0].cpu().numpy())+'\\n')\n",
    "    f.close()\n",
    "    ####################################################\n",
    "    ################### Free memory ####################\n",
    "    mem()\n",
    "    print('GC collected objects : %d' % gc.collect())\n",
    "    mem()\n",
    "    \n",
    "    gc.collect()    \n",
    "    \n",
    "    print('GC collected objects : %d' % gc.collect())\n",
    "    mem()\n",
    "    ####################################################\n",
    "    ################### SEQUENCE CLASS #################\n",
    "    f=open(out+\"/Summary_macro.txt\", \"a+\")\n",
    "    f.write(\"------------------------------------\\n\")\n",
    "    f.write(\"Running Sentence Classification epoch: \"+str(epoch_macro)+'\\n')\n",
    "    \n",
    "    from_pre = False\n",
    "    \n",
    "    #qscd\n",
    "    d_lab=dict()\n",
    "    d_lab['NonH']=0\n",
    "    d_lab['IndirectH']=1\n",
    "    d_lab['PhysicalH']=2\n",
    "    d_lab['SexualH']=3\n",
    "      \n",
    "    max_len = 50\n",
    "    batch = 16 \n",
    "    fma, fno, acc = fase_SC(from_pre, batch, d_lab, max_len)\n",
    "    f.write(\"F macro: \"+str(fma)+'\\n')\n",
    "    f.write(\"F macro none avergage: \"+str(fno)+'\\n')\n",
    "    f.write(\"Accuracy: \"+str(acc)+'\\n')\n",
    "    f.write(\"------------------------------------\\n\")\n",
    "    f.close()\n",
    "    \n",
    "    ####################################################\n",
    "    ################### Free memory ####################\n",
    "    mem()\n",
    "    print('GC collected objects : %d' % gc.collect())\n",
    "    mem()\n",
    "    \n",
    "    gc.collect()    \n",
    "    \n",
    "    print('GC collected objects : %d' % gc.collect())\n",
    "    mem()\n",
    "    \n",
    "    epoch_macro += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:newpy3]",
   "language": "python",
   "name": "conda-env-newpy3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
