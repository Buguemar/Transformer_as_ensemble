{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/casapanshop/anaconda2/envs/newpy3_tf1/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/casapanshop/anaconda2/envs/newpy3_tf1/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/casapanshop/anaconda2/envs/newpy3_tf1/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/casapanshop/anaconda2/envs/newpy3_tf1/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/casapanshop/anaconda2/envs/newpy3_tf1/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/casapanshop/anaconda2/envs/newpy3_tf1/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/casapanshop/anaconda2/envs/newpy3_tf1/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/casapanshop/anaconda2/envs/newpy3_tf1/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/casapanshop/anaconda2/envs/newpy3_tf1/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/casapanshop/anaconda2/envs/newpy3_tf1/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/casapanshop/anaconda2/envs/newpy3_tf1/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/casapanshop/anaconda2/envs/newpy3_tf1/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: GeForce GTX 1060 6GB\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import pandas as pd\n",
    "import time\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
    "\n",
    "import re, io, nltk, torch \n",
    "from nltk.corpus import stopwords\n",
    "from numpy import linalg as LA\n",
    "from numpy.linalg import norm\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm, trange\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from gensim import utils, matutils  \n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from six import string_types, integer_types\n",
    "from six.moves import zip, range\n",
    "from numpy import linalg as LA\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from scipy import stats\n",
    "from gensim.utils import deprecated\n",
    "from numpy import dot, float32 as REAL, memmap as np_memmap, \\\n",
    "    double, array, zeros, vstack, sqrt, newaxis, integer, \\\n",
    "    ndarray, sum as np_sum, prod, argmax\n",
    "from collections import Counter\n",
    "from scipy.spatial import distance\n",
    "from numpy.random import binomial\n",
    "from scipy.stats import bernoulli\n",
    "import numpy as np\n",
    "\n",
    "import keras\n",
    "from keras.callbacks import Callback,ModelCheckpoint, ReduceLROnPlateau    \n",
    "from keras import backend as K\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, BatchNormalization\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Conv1D, Conv2D, MaxPooling1D, Input\n",
    "from keras.models import Model\n",
    "from keras.layers import LSTM,GRU,Dense\n",
    "from keras.utils import Sequence,to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import transformers\n",
    "from transformers import (\n",
    "    WEIGHTS_NAME,\n",
    "    AdamW,\n",
    "    BertConfig,\n",
    "    BertForMaskedLM,\n",
    "    BertForSequenceClassification,\n",
    "    BertTokenizer,\n",
    "    CamembertConfig,\n",
    "    CamembertForMaskedLM,\n",
    "    CamembertTokenizer,\n",
    "    DistilBertConfig,\n",
    "    DistilBertForMaskedLM,\n",
    "    DistilBertTokenizer,\n",
    "    GPT2Config,\n",
    "    GPT2LMHeadModel,\n",
    "    GPT2Tokenizer,\n",
    "    OpenAIGPTConfig,\n",
    "    OpenAIGPTLMHeadModel,\n",
    "    OpenAIGPTTokenizer,\n",
    "    RobertaConfig,\n",
    "    RobertaForMaskedLM,\n",
    "    RobertaTokenizer,\n",
    "    get_linear_schedule_with_warmup,\n",
    ")\n",
    "\n",
    "n_gpu = torch.cuda.device_count()\n",
    "torch.cuda.get_device_name(0)\n",
    "\n",
    "if torch.cuda.is_available():     \n",
    "    device = torch.device(\"cuda\")\n",
    "    print('There are',torch.cuda.device_count(),'GPU(s) available.')\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def my_normalize(v):\n",
    "    norm = np.linalg.norm(v)\n",
    "    if norm == 0: \n",
    "        return v\n",
    "    return v/norm\n",
    "\n",
    "#el peso max es igual a la clase mas grande\n",
    "def det_samples(df, dict_class, c_weights=None):\n",
    "    conteos= Counter(df)\n",
    "    aumentos= dict()\n",
    "    maxi= max(conteos.values())\n",
    "    for clase in conteos:\n",
    "        if conteos[clase]==maxi:\n",
    "            class_maxi=clase\n",
    "            \n",
    "    if c_weights is None:        # uniform class weights\n",
    "        for tipo in conteos:\n",
    "            actual= conteos[tipo]\n",
    "            if actual<maxi:\n",
    "                aumentos[tipo]=int(maxi-actual)\n",
    "            else:\n",
    "                aumentos[tipo]=0\n",
    "        return aumentos   \n",
    "    else:\n",
    "        try:\n",
    "            weigths=my_normalize(c_weights)\n",
    "            max_wei=max(weigths)\n",
    "            class_max= list(weigths).index(max_wei)\n",
    "            str_class=dict_class[class_max]\n",
    "            samples_max=conteos[str_class]\n",
    "            \n",
    "            clase=0\n",
    "            for peso in weigths:\n",
    "                k=dict_class[clase]\n",
    "                if peso == max_wei:\n",
    "                    aumentos[k]= int(samples_max-conteos[k])\n",
    "                else:\n",
    "                    futuro=(samples_max/max_wei)*peso\n",
    "                    aumentos[k]=int(futuro-conteos[k])\n",
    "                clase+=1\n",
    "            return aumentos\n",
    "        except:\n",
    "            print (\"No se ha especificado el vector de pesos par balance de datos\")\n",
    "            \n",
    "            \n",
    "stoplist = stopwords.words(\"english\")\n",
    "\n",
    "emoji_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "def my_cleaner(post):      \n",
    "    s= re.sub(r\"http\\S+\", \"html\", post)      \n",
    "    s= re.sub(r'#\\w+ ?', \"hashtag\", s) \n",
    "    s= re.sub(r'@\\w+ ?', \"user\", s)\n",
    "    s= s.lower()    \n",
    "    s=emoji_pattern.sub(r'', s)\n",
    "    sl= list(s.split())  \n",
    "    sl=[word for word in sl if word not in stoplist]\n",
    "    s=' '.join([word for word in s.split() if word not in stoplist])\n",
    "    return s, sl\n",
    "\n",
    "def Sort_Tuple(tup):  \n",
    "    return(sorted(tup, key = lambda x: x[1]))   \n",
    "\n",
    "\n",
    "def predict_data(trained, x_train, x_val, x_test, etiq, etiq_v, etiq_t, name_model):\n",
    "    etiq = etiq.astype(\"int\")\n",
    "    trainPredict = trained.predict(x_train, batch_size=32)\n",
    "    trainPredict=[np.argmax(pred) for pred in trainPredict]\n",
    "    acc= accuracy_score(etiq, trainPredict)  \n",
    "    f1=f1_score(etiq, trainPredict, average=None)  # labels=np.unique(trainPredict)\n",
    "    f1_ma=f1_score(etiq, trainPredict, average='macro')  # labels=np.unique(trainPredict)\n",
    "    print (\"\")\n",
    "    print (\"Accuracy sobre Train\", name_model, \":\",acc)  \n",
    "    print (\"F1-score None sobre Train\", name_model, \":\",f1)\n",
    "    print (\"F1-score macro sobre Train\", name_model, \":\",f1_ma)\n",
    "    ########################################\n",
    "    trainPredict = trained.predict(x_val, batch_size=32)\n",
    "    trainPredict=[np.argmax(pred) for pred in trainPredict]\n",
    "    acc= accuracy_score(etiq_v, trainPredict)  \n",
    "    f1=f1_score(etiq_v, trainPredict, average=None)  # labels=np.unique(trainPredict)\n",
    "    f1_ma=f1_score(etiq_v, trainPredict, average='macro')  # labels=np.unique(trainPredict)\n",
    "    print (\"\")\n",
    "    print (\"Accuracy sobre Val\", name_model, \":\",acc)  \n",
    "    print (\"F1-score None sobre Val\", name_model, \":\",f1)\n",
    "    print (\"F1-score macro sobre Val\", name_model, \":\",f1_ma)\n",
    "    ########################################\n",
    "    trainPredict = trained.predict(x_test, batch_size=32)\n",
    "    trainPredict=[np.argmax(pred) for pred in trainPredict]\n",
    "    acc_t= accuracy_score(etiq_t, trainPredict)  \n",
    "    f1_t=f1_score(etiq_t, trainPredict, average=None)  # labels=np.unique(trainPredict)\n",
    "    f1_ma_t=f1_score(etiq_t, trainPredict, average='macro')  # labels=np.unique(trainPredict)\n",
    "    matriz_t=normalize(confusion_matrix(etiq_t, trainPredict))\n",
    "    print (\"\")\n",
    "    print (\"Accuracy sobre Test\", name_model, \":\",acc_t)  \n",
    "    print (\"F1-score None sobre Test\", name_model, \":\",f1_t)\n",
    "    print (\"F1-score macro sobre Test\", name_model, \":\",f1_ma_t)\n",
    "    \n",
    "    return f1_ma_t, f1_t, acc_t, matriz_t\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conteo por harassment val\n",
      " Counter({'NonH': 1493, 'SexualH': 525, 'IndirectH': 71, 'PhysicalH': 36})\n",
      "Conteo por harassment test\n",
      " Counter({'NonH': 1601, 'SexualH': 340, 'IndirectH': 106, 'PhysicalH': 76})\n"
     ]
    }
   ],
   "source": [
    "stoplist = stopwords.words(\"english\")\n",
    "\n",
    "emoji_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "def my_cleaner(post):      \n",
    "    s= re.sub(r\"http\\S+\", \"html\", post)   ##########elimina http    \n",
    "    s= re.sub(r'#\\w+ ?', \"hashtag\", s) ########3\n",
    "    s= re.sub(r'@\\w+ ?', \"user\", s)##############    \n",
    "    s= s.lower()    \n",
    "    s=emoji_pattern.sub(r'', s)\n",
    "    sl= list(s.split())  \n",
    "    sl=[word for word in sl if word not in stoplist]\n",
    "    s=' '.join([word for word in s.split() if word not in stoplist])\n",
    "    return s, sl\n",
    "\n",
    "#train =pd.read_csv(\"../Fine-Tuning/CSV_Stance/train_semeval_raw.csv\")\n",
    "val=pd.read_csv(\"../Fine-Tuning/CSV_Harassment/val_format.csv\")\n",
    "test=pd.read_csv(\"../Fine-Tuning/CSV_Harassment/test_format.csv\")\n",
    "\n",
    "print (\"Conteo por harassment val\\n\", Counter(val['Label']))\n",
    "print (\"Conteo por harassment test\\n\", Counter(test['Label']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2123, 50, 768)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_LEN = 50\n",
    "\n",
    "dir_path='../../model_save/Dos-Fases-all_Harassment/'\n",
    "tokenizer_loaded = BertTokenizer.from_pretrained(dir_path)#'bert-base-uncased')\n",
    "model_loaded = BertForSequenceClassification.from_pretrained(dir_path, num_labels=4)#'bert-base-uncased',  num_labels=4) \n",
    "idx_2_token = tokenizer_loaded.ids_to_tokens\n",
    "archivo = open(dir_path+'vocab.txt','r')\n",
    "Word2Index={word.strip():i for i,word in enumerate(archivo.readlines())}\n",
    "M_BERT_space = model_loaded.bert.embeddings.word_embeddings.weight.detach().cpu().numpy()\n",
    "transformer = Normalizer().fit(M_BERT_space) \n",
    "M_BERT_space=transformer.transform(M_BERT_space)\n",
    "\n",
    "dict_cl=dict()\n",
    "dict_cl[0]=\"NonH\"\n",
    "dict_cl[1]=\"IndirectH\"\n",
    "dict_cl[2]=\"PhysicalH\"\n",
    "dict_cl[3]=\"SexualH\"\n",
    "\n",
    "#nips\n",
    "d_lab=dict()\n",
    "d_lab[\"NonH\"]=0\n",
    "d_lab[\"IndirectH\"]=1\n",
    "d_lab[\"PhysicalH\"]=2\n",
    "d_lab[\"SexualH\"]=3\n",
    "\n",
    "etiq_v=[]\n",
    "for et in val['Label'].values:\n",
    "    etiq_v.append(d_lab[et])\n",
    "n_labels_val = np.array(etiq_v)\n",
    "y_val=to_categorical(n_labels_val,num_classes=4)\n",
    "sentences_val = val['Tweet'].values\n",
    "input_ids_val = []\n",
    "for sent in sentences_val:\n",
    "    encoded_sent = tokenizer_loaded.encode(my_cleaner(sent)[0],add_special_tokens = False)    \n",
    "    input_ids_val.append(encoded_sent)\n",
    "input_ids_val = pad_sequences(input_ids_val, maxlen=MAX_LEN, dtype=\"long\", \n",
    "                          value=0, truncating=\"post\", padding=\"post\")\n",
    "\n",
    "etiq_t=[]\n",
    "for et in test['Label'].values:\n",
    "    etiq_t.append(d_lab[et])\n",
    "n_labels_test = np.array(etiq_t)\n",
    "y_test=to_categorical(n_labels_test,num_classes=4)\n",
    "sentences_test = test['Tweet'].values\n",
    "input_ids_test = []\n",
    "for sent in sentences_test:\n",
    "    encoded_sent = tokenizer_loaded.encode(my_cleaner(sent)[0],add_special_tokens = False)    \n",
    "    input_ids_test.append(encoded_sent)\n",
    "input_ids_test = pad_sequences(input_ids_test, maxlen=MAX_LEN, dtype=\"long\", \n",
    "                          value=0, truncating=\"post\", padding=\"post\")\n",
    "\n",
    "\n",
    "shape_val=np.asarray(input_ids_val).shape\n",
    "x_val=np.zeros((shape_val[0], shape_val[1], 768))\n",
    "i=0\n",
    "for in_id in input_ids_val:\n",
    "    x_val[i]=M_BERT_space[in_id]\n",
    "    i+=1\n",
    "    \n",
    "shape_test=np.asarray(input_ids_test).shape\n",
    "x_test=np.zeros((shape_test[0], shape_test[1], 768))\n",
    "i=0\n",
    "for in_id in input_ids_test:\n",
    "    x_test[i]=M_BERT_space[in_id]\n",
    "    i+=1\n",
    "\n",
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2123, 50, 768)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def focal_loss(gamma=2., weights=1):   #weights np.asarray()\n",
    "    weights= K.variable(weights)\n",
    "    def focal_loss_fixed(y_true, y_pred):\n",
    "        y_true = K.clip(y_true, K.epsilon(),1)\n",
    "        y_pred = K.clip(y_pred,K.epsilon(),1)\n",
    "        return - K.sum(weights* K.pow(1. - y_pred, gamma)* y_true * K.log(y_pred), axis=-1) \n",
    "    return focal_loss_fixed\n",
    "\n",
    "class Metrics(Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.val_f1s = []\n",
    "\n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        prob = self.model.predict(self.validation_data[0])\n",
    "        predict = np.squeeze(prob>=0.5)*1\n",
    "        targ = np.squeeze(self.validation_data[1])\n",
    "        f1s = f1_score(targ, predict, average='macro')\n",
    "        self.val_f1s.append(f1s)\n",
    "        #print(\" - val_f1: %f \" %(f1s))\n",
    "        return\n",
    "\n",
    "def plot_confusion_matrix(cm, target_names, title='Confusion matrix (f1-score)',cmap=None, normalize=True):\n",
    "    \n",
    "    import itertools\n",
    "    accuracy = np.trace(cm) / float(np.sum(cm))\n",
    "    misclass = 1 - accuracy\n",
    "    if cmap is None:\n",
    "        cmap = plt.get_cmap('Blues')\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "\n",
    "    if target_names is not None:\n",
    "        tick_marks = np.arange(len(target_names))\n",
    "        plt.xticks(tick_marks, target_names, rotation=45)\n",
    "        plt.yticks(tick_marks, target_names)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    thresh = cm.max() / 1.5 if normalize else cm.max() / 2\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        if normalize:\n",
    "            plt.text(j, i, \"{:0.4f}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "        else:\n",
    "            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))\n",
    "    plt.show()\n",
    "    \n",
    "def predecir_modelos(bs,predichos_all):  #best fit\n",
    "    temp=predichos_all.copy()\n",
    "    final=[np.argmax(pred) for pred in temp]  \n",
    "    confianzas=[temp[i][final[i]] for i in range(len(final))]\n",
    "    predichos_new=[]\n",
    "    for i in range(len(temp)):\n",
    "        indice=final[i]%4\n",
    "        predichos_new.append(int(indice))\n",
    "    return predichos_new,final,confianzas    \n",
    "\n",
    "def predecir_modelos_norm(bs,predichos_all):  #normalizado\n",
    "    temp=predichos_all.copy()\n",
    "    final=[]\n",
    "    confianzas=[]\n",
    "    for pred in temp:   \n",
    "        aux=np.ones(4)\n",
    "        for i in range(1,6): #for machine pred\n",
    "            sub=pred[(4*(i-1)):(4*i)]\n",
    "            aux=aux*np.asarray(sub)\n",
    "            \n",
    "        aux=aux/np.sum(aux)\n",
    "        final.append(np.argmax(aux))\n",
    "        confianzas.append(aux[np.argmax(aux)])  \n",
    "           \n",
    "    predichos_new=[]\n",
    "    for i in range(len(temp)):\n",
    "        predichos_new.append(final[i])\n",
    "    return predichos_new,final,confianzas\n",
    "    \n",
    "def predecir_modelos_average(bs,predichos_all): #average\n",
    "    temp=predichos_all.copy()\n",
    "    final=[]\n",
    "    confianzas=[]\n",
    "    for pred in temp:\n",
    "        aux=np.zeros(4)\n",
    "        for i in range(1,6):\n",
    "            sub=pred[(4*(i-1)):(4*i)]\n",
    "            aux=aux+np.asarray(sub)\n",
    "            \n",
    "        aux=aux/5.0 #dividido en el total de machines\n",
    "        final.append(np.argmax(aux))\n",
    "        confianzas.append(aux[np.argmax(aux)])  \n",
    "           \n",
    "    predichos_new=[]\n",
    "    for i in range(len(temp)):\n",
    "        predichos_new.append(final[i])\n",
    "    return predichos_new,final,confianzas\n",
    "\n",
    "\n",
    "def predict_data(trained, x_val, x_test, etiq_v, etiq_t, name_model):\n",
    "    trainPredict = trained.predict(x_val, batch_size=32)\n",
    "    trainPredict=[np.argmax(pred) for pred in trainPredict]\n",
    "    acc= accuracy_score(etiq_v, trainPredict)  \n",
    "    f1=f1_score(etiq_v, trainPredict, average=None)  # labels=np.unique(trainPredict)\n",
    "    f1_ma=f1_score(etiq_v, trainPredict, average='macro')  # labels=np.unique(trainPredict)\n",
    "    #matriz=normalize(confusion_matrix(etiq_v, trainPredict))\n",
    "    print (\"\")\n",
    "    print (\"Accuracy sobre Val\", name_model, \":\",acc)  \n",
    "    print (\"F1-score None sobre Val\", name_model, \":\",f1)\n",
    "    print (\"F1-score macro sobre Val\", name_model, \":\",f1_ma)\n",
    "    ########################################\n",
    "    trainPredict = trained.predict(x_test, batch_size=32)\n",
    "    trainPredict=[np.argmax(pred) for pred in trainPredict]\n",
    "    acc_t= accuracy_score(etiq_t, trainPredict)  \n",
    "    f1_t=f1_score(etiq_t, trainPredict, average=None)  # labels=np.unique(trainPredict)\n",
    "    f1_ma_t=f1_score(etiq_t, trainPredict, average='macro')  # labels=np.unique(trainPredict)\n",
    "    matriz_t=normalize(confusion_matrix(etiq_t, trainPredict))\n",
    "    print (\"\")\n",
    "    print (\"Accuracy sobre Test\", name_model, \":\",acc_t)  \n",
    "    print (\"F1-score None sobre Test\", name_model, \":\",f1_t)\n",
    "    print (\"F1-score macro sobre Test\", name_model, \":\",f1_ma_t)\n",
    "    \n",
    "    return f1_ma_t, f1_t, acc_t #, matriz_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2123, 50, 768)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baselines masking=0.0 Sin class weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------- EJECUCIÓN #1----------------\n",
      "WARNING:tensorflow:Large dropout rate: 0.65 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.65 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:From /home/casapanshop/anaconda2/envs/newpy3_tf1/lib/python3.6/site-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/casapanshop/anaconda2/envs/newpy3_tf1/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1205: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:Large dropout rate: 0.65 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "Agregando predicciones Val set\n",
      "Agregando predicciones Test set rnn3\n",
      "--------VALIDATION SET--------\n",
      "\n",
      "Committee Best Fit\n",
      "Accuracy: 0.8847058823529412\n",
      "F1-macro: 0.45163846280059516\n",
      "F1-score SDQC: 0.45163846280059516\n",
      "\n",
      "Committee Norm\n",
      "Accuracy: 0.8927058823529411\n",
      "F1-macro: 0.4750348869663363\n",
      "F1-score SDQC: 0.4750348869663363\n",
      "\n",
      "Committee Voting\n",
      "Accuracy: 0.8931764705882353\n",
      "F1-macro: 0.47511232555063826\n",
      "F1-score SDQC: 0.47511232555063826\n",
      "--------TESTING SET--------\n",
      "\n",
      "Committee Best Fit\n",
      "Accuracy: 0.8398492699010833\n",
      "F1-macro: 0.41302354448906176\n",
      "F1-score SDQC: 0.41302354448906176\n",
      "\n",
      "Committee Norm\n",
      "Accuracy: 0.8450306170513424\n",
      "F1-macro: 0.4108621067231185\n",
      "F1-score SDQC: 0.4108621067231185\n",
      "\n",
      "Committee Voting\n",
      "Accuracy: 0.8469147432878003\n",
      "F1-macro: 0.4130111197088888\n",
      "F1-score SDQC: 0.4130111197088888\n",
      "\n",
      "---------------- EJECUCIÓN #2----------------\n",
      "Agregando predicciones Val set\n",
      "Agregando predicciones Test set rnn3\n",
      "--------VALIDATION SET--------\n",
      "\n",
      "Committee Best Fit\n",
      "Accuracy: 0.8771764705882353\n",
      "F1-macro: 0.4576757565170825\n",
      "F1-score SDQC: 0.4576757565170825\n",
      "\n",
      "Committee Norm\n",
      "Accuracy: 0.8875294117647059\n",
      "F1-macro: 0.46938178746012504\n",
      "F1-score SDQC: 0.46938178746012504\n",
      "\n",
      "Committee Voting\n",
      "Accuracy: 0.8875294117647059\n",
      "F1-macro: 0.4577768745833103\n",
      "F1-score SDQC: 0.4577768745833103\n",
      "--------TESTING SET--------\n",
      "\n",
      "Committee Best Fit\n",
      "Accuracy: 0.8440885539331136\n",
      "F1-macro: 0.4307012455023691\n",
      "F1-score SDQC: 0.4307012455023691\n",
      "\n",
      "Committee Norm\n",
      "Accuracy: 0.8497409326424871\n",
      "F1-macro: 0.419883795906703\n",
      "F1-score SDQC: 0.419883795906703\n",
      "\n",
      "Committee Voting\n",
      "Accuracy: 0.8497409326424871\n",
      "F1-macro: 0.4233541317480271\n",
      "F1-score SDQC: 0.4233541317480271\n",
      "\n",
      "---------------- EJECUCIÓN #3----------------\n",
      "Agregando predicciones Val set\n",
      "Agregando predicciones Test set rnn3\n",
      "--------VALIDATION SET--------\n",
      "\n",
      "Committee Best Fit\n",
      "Accuracy: 0.8682352941176471\n",
      "F1-macro: 0.4558130568396015\n",
      "F1-score SDQC: 0.4558130568396015\n",
      "\n",
      "Committee Norm\n",
      "Accuracy: 0.8743529411764706\n",
      "F1-macro: 0.45926910751626704\n",
      "F1-score SDQC: 0.45926910751626704\n",
      "\n",
      "Committee Voting\n",
      "Accuracy: 0.8771764705882353\n",
      "F1-macro: 0.4497600449627316\n",
      "F1-score SDQC: 0.4497600449627316\n",
      "--------TESTING SET--------\n",
      "\n",
      "Committee Best Fit\n",
      "Accuracy: 0.8224211022138483\n",
      "F1-macro: 0.3759916346716164\n",
      "F1-score SDQC: 0.3759916346716164\n",
      "\n",
      "Committee Norm\n",
      "Accuracy: 0.8271314178049929\n",
      "F1-macro: 0.3814903626636269\n",
      "F1-score SDQC: 0.3814903626636269\n",
      "\n",
      "Committee Voting\n",
      "Accuracy: 0.833254828073481\n",
      "F1-macro: 0.39091959690591394\n",
      "F1-score SDQC: 0.39091959690591394\n",
      "\n",
      "---------------- EJECUCIÓN #4----------------\n",
      "Agregando predicciones Val set\n",
      "Agregando predicciones Test set rnn3\n",
      "--------VALIDATION SET--------\n",
      "\n",
      "Committee Best Fit\n",
      "Accuracy: 0.8832941176470588\n",
      "F1-macro: 0.45529726460243614\n",
      "F1-score SDQC: 0.45529726460243614\n",
      "\n",
      "Committee Norm\n",
      "Accuracy: 0.8847058823529412\n",
      "F1-macro: 0.4687017864280492\n",
      "F1-score SDQC: 0.4687017864280492\n",
      "\n",
      "Committee Voting\n",
      "Accuracy: 0.8889411764705882\n",
      "F1-macro: 0.47225942157377093\n",
      "F1-score SDQC: 0.47225942157377093\n",
      "--------TESTING SET--------\n",
      "\n",
      "Committee Best Fit\n",
      "Accuracy: 0.8417333961375412\n",
      "F1-macro: 0.41570520044150877\n",
      "F1-score SDQC: 0.41570520044150877\n",
      "\n",
      "Committee Norm\n",
      "Accuracy: 0.8450306170513424\n",
      "F1-macro: 0.4151392773214111\n",
      "F1-score SDQC: 0.4151392773214111\n",
      "\n",
      "Committee Voting\n",
      "Accuracy: 0.8459726801695714\n",
      "F1-macro: 0.41655664822434624\n",
      "F1-score SDQC: 0.41655664822434624\n",
      "\n",
      "---------------- EJECUCIÓN #5----------------\n",
      "Agregando predicciones Val set\n",
      "Agregando predicciones Test set rnn3\n",
      "--------VALIDATION SET--------\n",
      "\n",
      "Committee Best Fit\n",
      "Accuracy: 0.8503529411764705\n",
      "F1-macro: 0.4449751527893404\n",
      "F1-score SDQC: 0.4449751527893404\n",
      "\n",
      "Committee Norm\n",
      "Accuracy: 0.8588235294117647\n",
      "F1-macro: 0.44987055426516415\n",
      "F1-score SDQC: 0.44987055426516415\n",
      "\n",
      "Committee Voting\n",
      "Accuracy: 0.8710588235294118\n",
      "F1-macro: 0.46591298110465973\n",
      "F1-score SDQC: 0.46591298110465973\n",
      "--------TESTING SET--------\n",
      "\n",
      "Committee Best Fit\n",
      "Accuracy: 0.8195949128591615\n",
      "F1-macro: 0.3959704031474216\n",
      "F1-score SDQC: 0.3959704031474216\n",
      "\n",
      "Committee Norm\n",
      "Accuracy: 0.8210080075365049\n",
      "F1-macro: 0.39010915778485633\n",
      "F1-score SDQC: 0.39010915778485633\n",
      "\n",
      "Committee Voting\n",
      "Accuracy: 0.826189354686764\n",
      "F1-macro: 0.3967555619303468\n",
      "F1-score SDQC: 0.3967555619303468\n"
     ]
    }
   ],
   "source": [
    "prob=0.0\n",
    "path='../Operador Data Augmentation/exec_baselines_harass/'\n",
    "bs=32\n",
    "    \n",
    "fs_macro_val={'cnn1':[], 'cnn2':[], 'rnn1':[],'rnn2':[],'rnn3':[]}\n",
    "fs_none_val={'cnn1':[], 'cnn2':[], 'rnn1':[],'rnn2':[],'rnn3':[]}\n",
    "accs_val={'cnn1':[], 'cnn2':[], 'rnn1':[],'rnn2':[],'rnn3':[]}\n",
    "confusions_val={'cnn1':[], 'cnn2':[], 'rnn1':[],'rnn2':[],'rnn3':[]}\n",
    "\n",
    "fs_macro_test={'cnn1':[], 'cnn2':[], 'rnn1':[],'rnn2':[],'rnn3':[]}\n",
    "fs_none_test={'cnn1':[], 'cnn2':[], 'rnn1':[],'rnn2':[],'rnn3':[]}\n",
    "accs_test={'cnn1':[], 'cnn2':[], 'rnn1':[],'rnn2':[],'rnn3':[]}\n",
    "confusions_test={'cnn1':[], 'cnn2':[], 'rnn1':[],'rnn2':[],'rnn3':[]}\n",
    "\n",
    "best={'acc':[], 'none':[], 'macro':[]}\n",
    "norm={'acc':[], 'none':[], 'macro':[]}\n",
    "voting={'acc':[], 'none':[], 'macro':[]}\n",
    "best_test={'acc':[], 'none':[], 'macro':[]}\n",
    "norm_test={'acc':[], 'none':[], 'macro':[]}\n",
    "voting_test={'acc':[], 'none':[], 'macro':[]}\n",
    "\n",
    "for j in range(5):\n",
    "    print (\"\")\n",
    "    print (\"---------------- EJECUCIÓN #\"+str(j+1)+'----------------')\n",
    "    cnn1= load_model(path+str(prob)+'_wo/cnn1_'+str(j+1)+'-exec_w4_app1_Top_'+str(prob)+'_1.h5', custom_objects={'FocalLoss': focal_loss, 'focal_loss_fixed': focal_loss()})  \n",
    "    cnn2= load_model(path+str(prob)+'_wo/cnn2_'+str(j+1)+'-exec_w4_app1_Top_'+str(prob)+'_1.h5', custom_objects={'FocalLoss': focal_loss, 'focal_loss_fixed': focal_loss()}) \n",
    "    rnn1= load_model(path+str(prob)+'_wo/rnn1_'+str(j+1)+'-exec_w4_app1_Top_'+str(prob)+'_1.h5', custom_objects={'FocalLoss': focal_loss, 'focal_loss_fixed': focal_loss()}) \n",
    "    rnn2= load_model(path+str(prob)+'_wo/rnn2_'+str(j+1)+'-exec_w4_app1_Top_'+str(prob)+'_1.h5', custom_objects={'FocalLoss': focal_loss, 'focal_loss_fixed': focal_loss()}) \n",
    "    rnn3= load_model(path+str(prob)+'_wo/rnn3_'+str(j+1)+'-exec_w4_app1_Top_'+str(prob)+'_1.h5') \n",
    "    \n",
    "    list_models=['cnn1', 'cnn2', 'rnn1', 'rnn2', 'rnn3']\n",
    "    index_models=np.arange(5)\n",
    "    dict_models=dict((key, value) for (key, value) in zip(index_models,list_models))\n",
    "    modelos=[cnn1, cnn2, rnn1, rnn2, rnn3]\n",
    "    ind=np.arange(5)\n",
    "    dict_trainedModel=dict((key, value) for (key, value) in zip(ind,modelos))\n",
    "    \n",
    "    print (\"Agregando predicciones Val set\")\n",
    "    for i in dict_trainedModel.keys(): \n",
    "        a_evaluar=dict_models[i]\n",
    "        predicho=dict_trainedModel[i].predict(x_val, batch_size=bs)\n",
    "        predicho=[np.argmax(pred) for pred in predicho]\n",
    "        acc= accuracy_score(etiq_v, predicho)  \n",
    "        f1_ma=f1_score(etiq_v, predicho, average='macro')\n",
    "        f1_no=f1_score(etiq_v, predicho, average=None)\n",
    "        matriz=normalize(confusion_matrix(etiq_v, predicho))\n",
    "        fs_macro_val[dict_models[i]].append(f1_ma)\n",
    "        fs_none_val[dict_models[i]].append(f1_no)\n",
    "        accs_val[dict_models[i]].append(acc)\n",
    "        confusions_val[dict_models[i]].append(matriz)\n",
    "        \n",
    "    predicciones_all_val=[]\n",
    "    for i in dict_trainedModel.keys(): \n",
    "        a_evaluar=dict_models[i]\n",
    "        predicciones_all_val.append(dict_trainedModel[i].predict(x_val, batch_size=bs))\n",
    "    predichos_all_val=np.concatenate(np.asarray(predicciones_all_val),axis=-1)\n",
    "    \n",
    "    print (\"Agregando predicciones Test set\", dict_models[i])    \n",
    "    for i in dict_trainedModel.keys(): \n",
    "        a_evaluar=dict_models[i]\n",
    "        predicho=dict_trainedModel[i].predict(x_test, batch_size=bs)\n",
    "        predicho=[np.argmax(pred) for pred in predicho]\n",
    "        acc= accuracy_score(etiq_t, predicho)  \n",
    "        f1_ma=f1_score(etiq_t, predicho, average='macro')\n",
    "        f1_no=f1_score(etiq_t, predicho, average=None)\n",
    "        matriz=normalize(confusion_matrix(etiq_t, predicho))\n",
    "        fs_macro_test[dict_models[i]].append(f1_ma)\n",
    "        fs_none_test[dict_models[i]].append(f1_no)\n",
    "        accs_test[dict_models[i]].append(acc)\n",
    "        confusions_test[dict_models[i]].append(matriz)\n",
    "\n",
    "    predicciones_all_test=[]\n",
    "    bs=32\n",
    "    for i in dict_trainedModel.keys(): \n",
    "        a_evaluar=dict_models[i]\n",
    "        predicciones_all_test.append(dict_trainedModel[i].predict(x_test, batch_size=bs))\n",
    "    predichos_all_test=np.concatenate(np.asarray(predicciones_all_test),axis=-1)\n",
    "    \n",
    "    print (\"--------VALIDATION SET--------\")\n",
    "    \n",
    "    print (\"\\nCommittee Best Fit\")\n",
    "    trainPredicho, maxPredicho, confianzas = predecir_modelos(bs,predichos_all_val)\n",
    "    acc_comite= accuracy_score(etiq_v, trainPredicho)  \n",
    "    print(\"Accuracy:\",acc_comite)\n",
    "    f1=f1_score(etiq_v, trainPredicho, average='macro') \n",
    "    print(\"F1-macro:\",f1)\n",
    "    f1_no=f1_score(etiq_v, trainPredicho, average=None)  \n",
    "    print(\"F1-score SDQC:\",f1)\n",
    "    best['acc'].append(acc_comite)\n",
    "    best['macro'].append(f1)\n",
    "    best['none'].append(f1_no)\n",
    "    \n",
    "    print (\"\\nCommittee Norm\")\n",
    "    trainPredicho, maxPredicho, confianzas = predecir_modelos_norm(bs,predichos_all_val)\n",
    "    acc_comite= accuracy_score(etiq_v, trainPredicho)  \n",
    "    print(\"Accuracy:\",acc_comite)\n",
    "    f1=f1_score(etiq_v, trainPredicho, average='macro') \n",
    "    print(\"F1-macro:\",f1)\n",
    "    f1_no=f1_score(etiq_v, trainPredicho, average=None) \n",
    "    print(\"F1-score SDQC:\",f1)\n",
    "    norm['acc'].append(acc_comite)\n",
    "    norm['macro'].append(f1)\n",
    "    norm['none'].append(f1_no)\n",
    "\n",
    "    print (\"\\nCommittee Voting\")\n",
    "    trainPredicho, maxPredicho, confianzas = predecir_modelos_average(bs,predichos_all_val)\n",
    "    acc_comite= accuracy_score(etiq_v, trainPredicho)  \n",
    "    print(\"Accuracy:\",acc_comite)\n",
    "    f1=f1_score(etiq_v, trainPredicho, average='macro') \n",
    "    print(\"F1-macro:\",f1)\n",
    "    f1_no=f1_score(etiq_v, trainPredicho, average=None)  \n",
    "    print(\"F1-score SDQC:\",f1)\n",
    "    voting['acc'].append(acc_comite)\n",
    "    voting['macro'].append(f1)\n",
    "    voting['none'].append(f1_no)\n",
    "    \n",
    "    print (\"--------TESTING SET--------\")\n",
    "    \n",
    "    print (\"\\nCommittee Best Fit\")\n",
    "    trainPredicho, maxPredicho, confianzas = predecir_modelos(bs,predichos_all_test)\n",
    "    acc_comite= accuracy_score(etiq_t, trainPredicho)  \n",
    "    print(\"Accuracy:\",acc_comite)\n",
    "    f1=f1_score(etiq_t, trainPredicho, average='macro') \n",
    "    print(\"F1-macro:\",f1)\n",
    "    f1_no=f1_score(etiq_t, trainPredicho, average=None)  \n",
    "    print(\"F1-score SDQC:\",f1)\n",
    "    best_test['acc'].append(acc_comite)\n",
    "    best_test['macro'].append(f1)\n",
    "    best_test['none'].append(f1_no)\n",
    "\n",
    "    print (\"\\nCommittee Norm\")\n",
    "    trainPredicho, maxPredicho, confianzas = predecir_modelos_norm(bs,predichos_all_test)\n",
    "    acc_comite= accuracy_score(etiq_t, trainPredicho)  \n",
    "    print(\"Accuracy:\",acc_comite)\n",
    "    f1=f1_score(etiq_t, trainPredicho, average='macro') \n",
    "    print(\"F1-macro:\",f1)\n",
    "    f1_no=f1_score(etiq_t, trainPredicho, average=None) \n",
    "    print(\"F1-score SDQC:\",f1)\n",
    "    norm_test['acc'].append(acc_comite)\n",
    "    norm_test['macro'].append(f1)\n",
    "    norm_test['none'].append(f1_no)\n",
    "\n",
    "    print (\"\\nCommittee Voting\")\n",
    "    trainPredicho, maxPredicho, confianzas = predecir_modelos_average(bs,predichos_all_test)\n",
    "    acc_comite= accuracy_score(etiq_t, trainPredicho)  \n",
    "    print(\"Accuracy:\",acc_comite)\n",
    "    f1=f1_score(etiq_t, trainPredicho, average='macro') \n",
    "    print(\"F1-macro:\",f1)\n",
    "    f1_no=f1_score(etiq_t, trainPredicho, average=None)  \n",
    "    print(\"F1-score SDQC:\",f1)\n",
    "    voting_test['acc'].append(acc_comite)\n",
    "    voting_test['macro'].append(f1)\n",
    "    voting_test['none'].append(f1_no)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAL] Acc Best comité: 0.8727529411764706\n",
      "[VAL] F1-score SDQC Best comité: [0.92465594 0.03186898 0.04055484 0.81523999]\n",
      "[VAL] F1 macro Best comité: 0.4530799387098111\n",
      "-----------\n",
      "[VAL] Acc Norm comité: 0.8796235294117647\n",
      "[VAL] F1-score SDQC Norm comité: [0.92688146 0.01614612 0.08620388 0.82857504]\n",
      "[VAL] F1 macro Norm comité: 0.46445162452718836\n",
      "-----------\n",
      "[VAL] Acc Voting comité: 0.8835764705882352\n",
      "[VAL] F1-score SDQC Voting comité: [0.92966982 0.02133333 0.06709576 0.83855841]\n",
      "[VAL] F1 macro Voting comité: 0.4641643295550222\n",
      "\n",
      "[TEST] Acc Best comité: 0.8335374470089496\n",
      "[TEST] F1-score SDQC Best comité: [0.90564302 0.01072125 0.03501866 0.6737307 ]\n",
      "[TEST] F1 macro Best comité: 0.4062784056503955\n",
      "-----------\n",
      "[TEST] Acc Norm comité: 0.8375883184173339\n",
      "[TEST] F1-score SDQC Norm comité: [0.90660699 0.00363636 0.01948052 0.68426389]\n",
      "[TEST] F1 macro Norm comité: 0.4034969400799432\n",
      "-----------\n",
      "[TEST] Acc Voting comité: 0.8404145077720206\n",
      "[TEST] F1-score SDQC Voting comité: [0.90827782 0.00720721 0.01867795 0.69831467]\n",
      "[TEST] F1 macro Voting comité: 0.4081194117035046\n"
     ]
    }
   ],
   "source": [
    "print (\"[VAL] Acc Best comité:\",np.sum(np.asarray(best['acc'])/5.0))\n",
    "temp=np.zeros(4)\n",
    "for result in best['none']:\n",
    "    temp+=result\n",
    "print (\"[VAL] F1-score SDQC Best comité:\" ,temp/5)\n",
    "print (\"[VAL] F1 macro Best comité:\",np.sum(np.asarray(best['macro'])/5.0))\n",
    "print (\"-----------\")\n",
    "print (\"[VAL] Acc Norm comité:\",np.sum(np.asarray(norm['acc'])/5.0))\n",
    "temp=np.zeros(4)\n",
    "for result in norm['none']:\n",
    "    temp+=result\n",
    "print (\"[VAL] F1-score SDQC Norm comité:\" ,temp/5)\n",
    "print (\"[VAL] F1 macro Norm comité:\",np.sum(np.asarray(norm['macro'])/5.0))\n",
    "print (\"-----------\")\n",
    "print (\"[VAL] Acc Voting comité:\",np.sum(np.asarray(voting['acc'])/5.0))\n",
    "temp=np.zeros(4)\n",
    "for result in voting['none']:\n",
    "    temp+=result\n",
    "print (\"[VAL] F1-score SDQC Voting comité:\" ,temp/5)\n",
    "print (\"[VAL] F1 macro Voting comité:\",np.sum(np.asarray(voting['macro'])/5.0))\n",
    "\n",
    "print (\"\")\n",
    "\n",
    "print (\"[TEST] Acc Best comité:\",np.sum(np.asarray(best_test['acc'])/5.0))\n",
    "temp=np.zeros(4)\n",
    "for result in best_test['none']:\n",
    "    temp+=result\n",
    "print (\"[TEST] F1-score SDQC Best comité:\" ,temp/5)\n",
    "print (\"[TEST] F1 macro Best comité:\",np.sum(np.asarray(best_test['macro'])/5.0))\n",
    "print (\"-----------\")\n",
    "print (\"[TEST] Acc Norm comité:\",np.sum(np.asarray(norm_test['acc'])/5.0))\n",
    "temp=np.zeros(4)\n",
    "for result in norm_test['none']:\n",
    "    temp+=result\n",
    "print (\"[TEST] F1-score SDQC Norm comité:\" ,temp/5)\n",
    "print (\"[TEST] F1 macro Norm comité:\",np.sum(np.asarray(norm_test['macro'])/5.0))\n",
    "print (\"-----------\")\n",
    "print (\"[TEST] Acc Voting comité:\",np.sum(np.asarray(voting_test['acc'])/5.0))\n",
    "temp=np.zeros(4)\n",
    "for result in voting_test['none']:\n",
    "    temp+=result\n",
    "print (\"[TEST] F1-score SDQC Voting comité:\" ,temp/5)\n",
    "print (\"[TEST] F1 macro Voting comité:\",np.sum(np.asarray(voting_test['macro'])/5.0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3946579417939224"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(fs_macro_test['cnn2'])/5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No data augmentation with class weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------- EJECUCIÓN #1----------------\n",
      "Agregando predicciones Val set\n",
      "Agregando predicciones Test set rnn3\n",
      "--------VALIDATION SET--------\n",
      "\n",
      "Committee Best Fit\n",
      "Accuracy: 0.856\n",
      "F1-macro: 0.4674628152431399\n",
      "F1-score SDQC: 0.4674628152431399\n",
      "\n",
      "Committee Norm\n",
      "Accuracy: 0.8861176470588236\n",
      "F1-macro: 0.4897821832896734\n",
      "F1-score SDQC: 0.4897821832896734\n",
      "\n",
      "Committee Voting\n",
      "Accuracy: 0.8856470588235295\n",
      "F1-macro: 0.4836263144462799\n",
      "F1-score SDQC: 0.4836263144462799\n",
      "--------TESTING SET--------\n",
      "\n",
      "Committee Best Fit\n",
      "Accuracy: 0.8257183231276496\n",
      "F1-macro: 0.4615048818711459\n",
      "F1-score SDQC: 0.4615048818711459\n",
      "\n",
      "Committee Norm\n",
      "Accuracy: 0.8455016486104568\n",
      "F1-macro: 0.45242057133090174\n",
      "F1-score SDQC: 0.45242057133090174\n",
      "\n",
      "Committee Voting\n",
      "Accuracy: 0.8520960904380593\n",
      "F1-macro: 0.47603460336458225\n",
      "F1-score SDQC: 0.47603460336458225\n",
      "\n",
      "---------------- EJECUCIÓN #2----------------\n",
      "Agregando predicciones Val set\n",
      "Agregando predicciones Test set rnn3\n",
      "--------VALIDATION SET--------\n",
      "\n",
      "Committee Best Fit\n",
      "Accuracy: 0.8654117647058823\n",
      "F1-macro: 0.4760309713788969\n",
      "F1-score SDQC: 0.4760309713788969\n",
      "\n",
      "Committee Norm\n",
      "Accuracy: 0.8781176470588236\n",
      "F1-macro: 0.46910936314241086\n",
      "F1-score SDQC: 0.46910936314241086\n",
      "\n",
      "Committee Voting\n",
      "Accuracy: 0.8776470588235294\n",
      "F1-macro: 0.46741155607852675\n",
      "F1-score SDQC: 0.46741155607852675\n",
      "--------TESTING SET--------\n",
      "\n",
      "Committee Best Fit\n",
      "Accuracy: 0.833254828073481\n",
      "F1-macro: 0.42704506180893564\n",
      "F1-score SDQC: 0.42704506180893564\n",
      "\n",
      "Committee Norm\n",
      "Accuracy: 0.8422044276966557\n",
      "F1-macro: 0.4214953800750253\n",
      "F1-score SDQC: 0.4214953800750253\n",
      "\n",
      "Committee Voting\n",
      "Accuracy: 0.8407913330193123\n",
      "F1-macro: 0.4246550268335044\n",
      "F1-score SDQC: 0.4246550268335044\n",
      "\n",
      "---------------- EJECUCIÓN #3----------------\n",
      "Agregando predicciones Val set\n",
      "Agregando predicciones Test set rnn3\n",
      "--------VALIDATION SET--------\n",
      "\n",
      "Committee Best Fit\n",
      "Accuracy: 0.8705882352941177\n",
      "F1-macro: 0.46931840288559146\n",
      "F1-score SDQC: 0.46931840288559146\n",
      "\n",
      "Committee Norm\n",
      "Accuracy: 0.8828235294117647\n",
      "F1-macro: 0.4659418496383239\n",
      "F1-score SDQC: 0.4659418496383239\n",
      "\n",
      "Committee Voting\n",
      "Accuracy: 0.88\n",
      "F1-macro: 0.4634433813495783\n",
      "F1-score SDQC: 0.4634433813495783\n",
      "--------TESTING SET--------\n",
      "\n",
      "Committee Best Fit\n",
      "Accuracy: 0.8247762600094206\n",
      "F1-macro: 0.40503020543248286\n",
      "F1-score SDQC: 0.40503020543248286\n",
      "\n",
      "Committee Norm\n",
      "Accuracy: 0.8379651436646255\n",
      "F1-macro: 0.409640623439568\n",
      "F1-score SDQC: 0.409640623439568\n",
      "\n",
      "Committee Voting\n",
      "Accuracy: 0.8403203014601979\n",
      "F1-macro: 0.42129180858740606\n",
      "F1-score SDQC: 0.42129180858740606\n",
      "\n",
      "---------------- EJECUCIÓN #4----------------\n",
      "Agregando predicciones Val set\n",
      "Agregando predicciones Test set rnn3\n",
      "--------VALIDATION SET--------\n",
      "\n",
      "Committee Best Fit\n",
      "Accuracy: 0.8602352941176471\n",
      "F1-macro: 0.47310328618080055\n",
      "F1-score SDQC: 0.47310328618080055\n",
      "\n",
      "Committee Norm\n",
      "Accuracy: 0.8870588235294118\n",
      "F1-macro: 0.49375331670720213\n",
      "F1-score SDQC: 0.49375331670720213\n",
      "\n",
      "Committee Voting\n",
      "Accuracy: 0.8847058823529412\n",
      "F1-macro: 0.48110191077291276\n",
      "F1-score SDQC: 0.48110191077291276\n",
      "--------TESTING SET--------\n",
      "\n",
      "Committee Best Fit\n",
      "Accuracy: 0.8257183231276496\n",
      "F1-macro: 0.4614211122687981\n",
      "F1-score SDQC: 0.4614211122687981\n",
      "\n",
      "Committee Norm\n",
      "Accuracy: 0.843617522373999\n",
      "F1-macro: 0.44366360911886127\n",
      "F1-score SDQC: 0.44366360911886127\n",
      "\n",
      "Committee Voting\n",
      "Accuracy: 0.8464437117286858\n",
      "F1-macro: 0.45744188268799413\n",
      "F1-score SDQC: 0.45744188268799413\n",
      "\n",
      "---------------- EJECUCIÓN #5----------------\n",
      "Agregando predicciones Val set\n",
      "Agregando predicciones Test set rnn3\n",
      "--------VALIDATION SET--------\n",
      "\n",
      "Committee Best Fit\n",
      "Accuracy: 0.8762352941176471\n",
      "F1-macro: 0.46078628500727337\n",
      "F1-score SDQC: 0.46078628500727337\n",
      "\n",
      "Committee Norm\n",
      "Accuracy: 0.88\n",
      "F1-macro: 0.45790179768509687\n",
      "F1-score SDQC: 0.45790179768509687\n",
      "\n",
      "Committee Voting\n",
      "Accuracy: 0.8795294117647059\n",
      "F1-macro: 0.4638679608310896\n",
      "F1-score SDQC: 0.4638679608310896\n",
      "--------TESTING SET--------\n",
      "\n",
      "Committee Best Fit\n",
      "Accuracy: 0.8356099858690532\n",
      "F1-macro: 0.42614812340649716\n",
      "F1-score SDQC: 0.42614812340649716\n",
      "\n",
      "Committee Norm\n",
      "Accuracy: 0.8393782383419689\n",
      "F1-macro: 0.40724864507360414\n",
      "F1-score SDQC: 0.40724864507360414\n",
      "\n",
      "Committee Voting\n",
      "Accuracy: 0.8398492699010833\n",
      "F1-macro: 0.4166154188363965\n",
      "F1-score SDQC: 0.4166154188363965\n"
     ]
    }
   ],
   "source": [
    "prob=0.0\n",
    "path='../Operador Data Augmentation/exec_baselines_harass/'\n",
    "bs=32\n",
    "    \n",
    "fs_macro_val={'cnn1':[], 'cnn2':[], 'rnn1':[],'rnn2':[],'rnn3':[]}\n",
    "fs_none_val={'cnn1':[], 'cnn2':[], 'rnn1':[],'rnn2':[],'rnn3':[]}\n",
    "accs_val={'cnn1':[], 'cnn2':[], 'rnn1':[],'rnn2':[],'rnn3':[]}\n",
    "confusions_val={'cnn1':[], 'cnn2':[], 'rnn1':[],'rnn2':[],'rnn3':[]}\n",
    "\n",
    "fs_macro_test={'cnn1':[], 'cnn2':[], 'rnn1':[],'rnn2':[],'rnn3':[]}\n",
    "fs_none_test={'cnn1':[], 'cnn2':[], 'rnn1':[],'rnn2':[],'rnn3':[]}\n",
    "accs_test={'cnn1':[], 'cnn2':[], 'rnn1':[],'rnn2':[],'rnn3':[]}\n",
    "confusions_test={'cnn1':[], 'cnn2':[], 'rnn1':[],'rnn2':[],'rnn3':[]}\n",
    "\n",
    "best={'acc':[], 'none':[], 'macro':[]}\n",
    "norm={'acc':[], 'none':[], 'macro':[]}\n",
    "voting={'acc':[], 'none':[], 'macro':[]}\n",
    "best_test={'acc':[], 'none':[], 'macro':[]}\n",
    "norm_test={'acc':[], 'none':[], 'macro':[]}\n",
    "voting_test={'acc':[], 'none':[], 'macro':[]}\n",
    "\n",
    "#exec_baselines/0.15/rnn1_'+str(i+1)+'-exec_w4_app1_Top_'+str(prob)+'_'+str(TOPN)+'.h5'\n",
    "#exec_baselines/SMOTE/cnn2_'+str(i+1)+'-exec_w4_app1.h5\n",
    "for j in range(5):\n",
    "    print (\"\")\n",
    "    print (\"---------------- EJECUCIÓN #\"+str(j+1)+'----------------')\n",
    "    cnn1= load_model(path+str(prob)+'/cnn1_'+str(j+1)+'-exec_w4_app1_Top_'+str(prob)+'_1.h5', custom_objects={'FocalLoss': focal_loss, 'focal_loss_fixed': focal_loss()})  \n",
    "    cnn2= load_model(path+str(prob)+'/cnn2_'+str(j+1)+'-exec_w4_app1_Top_'+str(prob)+'_1.h5', custom_objects={'FocalLoss': focal_loss, 'focal_loss_fixed': focal_loss()}) \n",
    "    rnn1= load_model(path+str(prob)+'/rnn1_'+str(j+1)+'-exec_w4_app1_Top_'+str(prob)+'_1.h5', custom_objects={'FocalLoss': focal_loss, 'focal_loss_fixed': focal_loss()}) \n",
    "    rnn2= load_model(path+str(prob)+'/rnn2_'+str(j+1)+'-exec_w4_app1_Top_'+str(prob)+'_1.h5', custom_objects={'FocalLoss': focal_loss, 'focal_loss_fixed': focal_loss()}) \n",
    "    rnn3= load_model(path+str(prob)+'/rnn3_'+str(j+1)+'-exec_w4_app1_Top_'+str(prob)+'_1.h5') \n",
    "    \n",
    "    list_models=['cnn1', 'cnn2', 'rnn1', 'rnn2', 'rnn3']\n",
    "    index_models=np.arange(5)\n",
    "    dict_models=dict((key, value) for (key, value) in zip(index_models,list_models))\n",
    "    modelos=[cnn1, cnn2, rnn1, rnn2, rnn3]\n",
    "    ind=np.arange(5)\n",
    "    dict_trainedModel=dict((key, value) for (key, value) in zip(ind,modelos))\n",
    "    \n",
    "    print (\"Agregando predicciones Val set\")\n",
    "    for i in dict_trainedModel.keys(): \n",
    "        a_evaluar=dict_models[i]\n",
    "        predicho=dict_trainedModel[i].predict(x_val, batch_size=bs)\n",
    "        predicho=[np.argmax(pred) for pred in predicho]\n",
    "        acc= accuracy_score(etiq_v, predicho)  \n",
    "        f1_ma=f1_score(etiq_v, predicho, average='macro')\n",
    "        f1_no=f1_score(etiq_v, predicho, average=None)\n",
    "        matriz=normalize(confusion_matrix(etiq_v, predicho))\n",
    "        fs_macro_val[dict_models[i]].append(f1_ma)\n",
    "        fs_none_val[dict_models[i]].append(f1_no)\n",
    "        accs_val[dict_models[i]].append(acc)\n",
    "        confusions_val[dict_models[i]].append(matriz)\n",
    "        \n",
    "    predicciones_all_val=[]\n",
    "    for i in dict_trainedModel.keys(): \n",
    "        a_evaluar=dict_models[i]\n",
    "        predicciones_all_val.append(dict_trainedModel[i].predict(x_val, batch_size=bs))\n",
    "    predichos_all_val=np.concatenate(np.asarray(predicciones_all_val),axis=-1)\n",
    "    \n",
    "    print (\"Agregando predicciones Test set\", dict_models[i])    \n",
    "    for i in dict_trainedModel.keys(): \n",
    "        a_evaluar=dict_models[i]\n",
    "        predicho=dict_trainedModel[i].predict(x_test, batch_size=bs)\n",
    "        predicho=[np.argmax(pred) for pred in predicho]\n",
    "        acc= accuracy_score(etiq_t, predicho)  \n",
    "        f1_ma=f1_score(etiq_t, predicho, average='macro')\n",
    "        f1_no=f1_score(etiq_t, predicho, average=None)\n",
    "        matriz=normalize(confusion_matrix(etiq_t, predicho))\n",
    "        fs_macro_test[dict_models[i]].append(f1_ma)\n",
    "        fs_none_test[dict_models[i]].append(f1_no)\n",
    "        accs_test[dict_models[i]].append(acc)\n",
    "        confusions_test[dict_models[i]].append(matriz)\n",
    "\n",
    "    predicciones_all_test=[]\n",
    "    bs=32\n",
    "    for i in dict_trainedModel.keys(): \n",
    "        a_evaluar=dict_models[i]\n",
    "        predicciones_all_test.append(dict_trainedModel[i].predict(x_test, batch_size=bs))\n",
    "    predichos_all_test=np.concatenate(np.asarray(predicciones_all_test),axis=-1)\n",
    "    \n",
    "    print (\"--------VALIDATION SET--------\")\n",
    "    \n",
    "    print (\"\\nCommittee Best Fit\")\n",
    "    trainPredicho, maxPredicho, confianzas = predecir_modelos(bs,predichos_all_val)\n",
    "    acc_comite= accuracy_score(etiq_v, trainPredicho)  \n",
    "    print(\"Accuracy:\",acc_comite)\n",
    "    f1=f1_score(etiq_v, trainPredicho, average='macro') \n",
    "    print(\"F1-macro:\",f1)\n",
    "    f1_no=f1_score(etiq_v, trainPredicho, average=None)  \n",
    "    print(\"F1-score SDQC:\",f1)\n",
    "    best['acc'].append(acc_comite)\n",
    "    best['macro'].append(f1)\n",
    "    best['none'].append(f1_no)\n",
    "    \n",
    "    print (\"\\nCommittee Norm\")\n",
    "    trainPredicho, maxPredicho, confianzas = predecir_modelos_norm(bs,predichos_all_val)\n",
    "    acc_comite= accuracy_score(etiq_v, trainPredicho)  \n",
    "    print(\"Accuracy:\",acc_comite)\n",
    "    f1=f1_score(etiq_v, trainPredicho, average='macro') \n",
    "    print(\"F1-macro:\",f1)\n",
    "    f1_no=f1_score(etiq_v, trainPredicho, average=None) \n",
    "    print(\"F1-score SDQC:\",f1)\n",
    "    norm['acc'].append(acc_comite)\n",
    "    norm['macro'].append(f1)\n",
    "    norm['none'].append(f1_no)\n",
    "\n",
    "    print (\"\\nCommittee Voting\")\n",
    "    trainPredicho, maxPredicho, confianzas = predecir_modelos_average(bs,predichos_all_val)\n",
    "    acc_comite= accuracy_score(etiq_v, trainPredicho)  \n",
    "    print(\"Accuracy:\",acc_comite)\n",
    "    f1=f1_score(etiq_v, trainPredicho, average='macro') \n",
    "    print(\"F1-macro:\",f1)\n",
    "    f1_no=f1_score(etiq_v, trainPredicho, average=None)  \n",
    "    print(\"F1-score SDQC:\",f1)\n",
    "    voting['acc'].append(acc_comite)\n",
    "    voting['macro'].append(f1)\n",
    "    voting['none'].append(f1_no)\n",
    "    \n",
    "    print (\"--------TESTING SET--------\")\n",
    "    \n",
    "    print (\"\\nCommittee Best Fit\")\n",
    "    trainPredicho, maxPredicho, confianzas = predecir_modelos(bs,predichos_all_test)\n",
    "    acc_comite= accuracy_score(etiq_t, trainPredicho)  \n",
    "    print(\"Accuracy:\",acc_comite)\n",
    "    f1=f1_score(etiq_t, trainPredicho, average='macro') \n",
    "    print(\"F1-macro:\",f1)\n",
    "    f1_no=f1_score(etiq_t, trainPredicho, average=None)  \n",
    "    print(\"F1-score SDQC:\",f1)\n",
    "    best_test['acc'].append(acc_comite)\n",
    "    best_test['macro'].append(f1)\n",
    "    best_test['none'].append(f1_no)\n",
    "\n",
    "    print (\"\\nCommittee Norm\")\n",
    "    trainPredicho, maxPredicho, confianzas = predecir_modelos_norm(bs,predichos_all_test)\n",
    "    acc_comite= accuracy_score(etiq_t, trainPredicho)  \n",
    "    print(\"Accuracy:\",acc_comite)\n",
    "    f1=f1_score(etiq_t, trainPredicho, average='macro') \n",
    "    print(\"F1-macro:\",f1)\n",
    "    f1_no=f1_score(etiq_t, trainPredicho, average=None) \n",
    "    print(\"F1-score SDQC:\",f1)\n",
    "    norm_test['acc'].append(acc_comite)\n",
    "    norm_test['macro'].append(f1)\n",
    "    norm_test['none'].append(f1_no)\n",
    "\n",
    "    print (\"\\nCommittee Voting\")\n",
    "    trainPredicho, maxPredicho, confianzas = predecir_modelos_average(bs,predichos_all_test)\n",
    "    acc_comite= accuracy_score(etiq_t, trainPredicho)  \n",
    "    print(\"Accuracy:\",acc_comite)\n",
    "    f1=f1_score(etiq_t, trainPredicho, average='macro') \n",
    "    print(\"F1-macro:\",f1)\n",
    "    f1_no=f1_score(etiq_t, trainPredicho, average=None)  \n",
    "    print(\"F1-score SDQC:\",f1)\n",
    "    voting_test['acc'].append(acc_comite)\n",
    "    voting_test['macro'].append(f1)\n",
    "    voting_test['none'].append(f1_no)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAL] Acc Best comité: 0.8656941176470588\n",
      "[VAL] F1-score SDQC Best comité: [0.92554721 0.07382042 0.06402727 0.81396651]\n",
      "[VAL] F1 macro Best comité: 0.46934035213914044\n",
      "-----------\n",
      "[VAL] Acc Norm comité: 0.8828235294117648\n",
      "[VAL] F1-score SDQC Norm comité: [0.93129419 0.07021738 0.05912418 0.84055507]\n",
      "[VAL] F1 macro Norm comité: 0.4752977020925414\n",
      "-----------\n",
      "[VAL] Acc Voting comité: 0.8815058823529414\n",
      "[VAL] F1-score SDQC Voting comité: [0.93163735 0.06928308 0.04695681 0.83968365]\n",
      "[VAL] F1 macro Voting comité: 0.47189022469567743\n",
      "\n",
      "[TEST] Acc Best comité: 0.8290155440414508\n",
      "[TEST] F1-score SDQC Best comité: [0.9065706  0.08448481 0.06193971 0.69192439]\n",
      "[TEST] F1 macro Best comité: 0.436229876957572\n",
      "-----------\n",
      "[TEST] Acc Norm comité: 0.8417333961375412\n",
      "[TEST] F1-score SDQC Norm comité: [0.91018925 0.04464833 0.03240069 0.72033679]\n",
      "[TEST] F1 macro Norm comité: 0.4268937658075921\n",
      "-----------\n",
      "[TEST] Acc Voting comité: 0.8439001413094677\n",
      "[TEST] F1-score SDQC Voting comité: [0.91146773 0.07084244 0.04642538 0.72809544]\n",
      "[TEST] F1 macro Voting comité: 0.4392077480619767\n"
     ]
    }
   ],
   "source": [
    "print (\"[VAL] Acc Best comité:\",np.sum(np.asarray(best['acc'])/5.0))\n",
    "temp=np.zeros(4)\n",
    "for result in best['none']:\n",
    "    temp+=result\n",
    "print (\"[VAL] F1-score SDQC Best comité:\" ,temp/5)\n",
    "print (\"[VAL] F1 macro Best comité:\",np.sum(np.asarray(best['macro'])/5.0))\n",
    "print (\"-----------\")\n",
    "print (\"[VAL] Acc Norm comité:\",np.sum(np.asarray(norm['acc'])/5.0))\n",
    "temp=np.zeros(4)\n",
    "for result in norm['none']:\n",
    "    temp+=result\n",
    "print (\"[VAL] F1-score SDQC Norm comité:\" ,temp/5)\n",
    "print (\"[VAL] F1 macro Norm comité:\",np.sum(np.asarray(norm['macro'])/5.0))\n",
    "print (\"-----------\")\n",
    "print (\"[VAL] Acc Voting comité:\",np.sum(np.asarray(voting['acc'])/5.0))\n",
    "temp=np.zeros(4)\n",
    "for result in voting['none']:\n",
    "    temp+=result\n",
    "print (\"[VAL] F1-score SDQC Voting comité:\" ,temp/5)\n",
    "print (\"[VAL] F1 macro Voting comité:\",np.sum(np.asarray(voting['macro'])/5.0))\n",
    "\n",
    "print (\"\")\n",
    "\n",
    "print (\"[TEST] Acc Best comité:\",np.sum(np.asarray(best_test['acc'])/5.0))\n",
    "temp=np.zeros(4)\n",
    "for result in best_test['none']:\n",
    "    temp+=result\n",
    "print (\"[TEST] F1-score SDQC Best comité:\" ,temp/5)\n",
    "print (\"[TEST] F1 macro Best comité:\",np.sum(np.asarray(best_test['macro'])/5.0))\n",
    "print (\"-----------\")\n",
    "print (\"[TEST] Acc Norm comité:\",np.sum(np.asarray(norm_test['acc'])/5.0))\n",
    "temp=np.zeros(4)\n",
    "for result in norm_test['none']:\n",
    "    temp+=result\n",
    "print (\"[TEST] F1-score SDQC Norm comité:\" ,temp/5)\n",
    "print (\"[TEST] F1 macro Norm comité:\",np.sum(np.asarray(norm_test['macro'])/5.0))\n",
    "print (\"-----------\")\n",
    "print (\"[TEST] Acc Voting comité:\",np.sum(np.asarray(voting_test['acc'])/5.0))\n",
    "temp=np.zeros(4)\n",
    "for result in voting_test['none']:\n",
    "    temp+=result\n",
    "print (\"[TEST] F1-score SDQC Voting comité:\" ,temp/5)\n",
    "print (\"[TEST] F1 macro Voting comité:\",np.sum(np.asarray(voting_test['macro'])/5.0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.36815395067255396"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(fs_macro_test['cnn2'])/5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augmentation 0.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------- EJECUCIÓN #1----------------\n",
      "Agregando predicciones Val set\n",
      "Agregando predicciones Test set rnn3\n",
      "--------VALIDATION SET--------\n",
      "\n",
      "Committee Best Fit\n",
      "Accuracy: 0.8696470588235294\n",
      "F1-macro: 0.4790917114519783\n",
      "F1-score SDQC: 0.4790917114519783\n",
      "\n",
      "Committee Norm\n",
      "Accuracy: 0.8856470588235295\n",
      "F1-macro: 0.4573743908834216\n",
      "F1-score SDQC: 0.4573743908834216\n",
      "\n",
      "Committee Voting\n",
      "Accuracy: 0.8912941176470588\n",
      "F1-macro: 0.4675869805712206\n",
      "F1-score SDQC: 0.4675869805712206\n",
      "--------TESTING SET--------\n",
      "\n",
      "Committee Best Fit\n",
      "Accuracy: 0.8186528497409327\n",
      "F1-macro: 0.44014810212871286\n",
      "F1-score SDQC: 0.44014810212871286\n",
      "\n",
      "Committee Norm\n",
      "Accuracy: 0.8426754592557701\n",
      "F1-macro: 0.419759828271551\n",
      "F1-score SDQC: 0.419759828271551\n",
      "\n",
      "Committee Voting\n",
      "Accuracy: 0.8473857748469148\n",
      "F1-macro: 0.42889216613261777\n",
      "F1-score SDQC: 0.42889216613261777\n",
      "\n",
      "---------------- EJECUCIÓN #2----------------\n",
      "Agregando predicciones Val set\n",
      "Agregando predicciones Test set rnn3\n",
      "--------VALIDATION SET--------\n",
      "\n",
      "Committee Best Fit\n",
      "Accuracy: 0.7369411764705882\n",
      "F1-macro: 0.40135035026409094\n",
      "F1-score SDQC: 0.40135035026409094\n",
      "\n",
      "Committee Norm\n",
      "Accuracy: 0.8564705882352941\n",
      "F1-macro: 0.44139514307485733\n",
      "F1-score SDQC: 0.44139514307485733\n",
      "\n",
      "Committee Voting\n",
      "Accuracy: 0.8837647058823529\n",
      "F1-macro: 0.457825381185719\n",
      "F1-score SDQC: 0.457825381185719\n",
      "--------TESTING SET--------\n",
      "\n",
      "Committee Best Fit\n",
      "Accuracy: 0.688177107866227\n",
      "F1-macro: 0.358392764787049\n",
      "F1-score SDQC: 0.358392764787049\n",
      "\n",
      "Committee Norm\n",
      "Accuracy: 0.8106453132359868\n",
      "F1-macro: 0.41073252892990647\n",
      "F1-score SDQC: 0.41073252892990647\n",
      "\n",
      "Committee Voting\n",
      "Accuracy: 0.83843617522374\n",
      "F1-macro: 0.43842616555478875\n",
      "F1-score SDQC: 0.43842616555478875\n",
      "\n",
      "---------------- EJECUCIÓN #3----------------\n",
      "Agregando predicciones Val set\n",
      "Agregando predicciones Test set rnn3\n",
      "--------VALIDATION SET--------\n",
      "\n",
      "Committee Best Fit\n",
      "Accuracy: 0.7534117647058823\n",
      "F1-macro: 0.4775529082050037\n",
      "F1-score SDQC: 0.4775529082050037\n",
      "\n",
      "Committee Norm\n",
      "Accuracy: 0.8856470588235295\n",
      "F1-macro: 0.47018339691694877\n",
      "F1-score SDQC: 0.47018339691694877\n",
      "\n",
      "Committee Voting\n",
      "Accuracy: 0.8884705882352941\n",
      "F1-macro: 0.46565939943397305\n",
      "F1-score SDQC: 0.46565939943397305\n",
      "--------TESTING SET--------\n",
      "\n",
      "Committee Best Fit\n",
      "Accuracy: 0.7093735280263778\n",
      "F1-macro: 0.42172382447924694\n",
      "F1-score SDQC: 0.42172382447924694\n",
      "\n",
      "Committee Norm\n",
      "Accuracy: 0.8341968911917098\n",
      "F1-macro: 0.4186870747703352\n",
      "F1-score SDQC: 0.4186870747703352\n",
      "\n",
      "Committee Voting\n",
      "Accuracy: 0.8379651436646255\n",
      "F1-macro: 0.4248930780708803\n",
      "F1-score SDQC: 0.4248930780708803\n",
      "\n",
      "---------------- EJECUCIÓN #4----------------\n",
      "Agregando predicciones Val set\n",
      "Agregando predicciones Test set rnn3\n",
      "--------VALIDATION SET--------\n",
      "\n",
      "Committee Best Fit\n",
      "Accuracy: 0.8672941176470588\n",
      "F1-macro: 0.47020237442899304\n",
      "F1-score SDQC: 0.47020237442899304\n",
      "\n",
      "Committee Norm\n",
      "Accuracy: 0.884235294117647\n",
      "F1-macro: 0.46227703331111103\n",
      "F1-score SDQC: 0.46227703331111103\n",
      "\n",
      "Committee Voting\n",
      "Accuracy: 0.8851764705882353\n",
      "F1-macro: 0.46291425752253357\n",
      "F1-score SDQC: 0.46291425752253357\n",
      "--------TESTING SET--------\n",
      "\n",
      "Committee Best Fit\n",
      "Accuracy: 0.8247762600094206\n",
      "F1-macro: 0.42399205660853123\n",
      "F1-score SDQC: 0.42399205660853123\n",
      "\n",
      "Committee Norm\n",
      "Accuracy: 0.8389072067828545\n",
      "F1-macro: 0.4035358668818121\n",
      "F1-score SDQC: 0.4035358668818121\n",
      "\n",
      "Committee Voting\n",
      "Accuracy: 0.83843617522374\n",
      "F1-macro: 0.404032936431556\n",
      "F1-score SDQC: 0.404032936431556\n",
      "\n",
      "---------------- EJECUCIÓN #5----------------\n",
      "Agregando predicciones Val set\n",
      "Agregando predicciones Test set rnn3\n",
      "--------VALIDATION SET--------\n",
      "\n",
      "Committee Best Fit\n",
      "Accuracy: 0.8498823529411764\n",
      "F1-macro: 0.4534148020366755\n",
      "F1-score SDQC: 0.4534148020366755\n",
      "\n",
      "Committee Norm\n",
      "Accuracy: 0.8729411764705882\n",
      "F1-macro: 0.4525343050231847\n",
      "F1-score SDQC: 0.4525343050231847\n",
      "\n",
      "Committee Voting\n",
      "Accuracy: 0.8795294117647059\n",
      "F1-macro: 0.4580422754301735\n",
      "F1-score SDQC: 0.4580422754301735\n",
      "--------TESTING SET--------\n",
      "\n",
      "Committee Best Fit\n",
      "Accuracy: 0.8134715025906736\n",
      "F1-macro: 0.39391947931645477\n",
      "F1-score SDQC: 0.39391947931645477\n",
      "\n",
      "Committee Norm\n",
      "Accuracy: 0.8285445124823363\n",
      "F1-macro: 0.39448842616414553\n",
      "F1-score SDQC: 0.39448842616414553\n",
      "\n",
      "Committee Voting\n",
      "Accuracy: 0.8351389543099388\n",
      "F1-macro: 0.4099948244051532\n",
      "F1-score SDQC: 0.4099948244051532\n"
     ]
    }
   ],
   "source": [
    "prob=0.15\n",
    "path='../Operador Data Augmentation/exec_baselines_harass/'\n",
    "bs=32\n",
    "    \n",
    "fs_macro_val={'cnn1':[], 'cnn2':[], 'rnn1':[],'rnn2':[],'rnn3':[]}\n",
    "fs_none_val={'cnn1':[], 'cnn2':[], 'rnn1':[],'rnn2':[],'rnn3':[]}\n",
    "accs_val={'cnn1':[], 'cnn2':[], 'rnn1':[],'rnn2':[],'rnn3':[]}\n",
    "confusions_val={'cnn1':[], 'cnn2':[], 'rnn1':[],'rnn2':[],'rnn3':[]}\n",
    "\n",
    "fs_macro_test={'cnn1':[], 'cnn2':[], 'rnn1':[],'rnn2':[],'rnn3':[]}\n",
    "fs_none_test={'cnn1':[], 'cnn2':[], 'rnn1':[],'rnn2':[],'rnn3':[]}\n",
    "accs_test={'cnn1':[], 'cnn2':[], 'rnn1':[],'rnn2':[],'rnn3':[]}\n",
    "confusions_test={'cnn1':[], 'cnn2':[], 'rnn1':[],'rnn2':[],'rnn3':[]}\n",
    "\n",
    "best={'acc':[], 'none':[], 'macro':[]}\n",
    "norm={'acc':[], 'none':[], 'macro':[]}\n",
    "voting={'acc':[], 'none':[], 'macro':[]}\n",
    "best_test={'acc':[], 'none':[], 'macro':[]}\n",
    "norm_test={'acc':[], 'none':[], 'macro':[]}\n",
    "voting_test={'acc':[], 'none':[], 'macro':[]}\n",
    "\n",
    "#exec_baselines/0.15/rnn1_'+str(i+1)+'-exec_w4_app1_Top_'+str(prob)+'_'+str(TOPN)+'.h5'\n",
    "#exec_baselines/SMOTE/cnn2_'+str(i+1)+'-exec_w4_app1.h5\n",
    "for j in range(5):\n",
    "    print (\"\")\n",
    "    print (\"---------------- EJECUCIÓN #\"+str(j+1)+'----------------')\n",
    "    cnn1= load_model(path+str(prob)+'/cnn1_'+str(j+1)+'-exec_w4_app1_Top_'+str(prob)+'_1.h5', custom_objects={'FocalLoss': focal_loss, 'focal_loss_fixed': focal_loss()})  \n",
    "    cnn2= load_model(path+str(prob)+'/cnn2_'+str(j+1)+'-exec_w4_app1_Top_'+str(prob)+'_1.h5', custom_objects={'FocalLoss': focal_loss, 'focal_loss_fixed': focal_loss()}) \n",
    "    rnn1= load_model(path+str(prob)+'/rnn1_'+str(j+1)+'-exec_w4_app1_Top_'+str(prob)+'_1.h5', custom_objects={'FocalLoss': focal_loss, 'focal_loss_fixed': focal_loss()}) \n",
    "    rnn2= load_model(path+str(prob)+'/rnn2_'+str(j+1)+'-exec_w4_app1_Top_'+str(prob)+'_1.h5', custom_objects={'FocalLoss': focal_loss, 'focal_loss_fixed': focal_loss()}) \n",
    "    rnn3= load_model(path+str(prob)+'/rnn3_'+str(j+1)+'-exec_w4_app1_Top_'+str(prob)+'_1.h5') \n",
    "    \n",
    "    list_models=['cnn1', 'cnn2', 'rnn1', 'rnn2', 'rnn3']\n",
    "    index_models=np.arange(5)\n",
    "    dict_models=dict((key, value) for (key, value) in zip(index_models,list_models))\n",
    "    modelos=[cnn1, cnn2, rnn1, rnn2, rnn3]\n",
    "    ind=np.arange(5)\n",
    "    dict_trainedModel=dict((key, value) for (key, value) in zip(ind,modelos))\n",
    "    \n",
    "    print (\"Agregando predicciones Val set\")\n",
    "    for i in dict_trainedModel.keys(): \n",
    "        a_evaluar=dict_models[i]\n",
    "        predicho=dict_trainedModel[i].predict(x_val, batch_size=bs)\n",
    "        predicho=[np.argmax(pred) for pred in predicho]\n",
    "        acc= accuracy_score(etiq_v, predicho)  \n",
    "        f1_ma=f1_score(etiq_v, predicho, average='macro')\n",
    "        f1_no=f1_score(etiq_v, predicho, average=None)\n",
    "        matriz=normalize(confusion_matrix(etiq_v, predicho))\n",
    "        fs_macro_val[dict_models[i]].append(f1_ma)\n",
    "        fs_none_val[dict_models[i]].append(f1_no)\n",
    "        accs_val[dict_models[i]].append(acc)\n",
    "        confusions_val[dict_models[i]].append(matriz)\n",
    "        \n",
    "    predicciones_all_val=[]\n",
    "    for i in dict_trainedModel.keys(): \n",
    "        a_evaluar=dict_models[i]\n",
    "        predicciones_all_val.append(dict_trainedModel[i].predict(x_val, batch_size=bs))\n",
    "    predichos_all_val=np.concatenate(np.asarray(predicciones_all_val),axis=-1)\n",
    "    \n",
    "    print (\"Agregando predicciones Test set\", dict_models[i])    \n",
    "    for i in dict_trainedModel.keys(): \n",
    "        a_evaluar=dict_models[i]\n",
    "        predicho=dict_trainedModel[i].predict(x_test, batch_size=bs)\n",
    "        predicho=[np.argmax(pred) for pred in predicho]\n",
    "        acc= accuracy_score(etiq_t, predicho)  \n",
    "        f1_ma=f1_score(etiq_t, predicho, average='macro')\n",
    "        f1_no=f1_score(etiq_t, predicho, average=None)\n",
    "        matriz=normalize(confusion_matrix(etiq_t, predicho))\n",
    "        fs_macro_test[dict_models[i]].append(f1_ma)\n",
    "        fs_none_test[dict_models[i]].append(f1_no)\n",
    "        accs_test[dict_models[i]].append(acc)\n",
    "        confusions_test[dict_models[i]].append(matriz)\n",
    "\n",
    "    predicciones_all_test=[]\n",
    "    bs=32\n",
    "    for i in dict_trainedModel.keys(): \n",
    "        a_evaluar=dict_models[i]\n",
    "        predicciones_all_test.append(dict_trainedModel[i].predict(x_test, batch_size=bs))\n",
    "    predichos_all_test=np.concatenate(np.asarray(predicciones_all_test),axis=-1)\n",
    "    \n",
    "    print (\"--------VALIDATION SET--------\")\n",
    "    \n",
    "    print (\"\\nCommittee Best Fit\")\n",
    "    trainPredicho, maxPredicho, confianzas = predecir_modelos(bs,predichos_all_val)\n",
    "    acc_comite= accuracy_score(etiq_v, trainPredicho)  \n",
    "    print(\"Accuracy:\",acc_comite)\n",
    "    f1=f1_score(etiq_v, trainPredicho, average='macro') \n",
    "    print(\"F1-macro:\",f1)\n",
    "    f1_no=f1_score(etiq_v, trainPredicho, average=None)  \n",
    "    print(\"F1-score SDQC:\",f1)\n",
    "    best['acc'].append(acc_comite)\n",
    "    best['macro'].append(f1)\n",
    "    best['none'].append(f1_no)\n",
    "    \n",
    "    print (\"\\nCommittee Norm\")\n",
    "    trainPredicho, maxPredicho, confianzas = predecir_modelos_norm(bs,predichos_all_val)\n",
    "    acc_comite= accuracy_score(etiq_v, trainPredicho)  \n",
    "    print(\"Accuracy:\",acc_comite)\n",
    "    f1=f1_score(etiq_v, trainPredicho, average='macro') \n",
    "    print(\"F1-macro:\",f1)\n",
    "    f1_no=f1_score(etiq_v, trainPredicho, average=None) \n",
    "    print(\"F1-score SDQC:\",f1)\n",
    "    norm['acc'].append(acc_comite)\n",
    "    norm['macro'].append(f1)\n",
    "    norm['none'].append(f1_no)\n",
    "\n",
    "    print (\"\\nCommittee Voting\")\n",
    "    trainPredicho, maxPredicho, confianzas = predecir_modelos_average(bs,predichos_all_val)\n",
    "    acc_comite= accuracy_score(etiq_v, trainPredicho)  \n",
    "    print(\"Accuracy:\",acc_comite)\n",
    "    f1=f1_score(etiq_v, trainPredicho, average='macro') \n",
    "    print(\"F1-macro:\",f1)\n",
    "    f1_no=f1_score(etiq_v, trainPredicho, average=None)  \n",
    "    print(\"F1-score SDQC:\",f1)\n",
    "    voting['acc'].append(acc_comite)\n",
    "    voting['macro'].append(f1)\n",
    "    voting['none'].append(f1_no)\n",
    "    \n",
    "    print (\"--------TESTING SET--------\")\n",
    "    \n",
    "    print (\"\\nCommittee Best Fit\")\n",
    "    trainPredicho, maxPredicho, confianzas = predecir_modelos(bs,predichos_all_test)\n",
    "    acc_comite= accuracy_score(etiq_t, trainPredicho)  \n",
    "    print(\"Accuracy:\",acc_comite)\n",
    "    f1=f1_score(etiq_t, trainPredicho, average='macro') \n",
    "    print(\"F1-macro:\",f1)\n",
    "    f1_no=f1_score(etiq_t, trainPredicho, average=None)  \n",
    "    print(\"F1-score SDQC:\",f1)\n",
    "    best_test['acc'].append(acc_comite)\n",
    "    best_test['macro'].append(f1)\n",
    "    best_test['none'].append(f1_no)\n",
    "\n",
    "    print (\"\\nCommittee Norm\")\n",
    "    trainPredicho, maxPredicho, confianzas = predecir_modelos_norm(bs,predichos_all_test)\n",
    "    acc_comite= accuracy_score(etiq_t, trainPredicho)  \n",
    "    print(\"Accuracy:\",acc_comite)\n",
    "    f1=f1_score(etiq_t, trainPredicho, average='macro') \n",
    "    print(\"F1-macro:\",f1)\n",
    "    f1_no=f1_score(etiq_t, trainPredicho, average=None) \n",
    "    print(\"F1-score SDQC:\",f1)\n",
    "    norm_test['acc'].append(acc_comite)\n",
    "    norm_test['macro'].append(f1)\n",
    "    norm_test['none'].append(f1_no)\n",
    "\n",
    "    print (\"\\nCommittee Voting\")\n",
    "    trainPredicho, maxPredicho, confianzas = predecir_modelos_average(bs,predichos_all_test)\n",
    "    acc_comite= accuracy_score(etiq_t, trainPredicho)  \n",
    "    print(\"Accuracy:\",acc_comite)\n",
    "    f1=f1_score(etiq_t, trainPredicho, average='macro') \n",
    "    print(\"F1-macro:\",f1)\n",
    "    f1_no=f1_score(etiq_t, trainPredicho, average=None)  \n",
    "    print(\"F1-score SDQC:\",f1)\n",
    "    voting_test['acc'].append(acc_comite)\n",
    "    voting_test['macro'].append(f1)\n",
    "    voting_test['none'].append(f1_no)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAL] Acc Best comité: 0.815435294117647\n",
      "[VAL] F1-score SDQC Best comité: [0.8816178  0.04871317 0.11390293 0.78105582]\n",
      "[VAL] F1 macro Best comité: 0.4563224292773483\n",
      "-----------\n",
      "[VAL] Acc Norm comité: 0.8769882352941176\n",
      "[VAL] F1-score SDQC Norm comité: [0.92541438 0.02099515 0.05005102 0.83055087]\n",
      "[VAL] F1 macro Norm comité: 0.4567528538419047\n",
      "-----------\n",
      "[VAL] Acc Voting comité: 0.8856470588235295\n",
      "[VAL] F1-score SDQC Voting comité: [0.93163373 0.02106519 0.04859474 0.84832898]\n",
      "[VAL] F1 macro Voting comité: 0.4624056588287239\n",
      "\n",
      "[TEST] Acc Best comité: 0.7708902496467264\n",
      "[TEST] F1-score SDQC Best comité: [0.86358491 0.07892092 0.06957839 0.61845676]\n",
      "[TEST] F1 macro Best comité: 0.407635245463999\n",
      "-----------\n",
      "[TEST] Acc Norm comité: 0.8309938765897316\n",
      "[TEST] F1-score SDQC Norm comité: [0.90303666 0.03559777 0.02039117 0.67873738]\n",
      "[TEST] F1 macro Norm comité: 0.4094407450035501\n",
      "-----------\n",
      "[TEST] Acc Voting comité: 0.8394724446537918\n",
      "[TEST] F1-score SDQC Voting comité: [0.90824274 0.04927884 0.03007598 0.69739377]\n",
      "[TEST] F1 macro Voting comité: 0.42124783411899913\n"
     ]
    }
   ],
   "source": [
    "print (\"[VAL] Acc Best comité:\",np.sum(np.asarray(best['acc'])/5.0))\n",
    "temp=np.zeros(4)\n",
    "for result in best['none']:\n",
    "    temp+=result\n",
    "print (\"[VAL] F1-score SDQC Best comité:\" ,temp/5)\n",
    "print (\"[VAL] F1 macro Best comité:\",np.sum(np.asarray(best['macro'])/5.0))\n",
    "print (\"-----------\")\n",
    "print (\"[VAL] Acc Norm comité:\",np.sum(np.asarray(norm['acc'])/5.0))\n",
    "temp=np.zeros(4)\n",
    "for result in norm['none']:\n",
    "    temp+=result\n",
    "print (\"[VAL] F1-score SDQC Norm comité:\" ,temp/5)\n",
    "print (\"[VAL] F1 macro Norm comité:\",np.sum(np.asarray(norm['macro'])/5.0))\n",
    "print (\"-----------\")\n",
    "print (\"[VAL] Acc Voting comité:\",np.sum(np.asarray(voting['acc'])/5.0))\n",
    "temp=np.zeros(4)\n",
    "for result in voting['none']:\n",
    "    temp+=result\n",
    "print (\"[VAL] F1-score SDQC Voting comité:\" ,temp/5)\n",
    "print (\"[VAL] F1 macro Voting comité:\",np.sum(np.asarray(voting['macro'])/5.0))\n",
    "\n",
    "print (\"\")\n",
    "\n",
    "print (\"[TEST] Acc Best comité:\",np.sum(np.asarray(best_test['acc'])/5.0))\n",
    "temp=np.zeros(4)\n",
    "for result in best_test['none']:\n",
    "    temp+=result\n",
    "print (\"[TEST] F1-score SDQC Best comité:\" ,temp/5)\n",
    "print (\"[TEST] F1 macro Best comité:\",np.sum(np.asarray(best_test['macro'])/5.0))\n",
    "print (\"-----------\")\n",
    "print (\"[TEST] Acc Norm comité:\",np.sum(np.asarray(norm_test['acc'])/5.0))\n",
    "temp=np.zeros(4)\n",
    "for result in norm_test['none']:\n",
    "    temp+=result\n",
    "print (\"[TEST] F1-score SDQC Norm comité:\" ,temp/5)\n",
    "print (\"[TEST] F1 macro Norm comité:\",np.sum(np.asarray(norm_test['macro'])/5.0))\n",
    "print (\"-----------\")\n",
    "print (\"[TEST] Acc Voting comité:\",np.sum(np.asarray(voting_test['acc'])/5.0))\n",
    "temp=np.zeros(4)\n",
    "for result in voting_test['none']:\n",
    "    temp+=result\n",
    "print (\"[TEST] F1-score SDQC Voting comité:\" ,temp/5)\n",
    "print (\"[TEST] F1 macro Voting comité:\",np.sum(np.asarray(voting_test['macro'])/5.0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3975311115881673"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(fs_macro_test['cnn2'])/5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augmentation 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------- EJECUCIÓN #1----------------\n",
      "Agregando predicciones Val set\n",
      "Agregando predicciones Test set rnn3\n",
      "--------VALIDATION SET--------\n",
      "\n",
      "Committee Best Fit\n",
      "Accuracy: 0.8202352941176471\n",
      "F1-macro: 0.45051219389033403\n",
      "F1-score SDQC: 0.45051219389033403\n",
      "\n",
      "Committee Norm\n",
      "Accuracy: 0.8611764705882353\n",
      "F1-macro: 0.4496277138087339\n",
      "F1-score SDQC: 0.4496277138087339\n",
      "\n",
      "Committee Voting\n",
      "Accuracy: 0.8729411764705882\n",
      "F1-macro: 0.4667060088822116\n",
      "F1-score SDQC: 0.4667060088822116\n",
      "--------TESTING SET--------\n",
      "\n",
      "Committee Best Fit\n",
      "Accuracy: 0.7508243052284503\n",
      "F1-macro: 0.38369145555527484\n",
      "F1-score SDQC: 0.38369145555527484\n",
      "\n",
      "Committee Norm\n",
      "Accuracy: 0.7974564295807819\n",
      "F1-macro: 0.3997421755382652\n",
      "F1-score SDQC: 0.3997421755382652\n",
      "\n",
      "Committee Voting\n",
      "Accuracy: 0.8162976919453604\n",
      "F1-macro: 0.4208996298713136\n",
      "F1-score SDQC: 0.4208996298713136\n",
      "\n",
      "---------------- EJECUCIÓN #2----------------\n",
      "Agregando predicciones Val set\n",
      "Agregando predicciones Test set rnn3\n",
      "--------VALIDATION SET--------\n",
      "\n",
      "Committee Best Fit\n",
      "Accuracy: 0.8442352941176471\n",
      "F1-macro: 0.43422644301662805\n",
      "F1-score SDQC: 0.43422644301662805\n",
      "\n",
      "Committee Norm\n",
      "Accuracy: 0.8602352941176471\n",
      "F1-macro: 0.4424465178762479\n",
      "F1-score SDQC: 0.4424465178762479\n",
      "\n",
      "Committee Voting\n",
      "Accuracy: 0.8734117647058823\n",
      "F1-macro: 0.4536095848595849\n",
      "F1-score SDQC: 0.4536095848595849\n",
      "--------TESTING SET--------\n",
      "\n",
      "Committee Best Fit\n",
      "Accuracy: 0.8106453132359868\n",
      "F1-macro: 0.3860974953008996\n",
      "F1-score SDQC: 0.3860974953008996\n",
      "\n",
      "Committee Norm\n",
      "Accuracy: 0.8200659444182761\n",
      "F1-macro: 0.3890566024837125\n",
      "F1-score SDQC: 0.3890566024837125\n",
      "\n",
      "Committee Voting\n",
      "Accuracy: 0.8313707018370231\n",
      "F1-macro: 0.4122394411121392\n",
      "F1-score SDQC: 0.4122394411121392\n",
      "\n",
      "---------------- EJECUCIÓN #3----------------\n",
      "Agregando predicciones Val set\n",
      "Agregando predicciones Test set rnn3\n",
      "--------VALIDATION SET--------\n",
      "\n",
      "Committee Best Fit\n",
      "Accuracy: 0.8371764705882353\n",
      "F1-macro: 0.4548701200539902\n",
      "F1-score SDQC: 0.4548701200539902\n",
      "\n",
      "Committee Norm\n",
      "Accuracy: 0.8748235294117647\n",
      "F1-macro: 0.45687710702340856\n",
      "F1-score SDQC: 0.45687710702340856\n",
      "\n",
      "Committee Voting\n",
      "Accuracy: 0.8870588235294118\n",
      "F1-macro: 0.476781384135916\n",
      "F1-score SDQC: 0.476781384135916\n",
      "--------TESTING SET--------\n",
      "\n",
      "Committee Best Fit\n",
      "Accuracy: 0.7880357983984927\n",
      "F1-macro: 0.4146012813871316\n",
      "F1-score SDQC: 0.4146012813871316\n",
      "\n",
      "Committee Norm\n",
      "Accuracy: 0.8228921337729628\n",
      "F1-macro: 0.39453784688478566\n",
      "F1-score SDQC: 0.39453784688478566\n",
      "\n",
      "Committee Voting\n",
      "Accuracy: 0.8280734809232219\n",
      "F1-macro: 0.40340090802834205\n",
      "F1-score SDQC: 0.40340090802834205\n",
      "\n",
      "---------------- EJECUCIÓN #4----------------\n",
      "Agregando predicciones Val set\n",
      "Agregando predicciones Test set rnn3\n",
      "--------VALIDATION SET--------\n",
      "\n",
      "Committee Best Fit\n",
      "Accuracy: 0.8724705882352941\n",
      "F1-macro: 0.4749919217127536\n",
      "F1-score SDQC: 0.4749919217127536\n",
      "\n",
      "Committee Norm\n",
      "Accuracy: 0.884235294117647\n",
      "F1-macro: 0.4808345144848005\n",
      "F1-score SDQC: 0.4808345144848005\n",
      "\n",
      "Committee Voting\n",
      "Accuracy: 0.8931764705882353\n",
      "F1-macro: 0.4816021416803954\n",
      "F1-score SDQC: 0.4816021416803954\n",
      "--------TESTING SET--------\n",
      "\n",
      "Committee Best Fit\n",
      "Accuracy: 0.8186528497409327\n",
      "F1-macro: 0.4111560541501545\n",
      "F1-score SDQC: 0.4111560541501545\n",
      "\n",
      "Committee Norm\n",
      "Accuracy: 0.8304286387187941\n",
      "F1-macro: 0.42336801233653826\n",
      "F1-score SDQC: 0.42336801233653826\n",
      "\n",
      "Committee Voting\n",
      "Accuracy: 0.8370230805463966\n",
      "F1-macro: 0.4295126211912502\n",
      "F1-score SDQC: 0.4295126211912502\n",
      "\n",
      "---------------- EJECUCIÓN #5----------------\n",
      "Agregando predicciones Val set\n",
      "Agregando predicciones Test set rnn3\n",
      "--------VALIDATION SET--------\n",
      "\n",
      "Committee Best Fit\n",
      "Accuracy: 0.864\n",
      "F1-macro: 0.46517661931411725\n",
      "F1-score SDQC: 0.46517661931411725\n",
      "\n",
      "Committee Norm\n",
      "Accuracy: 0.8752941176470588\n",
      "F1-macro: 0.46151009453820435\n",
      "F1-score SDQC: 0.46151009453820435\n",
      "\n",
      "Committee Voting\n",
      "Accuracy: 0.8809411764705882\n",
      "F1-macro: 0.4649245893093453\n",
      "F1-score SDQC: 0.4649245893093453\n",
      "--------TESTING SET--------\n",
      "\n",
      "Committee Best Fit\n",
      "Accuracy: 0.8087611869995289\n",
      "F1-macro: 0.3939095444882237\n",
      "F1-score SDQC: 0.3939095444882237\n",
      "\n",
      "Committee Norm\n",
      "Accuracy: 0.8238341968911918\n",
      "F1-macro: 0.39621084009893864\n",
      "F1-score SDQC: 0.39621084009893864\n",
      "\n",
      "Committee Voting\n",
      "Accuracy: 0.8304286387187941\n",
      "F1-macro: 0.401451009810464\n",
      "F1-score SDQC: 0.401451009810464\n"
     ]
    }
   ],
   "source": [
    "prob=0.5\n",
    "path='../Operador Data Augmentation/exec_baselines_harass/'\n",
    "bs=32\n",
    "    \n",
    "fs_macro_val={'cnn1':[], 'cnn2':[], 'rnn1':[],'rnn2':[],'rnn3':[]}\n",
    "fs_none_val={'cnn1':[], 'cnn2':[], 'rnn1':[],'rnn2':[],'rnn3':[]}\n",
    "accs_val={'cnn1':[], 'cnn2':[], 'rnn1':[],'rnn2':[],'rnn3':[]}\n",
    "confusions_val={'cnn1':[], 'cnn2':[], 'rnn1':[],'rnn2':[],'rnn3':[]}\n",
    "\n",
    "fs_macro_test={'cnn1':[], 'cnn2':[], 'rnn1':[],'rnn2':[],'rnn3':[]}\n",
    "fs_none_test={'cnn1':[], 'cnn2':[], 'rnn1':[],'rnn2':[],'rnn3':[]}\n",
    "accs_test={'cnn1':[], 'cnn2':[], 'rnn1':[],'rnn2':[],'rnn3':[]}\n",
    "confusions_test={'cnn1':[], 'cnn2':[], 'rnn1':[],'rnn2':[],'rnn3':[]}\n",
    "\n",
    "best={'acc':[], 'none':[], 'macro':[]}\n",
    "norm={'acc':[], 'none':[], 'macro':[]}\n",
    "voting={'acc':[], 'none':[], 'macro':[]}\n",
    "best_test={'acc':[], 'none':[], 'macro':[]}\n",
    "norm_test={'acc':[], 'none':[], 'macro':[]}\n",
    "voting_test={'acc':[], 'none':[], 'macro':[]}\n",
    "\n",
    "#exec_baselines/0.15/rnn1_'+str(i+1)+'-exec_w4_app1_Top_'+str(prob)+'_'+str(TOPN)+'.h5'\n",
    "#exec_baselines/SMOTE/cnn2_'+str(i+1)+'-exec_w4_app1.h5\n",
    "for j in range(5):\n",
    "    print (\"\")\n",
    "    print (\"---------------- EJECUCIÓN #\"+str(j+1)+'----------------')\n",
    "    cnn1= load_model(path+str(prob)+'/cnn1_'+str(j+1)+'-exec_w4_app1_Top_'+str(prob)+'_1.h5', custom_objects={'FocalLoss': focal_loss, 'focal_loss_fixed': focal_loss()})  \n",
    "    cnn2= load_model(path+str(prob)+'/cnn2_'+str(j+1)+'-exec_w4_app1_Top_'+str(prob)+'_1.h5', custom_objects={'FocalLoss': focal_loss, 'focal_loss_fixed': focal_loss()}) \n",
    "    rnn1= load_model(path+str(prob)+'/rnn1_'+str(j+1)+'-exec_w4_app1_Top_'+str(prob)+'_1.h5', custom_objects={'FocalLoss': focal_loss, 'focal_loss_fixed': focal_loss()}) \n",
    "    rnn2= load_model(path+str(prob)+'/rnn2_'+str(j+1)+'-exec_w4_app1_Top_'+str(prob)+'_1.h5', custom_objects={'FocalLoss': focal_loss, 'focal_loss_fixed': focal_loss()}) \n",
    "    rnn3= load_model(path+str(prob)+'/rnn3_'+str(j+1)+'-exec_w4_app1_Top_'+str(prob)+'_1.h5') \n",
    "    \n",
    "    list_models=['cnn1', 'cnn2', 'rnn1', 'rnn2', 'rnn3']\n",
    "    index_models=np.arange(5)\n",
    "    dict_models=dict((key, value) for (key, value) in zip(index_models,list_models))\n",
    "    modelos=[cnn1, cnn2, rnn1, rnn2, rnn3]\n",
    "    ind=np.arange(5)\n",
    "    dict_trainedModel=dict((key, value) for (key, value) in zip(ind,modelos))\n",
    "    \n",
    "    print (\"Agregando predicciones Val set\")\n",
    "    for i in dict_trainedModel.keys(): \n",
    "        a_evaluar=dict_models[i]\n",
    "        predicho=dict_trainedModel[i].predict(x_val, batch_size=bs)\n",
    "        predicho=[np.argmax(pred) for pred in predicho]\n",
    "        acc= accuracy_score(etiq_v, predicho)  \n",
    "        f1_ma=f1_score(etiq_v, predicho, average='macro')\n",
    "        f1_no=f1_score(etiq_v, predicho, average=None)\n",
    "        matriz=normalize(confusion_matrix(etiq_v, predicho))\n",
    "        fs_macro_val[dict_models[i]].append(f1_ma)\n",
    "        fs_none_val[dict_models[i]].append(f1_no)\n",
    "        accs_val[dict_models[i]].append(acc)\n",
    "        confusions_val[dict_models[i]].append(matriz)\n",
    "        \n",
    "    predicciones_all_val=[]\n",
    "    for i in dict_trainedModel.keys(): \n",
    "        a_evaluar=dict_models[i]\n",
    "        predicciones_all_val.append(dict_trainedModel[i].predict(x_val, batch_size=bs))\n",
    "    predichos_all_val=np.concatenate(np.asarray(predicciones_all_val),axis=-1)\n",
    "    \n",
    "    print (\"Agregando predicciones Test set\", dict_models[i])    \n",
    "    for i in dict_trainedModel.keys(): \n",
    "        a_evaluar=dict_models[i]\n",
    "        predicho=dict_trainedModel[i].predict(x_test, batch_size=bs)\n",
    "        predicho=[np.argmax(pred) for pred in predicho]\n",
    "        acc= accuracy_score(etiq_t, predicho)  \n",
    "        f1_ma=f1_score(etiq_t, predicho, average='macro')\n",
    "        f1_no=f1_score(etiq_t, predicho, average=None)\n",
    "        matriz=normalize(confusion_matrix(etiq_t, predicho))\n",
    "        fs_macro_test[dict_models[i]].append(f1_ma)\n",
    "        fs_none_test[dict_models[i]].append(f1_no)\n",
    "        accs_test[dict_models[i]].append(acc)\n",
    "        confusions_test[dict_models[i]].append(matriz)\n",
    "\n",
    "    predicciones_all_test=[]\n",
    "    bs=32\n",
    "    for i in dict_trainedModel.keys(): \n",
    "        a_evaluar=dict_models[i]\n",
    "        predicciones_all_test.append(dict_trainedModel[i].predict(x_test, batch_size=bs))\n",
    "    predichos_all_test=np.concatenate(np.asarray(predicciones_all_test),axis=-1)\n",
    "    \n",
    "    print (\"--------VALIDATION SET--------\")\n",
    "    \n",
    "    print (\"\\nCommittee Best Fit\")\n",
    "    trainPredicho, maxPredicho, confianzas = predecir_modelos(bs,predichos_all_val)\n",
    "    acc_comite= accuracy_score(etiq_v, trainPredicho)  \n",
    "    print(\"Accuracy:\",acc_comite)\n",
    "    f1=f1_score(etiq_v, trainPredicho, average='macro') \n",
    "    print(\"F1-macro:\",f1)\n",
    "    f1_no=f1_score(etiq_v, trainPredicho, average=None)  \n",
    "    print(\"F1-score SDQC:\",f1)\n",
    "    best['acc'].append(acc_comite)\n",
    "    best['macro'].append(f1)\n",
    "    best['none'].append(f1_no)\n",
    "    \n",
    "    print (\"\\nCommittee Norm\")\n",
    "    trainPredicho, maxPredicho, confianzas = predecir_modelos_norm(bs,predichos_all_val)\n",
    "    acc_comite= accuracy_score(etiq_v, trainPredicho)  \n",
    "    print(\"Accuracy:\",acc_comite)\n",
    "    f1=f1_score(etiq_v, trainPredicho, average='macro') \n",
    "    print(\"F1-macro:\",f1)\n",
    "    f1_no=f1_score(etiq_v, trainPredicho, average=None) \n",
    "    print(\"F1-score SDQC:\",f1)\n",
    "    norm['acc'].append(acc_comite)\n",
    "    norm['macro'].append(f1)\n",
    "    norm['none'].append(f1_no)\n",
    "\n",
    "    print (\"\\nCommittee Voting\")\n",
    "    trainPredicho, maxPredicho, confianzas = predecir_modelos_average(bs,predichos_all_val)\n",
    "    acc_comite= accuracy_score(etiq_v, trainPredicho)  \n",
    "    print(\"Accuracy:\",acc_comite)\n",
    "    f1=f1_score(etiq_v, trainPredicho, average='macro') \n",
    "    print(\"F1-macro:\",f1)\n",
    "    f1_no=f1_score(etiq_v, trainPredicho, average=None)  \n",
    "    print(\"F1-score SDQC:\",f1)\n",
    "    voting['acc'].append(acc_comite)\n",
    "    voting['macro'].append(f1)\n",
    "    voting['none'].append(f1_no)\n",
    "    \n",
    "    print (\"--------TESTING SET--------\")\n",
    "    \n",
    "    print (\"\\nCommittee Best Fit\")\n",
    "    trainPredicho, maxPredicho, confianzas = predecir_modelos(bs,predichos_all_test)\n",
    "    acc_comite= accuracy_score(etiq_t, trainPredicho)  \n",
    "    print(\"Accuracy:\",acc_comite)\n",
    "    f1=f1_score(etiq_t, trainPredicho, average='macro') \n",
    "    print(\"F1-macro:\",f1)\n",
    "    f1_no=f1_score(etiq_t, trainPredicho, average=None)  \n",
    "    print(\"F1-score SDQC:\",f1)\n",
    "    best_test['acc'].append(acc_comite)\n",
    "    best_test['macro'].append(f1)\n",
    "    best_test['none'].append(f1_no)\n",
    "\n",
    "    print (\"\\nCommittee Norm\")\n",
    "    trainPredicho, maxPredicho, confianzas = predecir_modelos_norm(bs,predichos_all_test)\n",
    "    acc_comite= accuracy_score(etiq_t, trainPredicho)  \n",
    "    print(\"Accuracy:\",acc_comite)\n",
    "    f1=f1_score(etiq_t, trainPredicho, average='macro') \n",
    "    print(\"F1-macro:\",f1)\n",
    "    f1_no=f1_score(etiq_t, trainPredicho, average=None) \n",
    "    print(\"F1-score SDQC:\",f1)\n",
    "    norm_test['acc'].append(acc_comite)\n",
    "    norm_test['macro'].append(f1)\n",
    "    norm_test['none'].append(f1_no)\n",
    "\n",
    "    print (\"\\nCommittee Voting\")\n",
    "    trainPredicho, maxPredicho, confianzas = predecir_modelos_average(bs,predichos_all_test)\n",
    "    acc_comite= accuracy_score(etiq_t, trainPredicho)  \n",
    "    print(\"Accuracy:\",acc_comite)\n",
    "    f1=f1_score(etiq_t, trainPredicho, average='macro') \n",
    "    print(\"F1-macro:\",f1)\n",
    "    f1_no=f1_score(etiq_t, trainPredicho, average=None)  \n",
    "    print(\"F1-score SDQC:\",f1)\n",
    "    voting_test['acc'].append(acc_comite)\n",
    "    voting_test['macro'].append(f1)\n",
    "    voting_test['none'].append(f1_no)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAL] Acc Best comité: 0.8476235294117647\n",
      "[VAL] F1-score SDQC Best comité: [0.90622688 0.03528168 0.09238461 0.78992867]\n",
      "[VAL] F1 macro Best comité: 0.45595545959756456\n",
      "-----------\n",
      "[VAL] Acc Norm comité: 0.8711529411764705\n",
      "[VAL] F1-score SDQC Norm comité: [0.92170342 0.02696286 0.06714205 0.81722844]\n",
      "[VAL] F1 macro Norm comité: 0.458259189546279\n",
      "-----------\n",
      "[VAL] Acc Voting comité: 0.8815058823529413\n",
      "[VAL] F1-score SDQC Voting comité: [0.92933741 0.04213997 0.0661053  0.83731628]\n",
      "[VAL] F1 macro Voting comité: 0.46872474177349066\n",
      "\n",
      "[TEST] Acc Best comité: 0.7953838907206783\n",
      "[TEST] F1-score SDQC Best comité: [0.88310553 0.05065818 0.04864069 0.60916026]\n",
      "[TEST] F1 macro Best comité: 0.39789116617633685\n",
      "-----------\n",
      "[TEST] Acc Norm comité: 0.8189354686764013\n",
      "[TEST] F1-score SDQC Norm comité: [0.89785981 0.04231314 0.01904762 0.64311181]\n",
      "[TEST] F1 macro Norm comité: 0.40058309546844806\n",
      "-----------\n",
      "[TEST] Acc Voting comité: 0.8286387187941593\n",
      "[TEST] F1-score SDQC Voting comité: [0.90355939 0.04902178 0.02898756 0.67243416]\n",
      "[TEST] F1 macro Voting comité: 0.4135007220027018\n"
     ]
    }
   ],
   "source": [
    "print (\"[VAL] Acc Best comité:\",np.sum(np.asarray(best['acc'])/5.0))\n",
    "temp=np.zeros(4)\n",
    "for result in best['none']:\n",
    "    temp+=result\n",
    "print (\"[VAL] F1-score SDQC Best comité:\" ,temp/5)\n",
    "print (\"[VAL] F1 macro Best comité:\",np.sum(np.asarray(best['macro'])/5.0))\n",
    "print (\"-----------\")\n",
    "print (\"[VAL] Acc Norm comité:\",np.sum(np.asarray(norm['acc'])/5.0))\n",
    "temp=np.zeros(4)\n",
    "for result in norm['none']:\n",
    "    temp+=result\n",
    "print (\"[VAL] F1-score SDQC Norm comité:\" ,temp/5)\n",
    "print (\"[VAL] F1 macro Norm comité:\",np.sum(np.asarray(norm['macro'])/5.0))\n",
    "print (\"-----------\")\n",
    "print (\"[VAL] Acc Voting comité:\",np.sum(np.asarray(voting['acc'])/5.0))\n",
    "temp=np.zeros(4)\n",
    "for result in voting['none']:\n",
    "    temp+=result\n",
    "print (\"[VAL] F1-score SDQC Voting comité:\" ,temp/5)\n",
    "print (\"[VAL] F1 macro Voting comité:\",np.sum(np.asarray(voting['macro'])/5.0))\n",
    "\n",
    "print (\"\")\n",
    "\n",
    "print (\"[TEST] Acc Best comité:\",np.sum(np.asarray(best_test['acc'])/5.0))\n",
    "temp=np.zeros(4)\n",
    "for result in best_test['none']:\n",
    "    temp+=result\n",
    "print (\"[TEST] F1-score SDQC Best comité:\" ,temp/5)\n",
    "print (\"[TEST] F1 macro Best comité:\",np.sum(np.asarray(best_test['macro'])/5.0))\n",
    "print (\"-----------\")\n",
    "print (\"[TEST] Acc Norm comité:\",np.sum(np.asarray(norm_test['acc'])/5.0))\n",
    "temp=np.zeros(4)\n",
    "for result in norm_test['none']:\n",
    "    temp+=result\n",
    "print (\"[TEST] F1-score SDQC Norm comité:\" ,temp/5)\n",
    "print (\"[TEST] F1 macro Norm comité:\",np.sum(np.asarray(norm_test['macro'])/5.0))\n",
    "print (\"-----------\")\n",
    "print (\"[TEST] Acc Voting comité:\",np.sum(np.asarray(voting_test['acc'])/5.0))\n",
    "temp=np.zeros(4)\n",
    "for result in voting_test['none']:\n",
    "    temp+=result\n",
    "print (\"[TEST] F1-score SDQC Voting comité:\" ,temp/5)\n",
    "print (\"[TEST] F1 macro Voting comité:\",np.sum(np.asarray(voting_test['macro'])/5.0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.40812622341881843"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(fs_macro_test['cnn2'])/5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augmentation 0.85"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------- EJECUCIÓN #1----------------\n",
      "Agregando predicciones Val set\n",
      "Agregando predicciones Test set rnn3\n",
      "--------VALIDATION SET--------\n",
      "\n",
      "Committee Best Fit\n",
      "Accuracy: 0.8672941176470588\n",
      "F1-macro: 0.4443142865487368\n",
      "F1-score SDQC: 0.4443142865487368\n",
      "\n",
      "Committee Norm\n",
      "Accuracy: 0.8818823529411765\n",
      "F1-macro: 0.45330817176540916\n",
      "F1-score SDQC: 0.45330817176540916\n",
      "\n",
      "Committee Voting\n",
      "Accuracy: 0.8804705882352941\n",
      "F1-macro: 0.45188218221567067\n",
      "F1-score SDQC: 0.45188218221567067\n",
      "--------TESTING SET--------\n",
      "\n",
      "Committee Best Fit\n",
      "Accuracy: 0.802637776731041\n",
      "F1-macro: 0.39937751253523657\n",
      "F1-score SDQC: 0.39937751253523657\n",
      "\n",
      "Committee Norm\n",
      "Accuracy: 0.8214790390956194\n",
      "F1-macro: 0.40853028010480863\n",
      "F1-score SDQC: 0.40853028010480863\n",
      "\n",
      "Committee Voting\n",
      "Accuracy: 0.8313707018370231\n",
      "F1-macro: 0.4003174588906891\n",
      "F1-score SDQC: 0.4003174588906891\n",
      "\n",
      "---------------- EJECUCIÓN #2----------------\n",
      "Agregando predicciones Val set\n",
      "Agregando predicciones Test set rnn3\n",
      "--------VALIDATION SET--------\n",
      "\n",
      "Committee Best Fit\n",
      "Accuracy: 0.832\n",
      "F1-macro: 0.4316229085897609\n",
      "F1-score SDQC: 0.4316229085897609\n",
      "\n",
      "Committee Norm\n",
      "Accuracy: 0.8705882352941177\n",
      "F1-macro: 0.44762768118852814\n",
      "F1-score SDQC: 0.44762768118852814\n",
      "\n",
      "Committee Voting\n",
      "Accuracy: 0.8762352941176471\n",
      "F1-macro: 0.4514755907296082\n",
      "F1-score SDQC: 0.4514755907296082\n",
      "--------TESTING SET--------\n",
      "\n",
      "Committee Best Fit\n",
      "Accuracy: 0.7616580310880829\n",
      "F1-macro: 0.37186203565036047\n",
      "F1-score SDQC: 0.37186203565036047\n",
      "\n",
      "Committee Norm\n",
      "Accuracy: 0.8092322185586435\n",
      "F1-macro: 0.39965763267710186\n",
      "F1-score SDQC: 0.39965763267710186\n",
      "\n",
      "Committee Voting\n",
      "Accuracy: 0.8153556288271314\n",
      "F1-macro: 0.3971140160113445\n",
      "F1-score SDQC: 0.3971140160113445\n",
      "\n",
      "---------------- EJECUCIÓN #3----------------\n",
      "Agregando predicciones Val set\n",
      "Agregando predicciones Test set rnn3\n",
      "--------VALIDATION SET--------\n",
      "\n",
      "Committee Best Fit\n",
      "Accuracy: 0.8334117647058824\n",
      "F1-macro: 0.4606032547736011\n",
      "F1-score SDQC: 0.4606032547736011\n",
      "\n",
      "Committee Norm\n",
      "Accuracy: 0.872\n",
      "F1-macro: 0.45410419427676263\n",
      "F1-score SDQC: 0.45410419427676263\n",
      "\n",
      "Committee Voting\n",
      "Accuracy: 0.8785882352941177\n",
      "F1-macro: 0.4580037794998429\n",
      "F1-score SDQC: 0.4580037794998429\n",
      "--------TESTING SET--------\n",
      "\n",
      "Committee Best Fit\n",
      "Accuracy: 0.7715496938294866\n",
      "F1-macro: 0.4217458970005659\n",
      "F1-score SDQC: 0.4217458970005659\n",
      "\n",
      "Committee Norm\n",
      "Accuracy: 0.8158266603862458\n",
      "F1-macro: 0.4009033994918138\n",
      "F1-score SDQC: 0.4009033994918138\n",
      "\n",
      "Committee Voting\n",
      "Accuracy: 0.8271314178049929\n",
      "F1-macro: 0.40911222486470245\n",
      "F1-score SDQC: 0.40911222486470245\n",
      "\n",
      "---------------- EJECUCIÓN #4----------------\n",
      "Agregando predicciones Val set\n",
      "Agregando predicciones Test set rnn3\n",
      "--------VALIDATION SET--------\n",
      "\n",
      "Committee Best Fit\n",
      "Accuracy: 0.8432941176470589\n",
      "F1-macro: 0.46956167056000186\n",
      "F1-score SDQC: 0.46956167056000186\n",
      "\n",
      "Committee Norm\n",
      "Accuracy: 0.8776470588235294\n",
      "F1-macro: 0.4583517826433361\n",
      "F1-score SDQC: 0.4583517826433361\n",
      "\n",
      "Committee Voting\n",
      "Accuracy: 0.8837647058823529\n",
      "F1-macro: 0.46778175622039464\n",
      "F1-score SDQC: 0.46778175622039464\n",
      "--------TESTING SET--------\n",
      "\n",
      "Committee Best Fit\n",
      "Accuracy: 0.7899199246349505\n",
      "F1-macro: 0.40937322725443764\n",
      "F1-score SDQC: 0.40937322725443764\n",
      "\n",
      "Committee Norm\n",
      "Accuracy: 0.8271314178049929\n",
      "F1-macro: 0.4112084028955428\n",
      "F1-score SDQC: 0.4112084028955428\n",
      "\n",
      "Committee Voting\n",
      "Accuracy: 0.8285445124823363\n",
      "F1-macro: 0.4112929382279228\n",
      "F1-score SDQC: 0.4112929382279228\n",
      "\n",
      "---------------- EJECUCIÓN #5----------------\n",
      "Agregando predicciones Val set\n",
      "Agregando predicciones Test set rnn3\n",
      "--------VALIDATION SET--------\n",
      "\n",
      "Committee Best Fit\n",
      "Accuracy: 0.827764705882353\n",
      "F1-macro: 0.45730616231784316\n",
      "F1-score SDQC: 0.45730616231784316\n",
      "\n",
      "Committee Norm\n",
      "Accuracy: 0.8550588235294118\n",
      "F1-macro: 0.4402499634191348\n",
      "F1-score SDQC: 0.4402499634191348\n",
      "\n",
      "Committee Voting\n",
      "Accuracy: 0.8649411764705882\n",
      "F1-macro: 0.4515862745783845\n",
      "F1-score SDQC: 0.4515862745783845\n",
      "--------TESTING SET--------\n",
      "\n",
      "Committee Best Fit\n",
      "Accuracy: 0.7470560527555347\n",
      "F1-macro: 0.3885970936154084\n",
      "F1-score SDQC: 0.3885970936154084\n",
      "\n",
      "Committee Norm\n",
      "Accuracy: 0.7847385774846914\n",
      "F1-macro: 0.39186260041579624\n",
      "F1-score SDQC: 0.39186260041579624\n",
      "\n",
      "Committee Voting\n",
      "Accuracy: 0.8031088082901554\n",
      "F1-macro: 0.39761094224924015\n",
      "F1-score SDQC: 0.39761094224924015\n"
     ]
    }
   ],
   "source": [
    "prob=0.85\n",
    "path='../Operador Data Augmentation/exec_baselines_harass/'\n",
    "bs=32\n",
    "    \n",
    "fs_macro_val={'cnn1':[], 'cnn2':[], 'rnn1':[],'rnn2':[],'rnn3':[]}\n",
    "fs_none_val={'cnn1':[], 'cnn2':[], 'rnn1':[],'rnn2':[],'rnn3':[]}\n",
    "accs_val={'cnn1':[], 'cnn2':[], 'rnn1':[],'rnn2':[],'rnn3':[]}\n",
    "confusions_val={'cnn1':[], 'cnn2':[], 'rnn1':[],'rnn2':[],'rnn3':[]}\n",
    "\n",
    "fs_macro_test={'cnn1':[], 'cnn2':[], 'rnn1':[],'rnn2':[],'rnn3':[]}\n",
    "fs_none_test={'cnn1':[], 'cnn2':[], 'rnn1':[],'rnn2':[],'rnn3':[]}\n",
    "accs_test={'cnn1':[], 'cnn2':[], 'rnn1':[],'rnn2':[],'rnn3':[]}\n",
    "confusions_test={'cnn1':[], 'cnn2':[], 'rnn1':[],'rnn2':[],'rnn3':[]}\n",
    "\n",
    "best={'acc':[], 'none':[], 'macro':[]}\n",
    "norm={'acc':[], 'none':[], 'macro':[]}\n",
    "voting={'acc':[], 'none':[], 'macro':[]}\n",
    "best_test={'acc':[], 'none':[], 'macro':[]}\n",
    "norm_test={'acc':[], 'none':[], 'macro':[]}\n",
    "voting_test={'acc':[], 'none':[], 'macro':[]}\n",
    "\n",
    "#exec_baselines/0.15/rnn1_'+str(i+1)+'-exec_w4_app1_Top_'+str(prob)+'_'+str(TOPN)+'.h5'\n",
    "#exec_baselines/SMOTE/cnn2_'+str(i+1)+'-exec_w4_app1.h5\n",
    "for j in range(5):\n",
    "    print (\"\")\n",
    "    print (\"---------------- EJECUCIÓN #\"+str(j+1)+'----------------')\n",
    "    cnn1= load_model(path+str(prob)+'/cnn1_'+str(j+1)+'-exec_w4_app1_Top_'+str(prob)+'_1.h5', custom_objects={'FocalLoss': focal_loss, 'focal_loss_fixed': focal_loss()})  \n",
    "    cnn2= load_model(path+str(prob)+'/cnn2_'+str(j+1)+'-exec_w4_app1_Top_'+str(prob)+'_1.h5', custom_objects={'FocalLoss': focal_loss, 'focal_loss_fixed': focal_loss()}) \n",
    "    rnn1= load_model(path+str(prob)+'/rnn1_'+str(j+1)+'-exec_w4_app1_Top_'+str(prob)+'_1.h5', custom_objects={'FocalLoss': focal_loss, 'focal_loss_fixed': focal_loss()}) \n",
    "    rnn2= load_model(path+str(prob)+'/rnn2_'+str(j+1)+'-exec_w4_app1_Top_'+str(prob)+'_1.h5', custom_objects={'FocalLoss': focal_loss, 'focal_loss_fixed': focal_loss()}) \n",
    "    rnn3= load_model(path+str(prob)+'/rnn3_'+str(j+1)+'-exec_w4_app1_Top_'+str(prob)+'_1.h5') \n",
    "    \n",
    "    list_models=['cnn1', 'cnn2', 'rnn1', 'rnn2', 'rnn3']\n",
    "    index_models=np.arange(5)\n",
    "    dict_models=dict((key, value) for (key, value) in zip(index_models,list_models))\n",
    "    modelos=[cnn1, cnn2, rnn1, rnn2, rnn3]\n",
    "    ind=np.arange(5)\n",
    "    dict_trainedModel=dict((key, value) for (key, value) in zip(ind,modelos))\n",
    "    \n",
    "    print (\"Agregando predicciones Val set\")\n",
    "    for i in dict_trainedModel.keys(): \n",
    "        a_evaluar=dict_models[i]\n",
    "        predicho=dict_trainedModel[i].predict(x_val, batch_size=bs)\n",
    "        predicho=[np.argmax(pred) for pred in predicho]\n",
    "        acc= accuracy_score(etiq_v, predicho)  \n",
    "        f1_ma=f1_score(etiq_v, predicho, average='macro')\n",
    "        f1_no=f1_score(etiq_v, predicho, average=None)\n",
    "        matriz=normalize(confusion_matrix(etiq_v, predicho))\n",
    "        fs_macro_val[dict_models[i]].append(f1_ma)\n",
    "        fs_none_val[dict_models[i]].append(f1_no)\n",
    "        accs_val[dict_models[i]].append(acc)\n",
    "        confusions_val[dict_models[i]].append(matriz)\n",
    "        \n",
    "    predicciones_all_val=[]\n",
    "    for i in dict_trainedModel.keys(): \n",
    "        a_evaluar=dict_models[i]\n",
    "        predicciones_all_val.append(dict_trainedModel[i].predict(x_val, batch_size=bs))\n",
    "    predichos_all_val=np.concatenate(np.asarray(predicciones_all_val),axis=-1)\n",
    "    \n",
    "    print (\"Agregando predicciones Test set\", dict_models[i])    \n",
    "    for i in dict_trainedModel.keys(): \n",
    "        a_evaluar=dict_models[i]\n",
    "        predicho=dict_trainedModel[i].predict(x_test, batch_size=bs)\n",
    "        predicho=[np.argmax(pred) for pred in predicho]\n",
    "        acc= accuracy_score(etiq_t, predicho)  \n",
    "        f1_ma=f1_score(etiq_t, predicho, average='macro')\n",
    "        f1_no=f1_score(etiq_t, predicho, average=None)\n",
    "        matriz=normalize(confusion_matrix(etiq_t, predicho))\n",
    "        fs_macro_test[dict_models[i]].append(f1_ma)\n",
    "        fs_none_test[dict_models[i]].append(f1_no)\n",
    "        accs_test[dict_models[i]].append(acc)\n",
    "        confusions_test[dict_models[i]].append(matriz)\n",
    "\n",
    "    predicciones_all_test=[]\n",
    "    bs=32\n",
    "    for i in dict_trainedModel.keys(): \n",
    "        a_evaluar=dict_models[i]\n",
    "        predicciones_all_test.append(dict_trainedModel[i].predict(x_test, batch_size=bs))\n",
    "    predichos_all_test=np.concatenate(np.asarray(predicciones_all_test),axis=-1)\n",
    "    \n",
    "    print (\"--------VALIDATION SET--------\")\n",
    "    \n",
    "    print (\"\\nCommittee Best Fit\")\n",
    "    trainPredicho, maxPredicho, confianzas = predecir_modelos(bs,predichos_all_val)\n",
    "    acc_comite= accuracy_score(etiq_v, trainPredicho)  \n",
    "    print(\"Accuracy:\",acc_comite)\n",
    "    f1=f1_score(etiq_v, trainPredicho, average='macro') \n",
    "    print(\"F1-macro:\",f1)\n",
    "    f1_no=f1_score(etiq_v, trainPredicho, average=None)  \n",
    "    print(\"F1-score SDQC:\",f1)\n",
    "    best['acc'].append(acc_comite)\n",
    "    best['macro'].append(f1)\n",
    "    best['none'].append(f1_no)\n",
    "    \n",
    "    print (\"\\nCommittee Norm\")\n",
    "    trainPredicho, maxPredicho, confianzas = predecir_modelos_norm(bs,predichos_all_val)\n",
    "    acc_comite= accuracy_score(etiq_v, trainPredicho)  \n",
    "    print(\"Accuracy:\",acc_comite)\n",
    "    f1=f1_score(etiq_v, trainPredicho, average='macro') \n",
    "    print(\"F1-macro:\",f1)\n",
    "    f1_no=f1_score(etiq_v, trainPredicho, average=None) \n",
    "    print(\"F1-score SDQC:\",f1)\n",
    "    norm['acc'].append(acc_comite)\n",
    "    norm['macro'].append(f1)\n",
    "    norm['none'].append(f1_no)\n",
    "\n",
    "    print (\"\\nCommittee Voting\")\n",
    "    trainPredicho, maxPredicho, confianzas = predecir_modelos_average(bs,predichos_all_val)\n",
    "    acc_comite= accuracy_score(etiq_v, trainPredicho)  \n",
    "    print(\"Accuracy:\",acc_comite)\n",
    "    f1=f1_score(etiq_v, trainPredicho, average='macro') \n",
    "    print(\"F1-macro:\",f1)\n",
    "    f1_no=f1_score(etiq_v, trainPredicho, average=None)  \n",
    "    print(\"F1-score SDQC:\",f1)\n",
    "    voting['acc'].append(acc_comite)\n",
    "    voting['macro'].append(f1)\n",
    "    voting['none'].append(f1_no)\n",
    "    \n",
    "    print (\"--------TESTING SET--------\")\n",
    "    \n",
    "    print (\"\\nCommittee Best Fit\")\n",
    "    trainPredicho, maxPredicho, confianzas = predecir_modelos(bs,predichos_all_test)\n",
    "    acc_comite= accuracy_score(etiq_t, trainPredicho)  \n",
    "    print(\"Accuracy:\",acc_comite)\n",
    "    f1=f1_score(etiq_t, trainPredicho, average='macro') \n",
    "    print(\"F1-macro:\",f1)\n",
    "    f1_no=f1_score(etiq_t, trainPredicho, average=None)  \n",
    "    print(\"F1-score SDQC:\",f1)\n",
    "    best_test['acc'].append(acc_comite)\n",
    "    best_test['macro'].append(f1)\n",
    "    best_test['none'].append(f1_no)\n",
    "\n",
    "    print (\"\\nCommittee Norm\")\n",
    "    trainPredicho, maxPredicho, confianzas = predecir_modelos_norm(bs,predichos_all_test)\n",
    "    acc_comite= accuracy_score(etiq_t, trainPredicho)  \n",
    "    print(\"Accuracy:\",acc_comite)\n",
    "    f1=f1_score(etiq_t, trainPredicho, average='macro') \n",
    "    print(\"F1-macro:\",f1)\n",
    "    f1_no=f1_score(etiq_t, trainPredicho, average=None) \n",
    "    print(\"F1-score SDQC:\",f1)\n",
    "    norm_test['acc'].append(acc_comite)\n",
    "    norm_test['macro'].append(f1)\n",
    "    norm_test['none'].append(f1_no)\n",
    "\n",
    "    print (\"\\nCommittee Voting\")\n",
    "    trainPredicho, maxPredicho, confianzas = predecir_modelos_average(bs,predichos_all_test)\n",
    "    acc_comite= accuracy_score(etiq_t, trainPredicho)  \n",
    "    print(\"Accuracy:\",acc_comite)\n",
    "    f1=f1_score(etiq_t, trainPredicho, average='macro') \n",
    "    print(\"F1-macro:\",f1)\n",
    "    f1_no=f1_score(etiq_t, trainPredicho, average=None)  \n",
    "    print(\"F1-score SDQC:\",f1)\n",
    "    voting_test['acc'].append(acc_comite)\n",
    "    voting_test['macro'].append(f1)\n",
    "    voting_test['none'].append(f1_no)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAL] Acc Best comité: 0.8407529411764706\n",
      "[VAL] F1-score SDQC Best comité: [0.9015838  0.0447579  0.07533924 0.78904568]\n",
      "[VAL] F1 macro Best comité: 0.45268165655798875\n",
      "-----------\n",
      "[VAL] Acc Norm comité: 0.8714352941176471\n",
      "[VAL] F1-score SDQC Norm comité: [0.92238185 0.01067426 0.04823197 0.82162536]\n",
      "[VAL] F1 macro Norm comité: 0.4507283586586342\n",
      "-----------\n",
      "[VAL] Acc Voting comité: 0.8768\n",
      "[VAL] F1-score SDQC Voting comité: [0.9267127  0.02033333 0.04742924 0.83010839]\n",
      "[VAL] F1 macro Voting comité: 0.4561459166487801\n",
      "\n",
      "[TEST] Acc Best comité: 0.7745642958078192\n",
      "[TEST] F1-score SDQC Best comité: [0.86851517 0.08654561 0.03896219 0.59874164]\n",
      "[TEST] F1 macro Best comité: 0.3981911532112018\n",
      "-----------\n",
      "[TEST] Acc Norm comité: 0.8116815826660386\n",
      "[TEST] F1-score SDQC Norm comité: [0.89237518 0.06032783 0.0101915  0.64683535]\n",
      "[TEST] F1 macro Norm comité: 0.4024324631170127\n",
      "-----------\n",
      "[TEST] Acc Voting comité: 0.8211022138483278\n",
      "[TEST] F1-score SDQC Voting comité: [0.89812982 0.05373344 0.         0.6604948 ]\n",
      "[TEST] F1 macro Voting comité: 0.40308951604877985\n"
     ]
    }
   ],
   "source": [
    "print (\"[VAL] Acc Best comité:\",np.sum(np.asarray(best['acc'])/5.0))\n",
    "temp=np.zeros(4)\n",
    "for result in best['none']:\n",
    "    temp+=result\n",
    "print (\"[VAL] F1-score SDQC Best comité:\" ,temp/5)\n",
    "print (\"[VAL] F1 macro Best comité:\",np.sum(np.asarray(best['macro'])/5.0))\n",
    "print (\"-----------\")\n",
    "print (\"[VAL] Acc Norm comité:\",np.sum(np.asarray(norm['acc'])/5.0))\n",
    "temp=np.zeros(4)\n",
    "for result in norm['none']:\n",
    "    temp+=result\n",
    "print (\"[VAL] F1-score SDQC Norm comité:\" ,temp/5)\n",
    "print (\"[VAL] F1 macro Norm comité:\",np.sum(np.asarray(norm['macro'])/5.0))\n",
    "print (\"-----------\")\n",
    "print (\"[VAL] Acc Voting comité:\",np.sum(np.asarray(voting['acc'])/5.0))\n",
    "temp=np.zeros(4)\n",
    "for result in voting['none']:\n",
    "    temp+=result\n",
    "print (\"[VAL] F1-score SDQC Voting comité:\" ,temp/5)\n",
    "print (\"[VAL] F1 macro Voting comité:\",np.sum(np.asarray(voting['macro'])/5.0))\n",
    "\n",
    "print (\"\")\n",
    "\n",
    "print (\"[TEST] Acc Best comité:\",np.sum(np.asarray(best_test['acc'])/5.0))\n",
    "temp=np.zeros(4)\n",
    "for result in best_test['none']:\n",
    "    temp+=result\n",
    "print (\"[TEST] F1-score SDQC Best comité:\" ,temp/5)\n",
    "print (\"[TEST] F1 macro Best comité:\",np.sum(np.asarray(best_test['macro'])/5.0))\n",
    "print (\"-----------\")\n",
    "print (\"[TEST] Acc Norm comité:\",np.sum(np.asarray(norm_test['acc'])/5.0))\n",
    "temp=np.zeros(4)\n",
    "for result in norm_test['none']:\n",
    "    temp+=result\n",
    "print (\"[TEST] F1-score SDQC Norm comité:\" ,temp/5)\n",
    "print (\"[TEST] F1 macro Norm comité:\",np.sum(np.asarray(norm_test['macro'])/5.0))\n",
    "print (\"-----------\")\n",
    "print (\"[TEST] Acc Voting comité:\",np.sum(np.asarray(voting_test['acc'])/5.0))\n",
    "temp=np.zeros(4)\n",
    "for result in voting_test['none']:\n",
    "    temp+=result\n",
    "print (\"[TEST] F1-score SDQC Voting comité:\" ,temp/5)\n",
    "print (\"[TEST] F1 macro Voting comité:\",np.sum(np.asarray(voting_test['macro'])/5.0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.40397515043187593"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(fs_macro_test['cnn2'])/5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SMOTE\n",
    "shape_val=np.asarray(input_ids_val).shape\n",
    "x_val=np.zeros((shape_val[0], 768))\n",
    "i=0\n",
    "for in_id in input_ids_val:\n",
    "    vector=np.zeros(768)\n",
    "    n = np.sum(in_id != 0)\n",
    "    for idv in in_id:\n",
    "        if idv!=0:\n",
    "            vector+=M_BERT_space[idv]\n",
    "    x_val[i]=(vector/n)\n",
    "    i+=1  \n",
    "\n",
    "shape_test=np.asarray(input_ids_test).shape\n",
    "x_test=np.zeros((shape_test[0], 768))\n",
    "i=0\n",
    "for in_id in input_ids_test:\n",
    "    vector=np.zeros(768)\n",
    "    n = np.sum(in_id != 0)\n",
    "    for idv in in_id:\n",
    "        if idv!=0:\n",
    "            vector+=M_BERT_space[idv]\n",
    "    x_test[i]=(vector/n)\n",
    "    i+=1 \n",
    "    \n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "x_val=np.expand_dims(x_val, axis=-1)\n",
    "x_test=np.expand_dims(x_test, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------- EJECUCIÓN #1----------------\n",
      "WARNING:tensorflow:Large dropout rate: 0.65 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "Agregando predicciones Val set\n",
      "Agregando predicciones Test set rnn3\n",
      "--------VALIDATION SET--------\n",
      "\n",
      "Committee Best Fit\n",
      "Accuracy: 0.8701176470588236\n",
      "F1-macro: 0.47879294914588266\n",
      "F1-score SDQC: 0.47879294914588266\n",
      "\n",
      "Committee Norm\n",
      "Accuracy: 0.8790588235294118\n",
      "F1-macro: 0.4638909962624448\n",
      "F1-score SDQC: 0.4638909962624448\n",
      "\n",
      "Committee Voting\n",
      "Accuracy: 0.8790588235294118\n",
      "F1-macro: 0.4634854926222458\n",
      "F1-score SDQC: 0.4634854926222458\n",
      "--------TESTING SET--------\n",
      "\n",
      "Committee Best Fit\n",
      "Accuracy: 0.8271314178049929\n",
      "F1-macro: 0.3942449103499841\n",
      "F1-score SDQC: 0.3942449103499841\n",
      "\n",
      "Committee Norm\n",
      "Accuracy: 0.8290155440414507\n",
      "F1-macro: 0.38745754520481335\n",
      "F1-score SDQC: 0.38745754520481335\n",
      "\n",
      "Committee Voting\n",
      "Accuracy: 0.8308996702779087\n",
      "F1-macro: 0.390646406775439\n",
      "F1-score SDQC: 0.390646406775439\n",
      "\n",
      "---------------- EJECUCIÓN #2----------------\n",
      "Agregando predicciones Val set\n",
      "Agregando predicciones Test set rnn3\n",
      "--------VALIDATION SET--------\n",
      "\n",
      "Committee Best Fit\n",
      "Accuracy: 0.8498823529411764\n",
      "F1-macro: 0.45764135114821614\n",
      "F1-score SDQC: 0.45764135114821614\n",
      "\n",
      "Committee Norm\n",
      "Accuracy: 0.8616470588235294\n",
      "F1-macro: 0.4402539513070973\n",
      "F1-score SDQC: 0.4402539513070973\n",
      "\n",
      "Committee Voting\n",
      "Accuracy: 0.8682352941176471\n",
      "F1-macro: 0.4445293645311254\n",
      "F1-score SDQC: 0.4445293645311254\n",
      "--------TESTING SET--------\n",
      "\n",
      "Committee Best Fit\n",
      "Accuracy: 0.8120584079133302\n",
      "F1-macro: 0.39469501469834\n",
      "F1-score SDQC: 0.39469501469834\n",
      "\n",
      "Committee Norm\n",
      "Accuracy: 0.8210080075365049\n",
      "F1-macro: 0.37732531080358755\n",
      "F1-score SDQC: 0.37732531080358755\n",
      "\n",
      "Committee Voting\n",
      "Accuracy: 0.8290155440414507\n",
      "F1-macro: 0.3921379208740755\n",
      "F1-score SDQC: 0.3921379208740755\n",
      "\n",
      "---------------- EJECUCIÓN #3----------------\n",
      "Agregando predicciones Val set\n",
      "Agregando predicciones Test set rnn3\n",
      "--------VALIDATION SET--------\n",
      "\n",
      "Committee Best Fit\n",
      "Accuracy: 0.8762352941176471\n",
      "F1-macro: 0.46951835924413865\n",
      "F1-score SDQC: 0.46951835924413865\n",
      "\n",
      "Committee Norm\n",
      "Accuracy: 0.8752941176470588\n",
      "F1-macro: 0.4676389464631958\n",
      "F1-score SDQC: 0.4676389464631958\n",
      "\n",
      "Committee Voting\n",
      "Accuracy: 0.8781176470588236\n",
      "F1-macro: 0.4702008553713274\n",
      "F1-score SDQC: 0.4702008553713274\n",
      "--------TESTING SET--------\n",
      "\n",
      "Committee Best Fit\n",
      "Accuracy: 0.8294865756005653\n",
      "F1-macro: 0.4003492841020718\n",
      "F1-score SDQC: 0.4003492841020718\n",
      "\n",
      "Committee Norm\n",
      "Accuracy: 0.814884597268017\n",
      "F1-macro: 0.3619057252230823\n",
      "F1-score SDQC: 0.3619057252230823\n",
      "\n",
      "Committee Voting\n",
      "Accuracy: 0.8224211022138483\n",
      "F1-macro: 0.3760074220060382\n",
      "F1-score SDQC: 0.3760074220060382\n",
      "\n",
      "---------------- EJECUCIÓN #4----------------\n",
      "Agregando predicciones Val set\n",
      "Agregando predicciones Test set rnn3\n",
      "--------VALIDATION SET--------\n",
      "\n",
      "Committee Best Fit\n",
      "Accuracy: 0.8748235294117647\n",
      "F1-macro: 0.46594337445601863\n",
      "F1-score SDQC: 0.46594337445601863\n",
      "\n",
      "Committee Norm\n",
      "Accuracy: 0.8752941176470588\n",
      "F1-macro: 0.4492368271356903\n",
      "F1-score SDQC: 0.4492368271356903\n",
      "\n",
      "Committee Voting\n",
      "Accuracy: 0.8748235294117647\n",
      "F1-macro: 0.44927230472806434\n",
      "F1-score SDQC: 0.44927230472806434\n",
      "--------TESTING SET--------\n",
      "\n",
      "Committee Best Fit\n",
      "Accuracy: 0.8186528497409327\n",
      "F1-macro: 0.3819921616235475\n",
      "F1-score SDQC: 0.3819921616235475\n",
      "\n",
      "Committee Norm\n",
      "Accuracy: 0.8172397550635893\n",
      "F1-macro: 0.3682384997977202\n",
      "F1-score SDQC: 0.3682384997977202\n",
      "\n",
      "Committee Voting\n",
      "Accuracy: 0.8200659444182761\n",
      "F1-macro: 0.3730871726951813\n",
      "F1-score SDQC: 0.3730871726951813\n",
      "\n",
      "---------------- EJECUCIÓN #5----------------\n",
      "Agregando predicciones Val set\n",
      "Agregando predicciones Test set rnn3\n",
      "--------VALIDATION SET--------\n",
      "\n",
      "Committee Best Fit\n",
      "Accuracy: 0.8672941176470588\n",
      "F1-macro: 0.45386410302711333\n",
      "F1-score SDQC: 0.45386410302711333\n",
      "\n",
      "Committee Norm\n",
      "Accuracy: 0.8611764705882353\n",
      "F1-macro: 0.4381880963025674\n",
      "F1-score SDQC: 0.4381880963025674\n",
      "\n",
      "Committee Voting\n",
      "Accuracy: 0.8691764705882353\n",
      "F1-macro: 0.456467519329363\n",
      "F1-score SDQC: 0.456467519329363\n",
      "--------TESTING SET--------\n",
      "\n",
      "Committee Best Fit\n",
      "Accuracy: 0.8195949128591615\n",
      "F1-macro: 0.3748397648862765\n",
      "F1-score SDQC: 0.3748397648862765\n",
      "\n",
      "Committee Norm\n",
      "Accuracy: 0.8219500706547339\n",
      "F1-macro: 0.378121334175533\n",
      "F1-score SDQC: 0.378121334175533\n",
      "\n",
      "Committee Voting\n",
      "Accuracy: 0.8257183231276496\n",
      "F1-macro: 0.38207017613912975\n",
      "F1-score SDQC: 0.38207017613912975\n"
     ]
    }
   ],
   "source": [
    "path='../Operador Data Augmentation/exec_baselines_harass/'\n",
    "bs=32\n",
    "    \n",
    "fs_macro_val={'cnn1':[], 'cnn2':[], 'rnn1':[],'rnn2':[],'rnn3':[]}\n",
    "fs_none_val={'cnn1':[], 'cnn2':[], 'rnn1':[],'rnn2':[],'rnn3':[]}\n",
    "accs_val={'cnn1':[], 'cnn2':[], 'rnn1':[],'rnn2':[],'rnn3':[]}\n",
    "confusions_val={'cnn1':[], 'cnn2':[], 'rnn1':[],'rnn2':[],'rnn3':[]}\n",
    "\n",
    "fs_macro_test={'cnn1':[], 'cnn2':[], 'rnn1':[],'rnn2':[],'rnn3':[]}\n",
    "fs_none_test={'cnn1':[], 'cnn2':[], 'rnn1':[],'rnn2':[],'rnn3':[]}\n",
    "accs_test={'cnn1':[], 'cnn2':[], 'rnn1':[],'rnn2':[],'rnn3':[]}\n",
    "confusions_test={'cnn1':[], 'cnn2':[], 'rnn1':[],'rnn2':[],'rnn3':[]}\n",
    "\n",
    "best={'acc':[], 'none':[], 'macro':[]}\n",
    "norm={'acc':[], 'none':[], 'macro':[]}\n",
    "voting={'acc':[], 'none':[], 'macro':[]}\n",
    "best_test={'acc':[], 'none':[], 'macro':[]}\n",
    "norm_test={'acc':[], 'none':[], 'macro':[]}\n",
    "voting_test={'acc':[], 'none':[], 'macro':[]}\n",
    "\n",
    "#exec_baselines/SMOTE/cnn2_'+str(i+1)+'-exec_w4_app1.h5\n",
    "#cnn1_4239-exec_w4_app1.h5\n",
    "for j in range(5):\n",
    "    print (\"\")\n",
    "    print (\"---------------- EJECUCIÓN #\"+str(j+1)+'----------------')\n",
    "    cnn1= load_model(path+'SMOTE/cnn1_'+str(j+1)+'-exec_w4_app1.h5', custom_objects={'FocalLoss': focal_loss, 'focal_loss_fixed': focal_loss()})  \n",
    "    cnn2= load_model(path+'SMOTE/cnn2_'+str(j+1)+'-exec_w4_app1.h5', custom_objects={'FocalLoss': focal_loss, 'focal_loss_fixed': focal_loss()}) \n",
    "    rnn1= load_model(path+'SMOTE/rnn1_'+str(j+1)+'-exec_w4_app1.h5', custom_objects={'FocalLoss': focal_loss, 'focal_loss_fixed': focal_loss()}) \n",
    "    rnn2= load_model(path+'SMOTE/rnn2_'+str(j+1)+'-exec_w4_app1.h5', custom_objects={'FocalLoss': focal_loss, 'focal_loss_fixed': focal_loss()}) \n",
    "    rnn3= load_model(path+'SMOTE/rnn3_'+str(j+1)+'-exec_w4_app1.h5') \n",
    "    \n",
    "    list_models=['cnn1', 'cnn2', 'rnn1', 'rnn2', 'rnn3']\n",
    "    index_models=np.arange(5)\n",
    "    dict_models=dict((key, value) for (key, value) in zip(index_models,list_models))\n",
    "    modelos=[cnn1, cnn2, rnn1, rnn2, rnn3]\n",
    "    ind=np.arange(5)\n",
    "    dict_trainedModel=dict((key, value) for (key, value) in zip(ind,modelos))\n",
    "    \n",
    "    print (\"Agregando predicciones Val set\")\n",
    "    for i in dict_trainedModel.keys(): \n",
    "        a_evaluar=dict_models[i]\n",
    "        predicho=dict_trainedModel[i].predict(x_val, batch_size=bs)\n",
    "        predicho=[np.argmax(pred) for pred in predicho]\n",
    "        acc= accuracy_score(etiq_v, predicho)  \n",
    "        f1_ma=f1_score(etiq_v, predicho, average='macro')\n",
    "        f1_no=f1_score(etiq_v, predicho, average=None)\n",
    "        matriz=normalize(confusion_matrix(etiq_v, predicho))\n",
    "        fs_macro_val[dict_models[i]].append(f1_ma)\n",
    "        fs_none_val[dict_models[i]].append(f1_no)\n",
    "        accs_val[dict_models[i]].append(acc)\n",
    "        confusions_val[dict_models[i]].append(matriz)\n",
    "        \n",
    "    predicciones_all_val=[]\n",
    "    for i in dict_trainedModel.keys(): \n",
    "        a_evaluar=dict_models[i]\n",
    "        predicciones_all_val.append(dict_trainedModel[i].predict(x_val, batch_size=bs))\n",
    "    predichos_all_val=np.concatenate(np.asarray(predicciones_all_val),axis=-1)\n",
    "    \n",
    "    print (\"Agregando predicciones Test set\", dict_models[i])    \n",
    "    for i in dict_trainedModel.keys(): \n",
    "        a_evaluar=dict_models[i]\n",
    "        predicho=dict_trainedModel[i].predict(x_test, batch_size=bs)\n",
    "        predicho=[np.argmax(pred) for pred in predicho]\n",
    "        acc= accuracy_score(etiq_t, predicho)  \n",
    "        f1_ma=f1_score(etiq_t, predicho, average='macro')\n",
    "        f1_no=f1_score(etiq_t, predicho, average=None)\n",
    "        matriz=normalize(confusion_matrix(etiq_t, predicho))\n",
    "        fs_macro_test[dict_models[i]].append(f1_ma)\n",
    "        fs_none_test[dict_models[i]].append(f1_no)\n",
    "        accs_test[dict_models[i]].append(acc)\n",
    "        confusions_test[dict_models[i]].append(matriz)\n",
    "\n",
    "    predicciones_all_test=[]\n",
    "    bs=32\n",
    "    for i in dict_trainedModel.keys(): \n",
    "        a_evaluar=dict_models[i]\n",
    "        predicciones_all_test.append(dict_trainedModel[i].predict(x_test, batch_size=bs))\n",
    "    predichos_all_test=np.concatenate(np.asarray(predicciones_all_test),axis=-1)\n",
    "    \n",
    "    print (\"--------VALIDATION SET--------\")\n",
    "    \n",
    "    print (\"\\nCommittee Best Fit\")\n",
    "    trainPredicho, maxPredicho, confianzas = predecir_modelos(bs,predichos_all_val)\n",
    "    acc_comite= accuracy_score(etiq_v, trainPredicho)  \n",
    "    print(\"Accuracy:\",acc_comite)\n",
    "    f1=f1_score(etiq_v, trainPredicho, average='macro') \n",
    "    print(\"F1-macro:\",f1)\n",
    "    f1_no=f1_score(etiq_v, trainPredicho, average=None)  \n",
    "    print(\"F1-score SDQC:\",f1)\n",
    "    best['acc'].append(acc_comite)\n",
    "    best['macro'].append(f1)\n",
    "    best['none'].append(f1_no)\n",
    "    \n",
    "    print (\"\\nCommittee Norm\")\n",
    "    trainPredicho, maxPredicho, confianzas = predecir_modelos_norm(bs,predichos_all_val)\n",
    "    acc_comite= accuracy_score(etiq_v, trainPredicho)  \n",
    "    print(\"Accuracy:\",acc_comite)\n",
    "    f1=f1_score(etiq_v, trainPredicho, average='macro') \n",
    "    print(\"F1-macro:\",f1)\n",
    "    f1_no=f1_score(etiq_v, trainPredicho, average=None) \n",
    "    print(\"F1-score SDQC:\",f1)\n",
    "    norm['acc'].append(acc_comite)\n",
    "    norm['macro'].append(f1)\n",
    "    norm['none'].append(f1_no)\n",
    "\n",
    "    print (\"\\nCommittee Voting\")\n",
    "    trainPredicho, maxPredicho, confianzas = predecir_modelos_average(bs,predichos_all_val)\n",
    "    acc_comite= accuracy_score(etiq_v, trainPredicho)  \n",
    "    print(\"Accuracy:\",acc_comite)\n",
    "    f1=f1_score(etiq_v, trainPredicho, average='macro') \n",
    "    print(\"F1-macro:\",f1)\n",
    "    f1_no=f1_score(etiq_v, trainPredicho, average=None)  \n",
    "    print(\"F1-score SDQC:\",f1)\n",
    "    voting['acc'].append(acc_comite)\n",
    "    voting['macro'].append(f1)\n",
    "    voting['none'].append(f1_no)\n",
    "    \n",
    "    print (\"--------TESTING SET--------\")\n",
    "    \n",
    "    print (\"\\nCommittee Best Fit\")\n",
    "    trainPredicho, maxPredicho, confianzas = predecir_modelos(bs,predichos_all_test)\n",
    "    acc_comite= accuracy_score(etiq_t, trainPredicho)  \n",
    "    print(\"Accuracy:\",acc_comite)\n",
    "    f1=f1_score(etiq_t, trainPredicho, average='macro') \n",
    "    print(\"F1-macro:\",f1)\n",
    "    f1_no=f1_score(etiq_t, trainPredicho, average=None)  \n",
    "    print(\"F1-score SDQC:\",f1)\n",
    "    best_test['acc'].append(acc_comite)\n",
    "    best_test['macro'].append(f1)\n",
    "    best_test['none'].append(f1_no)\n",
    "\n",
    "    print (\"\\nCommittee Norm\")\n",
    "    trainPredicho, maxPredicho, confianzas = predecir_modelos_norm(bs,predichos_all_test)\n",
    "    acc_comite= accuracy_score(etiq_t, trainPredicho)  \n",
    "    print(\"Accuracy:\",acc_comite)\n",
    "    f1=f1_score(etiq_t, trainPredicho, average='macro') \n",
    "    print(\"F1-macro:\",f1)\n",
    "    f1_no=f1_score(etiq_t, trainPredicho, average=None) \n",
    "    print(\"F1-score SDQC:\",f1)\n",
    "    norm_test['acc'].append(acc_comite)\n",
    "    norm_test['macro'].append(f1)\n",
    "    norm_test['none'].append(f1_no)\n",
    "\n",
    "    print (\"\\nCommittee Voting\")\n",
    "    trainPredicho, maxPredicho, confianzas = predecir_modelos_average(bs,predichos_all_test)\n",
    "    acc_comite= accuracy_score(etiq_t, trainPredicho)  \n",
    "    print(\"Accuracy:\",acc_comite)\n",
    "    f1=f1_score(etiq_t, trainPredicho, average='macro') \n",
    "    print(\"F1-macro:\",f1)\n",
    "    f1_no=f1_score(etiq_t, trainPredicho, average=None)  \n",
    "    print(\"F1-score SDQC:\",f1)\n",
    "    voting_test['acc'].append(acc_comite)\n",
    "    voting_test['macro'].append(f1)\n",
    "    voting_test['none'].append(f1_no)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAL] Acc Best comité: 0.8676705882352941\n",
      "[VAL] F1-score SDQC Best comité: [0.92101027 0.03874441 0.08686944 0.813984  ]\n",
      "[VAL] F1 macro Best comité: 0.46515202740427397\n",
      "-----------\n",
      "[VAL] Acc Norm comité: 0.8704941176470589\n",
      "[VAL] F1-score SDQC Norm comité: [0.92022483 0.00540541 0.06959895 0.81213786]\n",
      "[VAL] F1 macro Norm comité: 0.45184176349419913\n",
      "-----------\n",
      "[VAL] Acc Voting comité: 0.8738823529411766\n",
      "[VAL] F1-score SDQC Voting comité: [0.92255912 0.00540541 0.07831591 0.82088399]\n",
      "[VAL] F1 macro Voting comité: 0.4567911073164252\n",
      "\n",
      "[TEST] Acc Best comité: 0.8213848327837965\n",
      "[TEST] F1-score SDQC Best comité: [0.89955769 0.02368094 0.00493827 0.62872001]\n",
      "[TEST] F1 macro Best comité: 0.3892242271320441\n",
      "-----------\n",
      "[TEST] Acc Norm comité: 0.820819594912859\n",
      "[TEST] F1-score SDQC Norm comité: [0.89718446 0.         0.         0.60125427]\n",
      "[TEST] F1 macro Norm comité: 0.3746096830409472\n",
      "-----------\n",
      "[TEST] Acc Voting comité: 0.8256241168158267\n",
      "[TEST] F1-score SDQC Voting comité: [0.90013035 0.0037037  0.         0.62732522]\n",
      "[TEST] F1 macro Voting comité: 0.38278981969797277\n"
     ]
    }
   ],
   "source": [
    "print (\"[VAL] Acc Best comité:\",np.sum(np.asarray(best['acc'])/5.0))\n",
    "temp=np.zeros(4)\n",
    "for result in best['none']:\n",
    "    temp+=result\n",
    "print (\"[VAL] F1-score SDQC Best comité:\" ,temp/5)\n",
    "print (\"[VAL] F1 macro Best comité:\",np.sum(np.asarray(best['macro'])/5.0))\n",
    "print (\"-----------\")\n",
    "print (\"[VAL] Acc Norm comité:\",np.sum(np.asarray(norm['acc'])/5.0))\n",
    "temp=np.zeros(4)\n",
    "for result in norm['none']:\n",
    "    temp+=result\n",
    "print (\"[VAL] F1-score SDQC Norm comité:\" ,temp/5)\n",
    "print (\"[VAL] F1 macro Norm comité:\",np.sum(np.asarray(norm['macro'])/5.0))\n",
    "print (\"-----------\")\n",
    "print (\"[VAL] Acc Voting comité:\",np.sum(np.asarray(voting['acc'])/5.0))\n",
    "temp=np.zeros(4)\n",
    "for result in voting['none']:\n",
    "    temp+=result\n",
    "print (\"[VAL] F1-score SDQC Voting comité:\" ,temp/5)\n",
    "print (\"[VAL] F1 macro Voting comité:\",np.sum(np.asarray(voting['macro'])/5.0))\n",
    "\n",
    "print (\"\")\n",
    "\n",
    "print (\"[TEST] Acc Best comité:\",np.sum(np.asarray(best_test['acc'])/5.0))\n",
    "temp=np.zeros(4)\n",
    "for result in best_test['none']:\n",
    "    temp+=result\n",
    "print (\"[TEST] F1-score SDQC Best comité:\" ,temp/5)\n",
    "print (\"[TEST] F1 macro Best comité:\",np.sum(np.asarray(best_test['macro'])/5.0))\n",
    "print (\"-----------\")\n",
    "print (\"[TEST] Acc Norm comité:\",np.sum(np.asarray(norm_test['acc'])/5.0))\n",
    "temp=np.zeros(4)\n",
    "for result in norm_test['none']:\n",
    "    temp+=result\n",
    "print (\"[TEST] F1-score SDQC Norm comité:\" ,temp/5)\n",
    "print (\"[TEST] F1 macro Norm comité:\",np.sum(np.asarray(norm_test['macro'])/5.0))\n",
    "print (\"-----------\")\n",
    "print (\"[TEST] Acc Voting comité:\",np.sum(np.asarray(voting_test['acc'])/5.0))\n",
    "temp=np.zeros(4)\n",
    "for result in voting_test['none']:\n",
    "    temp+=result\n",
    "print (\"[TEST] F1-score SDQC Voting comité:\" ,temp/5)\n",
    "print (\"[TEST] F1 macro Voting comité:\",np.sum(np.asarray(voting_test['macro'])/5.0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3972362262138721"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(fs_macro_test['cnn2'])/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
