{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/casapanshop/anaconda2/envs/newpy3_tf1/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/casapanshop/anaconda2/envs/newpy3_tf1/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/casapanshop/anaconda2/envs/newpy3_tf1/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/casapanshop/anaconda2/envs/newpy3_tf1/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/casapanshop/anaconda2/envs/newpy3_tf1/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/casapanshop/anaconda2/envs/newpy3_tf1/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/casapanshop/anaconda2/envs/newpy3_tf1/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/casapanshop/anaconda2/envs/newpy3_tf1/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/casapanshop/anaconda2/envs/newpy3_tf1/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/casapanshop/anaconda2/envs/newpy3_tf1/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/casapanshop/anaconda2/envs/newpy3_tf1/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/casapanshop/anaconda2/envs/newpy3_tf1/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: GeForce GTX 1060 6GB\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import pandas as pd\n",
    "from models import *\n",
    "import time\n",
    "import re, io, nltk, torch \n",
    "from nltk.corpus import stopwords\n",
    "from numpy import linalg as LA\n",
    "from numpy.linalg import norm\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm, trange\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from gensim import utils, matutils  \n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from six import string_types, integer_types\n",
    "from six.moves import zip, range\n",
    "from numpy import linalg as LA\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from scipy import stats\n",
    "from gensim.utils import deprecated\n",
    "from numpy import dot, float32 as REAL, memmap as np_memmap, \\\n",
    "    double, array, zeros, vstack, sqrt, newaxis, integer, \\\n",
    "    ndarray, sum as np_sum, prod, argmax\n",
    "from collections import Counter\n",
    "from scipy.spatial import distance\n",
    "from numpy.random import binomial\n",
    "from scipy.stats import bernoulli\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "n_gpu = torch.cuda.device_count()\n",
    "torch.cuda.get_device_name(0)\n",
    "\n",
    "if torch.cuda.is_available():     \n",
    "    device = torch.device(\"cuda\")\n",
    "    print('There are',torch.cuda.device_count(),'GPU(s) available.')\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "train =pd.read_csv(\"../Tesis_exp/Fine-Tuning/CSV_Harassment/train_format.csv\")\n",
    "val=pd.read_csv(\"../Tesis_exp/Fine-Tuning/CSV_Harassment/val_format.csv\")\n",
    "test=pd.read_csv(\"../Tesis_exp/Fine-Tuning/CSV_Harassment/test_format.csv\")\n",
    "\n",
    "dict_cl=dict()\n",
    "dict_cl[0]=\"NonH\"\n",
    "dict_cl[1]=\"IndirectH\"\n",
    "dict_cl[2]=\"PhysicalH\"\n",
    "dict_cl[3]=\"SexualH\"\n",
    "\n",
    "#sdqc\n",
    "d_lab=dict()\n",
    "d_lab[\"NonH\"]=0\n",
    "d_lab[\"IndirectH\"]=1\n",
    "d_lab[\"PhysicalH\"]=2\n",
    "d_lab[\"SexualH\"]=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se encontraron 1193514 terminos con sus vectores de embedding.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import WordNetLemmatizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "\n",
    "stemmer_sn = SnowballStemmer(\"english\")\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "stoplist = stopwords.words(\"english\")\n",
    "lemmatizer=WordNetLemmatizer()\n",
    "\n",
    "emoji_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "\n",
    "def cleaner(post): \n",
    "    #re.sub(r'([a-z])\\1+', r'\\1', \"user i think that ' s all you loooooove \")\n",
    "    s= re.sub(r\"http\\S+\", \" <html> \", post)   ##########elimina http    \n",
    "    s= re.sub(r'#\\w+ ?', \" <hashtag> \", s) ########3\n",
    "    s= re.sub(r'@\\w+ ?', \" <user> \", s)##############    \n",
    "    s= s.lower()\n",
    "    s=emoji_pattern.sub(r'', s)\n",
    "    s=re.sub(r\"'\\b\", \" ' \", s)\n",
    "    s=re.sub(r\"\\b'\\b\", \" ' \", s)\n",
    "    s=re.sub(r\"“\\b\", \" “ \", s)\n",
    "    #“\n",
    "    s=re.sub(r\"\\b’\", \" ’ \", s)\n",
    "    s=re.sub(r\"‘\\b\", \" ‘ \", s)\n",
    "    s=re.sub(r\"\\b’\\b\", \" ‘ \", s)\n",
    "    s = re.sub(r\"-\", \" - \", s)\n",
    "    s = re.sub(r\"\\(\", \" \", s)\n",
    "    s = re.sub(r\"\\)\", \" \", s)\n",
    "    s = re.sub(r\"\\?\", \" ? \", s)    \n",
    "    s = re.sub(r\"\\/\", \" \", s)\n",
    "    s = re.sub(r\"' \", \" ' \", s)\n",
    "    s = re.sub(r\" '\", \" ' \", s)\n",
    "    s = re.sub(r\"\\!\", \" ! \", s)\n",
    "    s=re.sub(\"[\\.]+\", \" . \", s)\n",
    "    s=re.sub(\"[\\,]+\", \" , \", s)\n",
    "    s=re.sub(\"[\\;]+\", \" ; \", s)\n",
    "    s=re.sub(\"[\\:]+\", \" : \", s)\n",
    "    s=re.sub('[\\\"]+', ' \" ', s)\n",
    "    s=re.sub(r'\\b[0-9]\\b', \" <number> \",  s)\n",
    "    s=re.sub(r'\\b[0-9]*[0-9]\\b', \" <number> \",  s)    \n",
    "    s=re.sub(r'\\b”', ' \" ', s)\n",
    "    sl= list(s.split())\n",
    "    sl_2=[]\n",
    "    for wd in sl:\n",
    "        try: \n",
    "            q=Word2Index_valid[wd]\n",
    "            sl_2.append(wd)\n",
    "        except:\n",
    "            try: \n",
    "                if stemmer.stem(wd) in Word2Index_valid.keys():\n",
    "                    sl_2.append(stemmer.stem(wd))\n",
    "                elif lemmatizer.lemmatize(wd) in Word2Index_valid.keys():\n",
    "                    sl_2.append(lemmatizer.lemmatize(wd))\n",
    "                else:\n",
    "                    sl_2.append(wd)\n",
    "            except:\n",
    "                sl_2.append(wd)\n",
    "    sl=sl_2\n",
    "    s=' '.join([word for word in sl])# if word not in stoplist])\n",
    "    return s, sl\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join(\"glove.twitter.27B.200d.txt\"))#'glove.twitter.27B.100d.txt'))\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Se encontraron %s terminos con sus vectores de embedding.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22107"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "word_index=dict()\n",
    "j=1\n",
    "for frase in train[\"Tweet\"].values:\n",
    "    frase=cleaner(frase)[-1]\n",
    "    #print (frase)\n",
    "    for term in frase:\n",
    "        if term not in word_index.keys():\n",
    "            word_index[term]=j\n",
    "            j+=1\n",
    "\n",
    "for frase in val[\"Tweet\"].values:\n",
    "    frase=cleaner(frase)[-1]\n",
    "    #print (frase)\n",
    "    for term in frase:\n",
    "        if term not in word_index.keys():\n",
    "            word_index[term]=j\n",
    "            j+=1\n",
    "            \n",
    "for frase in test[\"Tweet\"].values:\n",
    "    frase=cleaner(frase)[-1]\n",
    "    #print (frase)\n",
    "    for term in frase:\n",
    "        if term not in word_index.keys():\n",
    "            word_index[term]=j\n",
    "            j+=1\n",
    "            \n",
    "len(word_index.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14642, 200)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "not_found=[]\n",
    "pos_tag_nf=[]\n",
    "Word2Index={}\n",
    "Word2Index_valid={}\n",
    "idx_2_token={}\n",
    "idx_2_token_valid={}\n",
    "\n",
    "g_dim=200\n",
    "emb_matrix = np.zeros((len(word_index.keys())+1, g_dim))  \n",
    "\n",
    "k=1\n",
    "for word, i in word_index.items():\n",
    "    vector = embeddings_index.get(word)\n",
    "    if vector is not None:\n",
    "        # words sin match en Glove, serán vectores de ceros.\n",
    "        emb_matrix[i] = vector\n",
    "        Word2Index[word]=i\n",
    "        Word2Index_valid[word]=k\n",
    "        idx_2_token_valid[k]=word\n",
    "        k+=1\n",
    "        idx_2_token[i]=word\n",
    "        \n",
    "    else:\n",
    "        Word2Index[word]=i\n",
    "        idx_2_token[i]=word\n",
    "        not_found.append(word)\n",
    "        pos_tag_nf.append(nltk.pos_tag([word])[0][-1])\n",
    "        \n",
    "glove_matrix = np.zeros((k, g_dim))\n",
    "\n",
    "for word, i in Word2Index_valid.items():\n",
    "    vector = embeddings_index.get(word)    \n",
    "    glove_matrix[i] = vector\n",
    "    \n",
    "glove_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transf = Normalizer().fit(glove_matrix) \n",
    "#M_GLOVE_space=transf.transform(glove_matrix)\n",
    "M_GLOVE_space=glove_matrix\n",
    "\n",
    "def my_tokenizer(lista):\n",
    "    encoded_sent=[]\n",
    "    to_return_ide=[]\n",
    "    for wd in lista:\n",
    "        try:               \n",
    "            to_return_ide.append(Word2Index_valid[wd])\n",
    "            encoded_sent.append(wd)\n",
    "        except: \n",
    "            continue   \n",
    "           \n",
    "    return to_return_ide, encoded_sent\n",
    "\n",
    "\n",
    "def predict_data(trained, x_train, x_val, x_test, etiq, etiq_v, etiq_t, name_model):\n",
    "    etiq = etiq.astype(\"int\")\n",
    "    trainPredict = trained.predict(x_train, batch_size=32)\n",
    "    trainPredict=[np.argmax(pred) for pred in trainPredict]\n",
    "    acc= accuracy_score(etiq, trainPredict)  \n",
    "    f1=f1_score(etiq, trainPredict, average=None)  # labels=np.unique(trainPredict)\n",
    "    f1_ma=f1_score(etiq, trainPredict, average='macro')  # labels=np.unique(trainPredict)\n",
    "    #matriz=normalize(confusion_matrix(etiq, trainPredict))\n",
    "    print (\"\")\n",
    "    print (\"Accuracy sobre Train\", name_model, \":\",acc)  \n",
    "    print (\"F1-score None sobre Train\", name_model, \":\",f1)\n",
    "    print (\"F1-score macro sobre Train\", name_model, \":\",f1_ma)\n",
    "    ########################################\n",
    "    trainPredict = trained.predict(x_val, batch_size=32)\n",
    "    trainPredict=[np.argmax(pred) for pred in trainPredict]\n",
    "    acc= accuracy_score(etiq_v, trainPredict)  \n",
    "    f1=f1_score(etiq_v, trainPredict, average=None)  # labels=np.unique(trainPredict)\n",
    "    f1_ma=f1_score(etiq_v, trainPredict, average='macro')  # labels=np.unique(trainPredict)\n",
    "    #matriz=normalize(confusion_matrix(etiq_v, trainPredict))\n",
    "    print (\"\")\n",
    "    print (\"Accuracy sobre Val\", name_model, \":\",acc)  \n",
    "    print (\"F1-score None sobre Val\", name_model, \":\",f1)\n",
    "    print (\"F1-score macro sobre Val\", name_model, \":\",f1_ma)\n",
    "    ########################################\n",
    "    trainPredict = trained.predict(x_test, batch_size=32)\n",
    "    trainPredict=[np.argmax(pred) for pred in trainPredict]\n",
    "    acc_t= accuracy_score(etiq_t, trainPredict)  \n",
    "    f1_t=f1_score(etiq_t, trainPredict, average=None)  # labels=np.unique(trainPredict)\n",
    "    f1_ma_t=f1_score(etiq_t, trainPredict, average='macro')  # labels=np.unique(trainPredict)\n",
    "    matriz_t=normalize(confusion_matrix(etiq_t, trainPredict))\n",
    "    print (\"\")\n",
    "    print (\"Accuracy sobre Test\", name_model, \":\",acc_t)  \n",
    "    print (\"F1-score None sobre Test\", name_model, \":\",f1_t)\n",
    "    print (\"F1-score macro sobre Test\", name_model, \":\",f1_ma_t)\n",
    "    \n",
    "    return f1_ma_t, f1_t, acc_t, matriz_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conteo por stance val\n",
      " Counter({'NonH': 1493, 'SexualH': 525, 'IndirectH': 71, 'PhysicalH': 36})\n",
      "Conteo por stance test\n",
      " Counter({'NonH': 1601, 'SexualH': 340, 'IndirectH': 106, 'PhysicalH': 76})\n"
     ]
    }
   ],
   "source": [
    "from models import *\n",
    "\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    \n",
    "class Dummy_Embeddings(nn.Module):\n",
    "    def __init__(self, d_model, dummy_vectors): \n",
    "        super(Dummy_Embeddings, self).__init__()\n",
    "        aux = torch.from_numpy(dummy_vectors)#, dtype='float32'))\n",
    "        self.index2dummy = nn.Embedding(aux.size()[0], d_model)\n",
    "        self.index2dummy.weigth=nn.Parameter(aux)\n",
    "        self.index2dummy.weigth.requires_grad=False\n",
    "        self.d_model = d_model\n",
    "        \n",
    "    def forward(self, x):\n",
    "        aux=x.numpy()\n",
    "        #print (\"aux original\", aux)\n",
    "        #print (\"aux restado...\",aux-np.ones(aux.shape))\n",
    "        new_x= aux #- np.ones(aux.shape)\n",
    "        new_x= torch.from_numpy(new_x)\n",
    "        return self.index2dummy(new_x.long()) * math.sqrt(self.d_model) #debiese retornar matriz de batch_size x [ind_tw, k1,k2,k3,k4,k5,k6] (si son 6 modelos)\n",
    "    \n",
    "def match(objetos,ejemplo):\n",
    "    i=0\n",
    "    for obj in objetos:\n",
    "        if obj==ejemplo:\n",
    "            return i\n",
    "        i+=1\n",
    "        \n",
    "def plot_confusion_matrix(cm, target_names, title='Confusion matrix (f1-score)',cmap=None, normalize=True):    \n",
    "    accuracy = np.trace(cm) / float(np.sum(cm))\n",
    "    misclass = 1 - accuracy\n",
    "    if cmap is None:\n",
    "        cmap = plt.get_cmap('Blues')\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap, vmin=0, vmax=1)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    \n",
    "    if target_names is not None:\n",
    "        tick_marks = np.arange(len(target_names))\n",
    "        plt.xticks(tick_marks, target_names, rotation=45)\n",
    "        plt.yticks(tick_marks, target_names)\n",
    "    thresh = cm.max() / 1.5 if normalize else cm.max() / 2\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        if normalize:\n",
    "            plt.text(j, i, \"{:0.4f}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "        else:\n",
    "            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))\n",
    "    plt.show()\n",
    "    \n",
    "MAX_LEN = 33\n",
    "\n",
    "print (\"Conteo por stance val\\n\", Counter(val['Label']))\n",
    "print (\"Conteo por stance test\\n\", Counter(test['Label']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pesos de clases: [1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "ids_train=train['tweet_id']\n",
    "tws_train=train['Tweet']\n",
    "labels_train=[d_lab[lb] for lb in train['Label']]\n",
    "#class_weights= compute_class_weight('balanced', np.unique(labels_train), labels_train)\n",
    "class_weights= [1.0, 1.0, 1.0, 1.0]\n",
    "class_weights=np.asarray(class_weights)\n",
    "print (\"Pesos de clases:\", class_weights)\n",
    "#print (\"Pesos de clases:\", class_weights)\n",
    "\n",
    "etiq=labels_train.copy()\n",
    "etiq=np.asarray(etiq)\n",
    "sentences = tws_train\n",
    "n_labels = np.array(etiq)\n",
    "y_train=to_categorical(n_labels,num_classes=4)\n",
    "\n",
    "\n",
    "input_ids=[]\n",
    "for sent in sentences:\n",
    "    encoded_sent, sent_valid = my_tokenizer(cleaner(sent)[1])  \n",
    "    input_ids.append(encoded_sent) \n",
    "    \n",
    "input_ids = pad_sequences(input_ids,maxlen=MAX_LEN,dtype=\"long\",value=0, truncating=\"post\", padding=\"post\")\n",
    "shape=np.asarray(input_ids).shape\n",
    "x_train=np.zeros((shape[0], shape[1], 200))\n",
    "j=0\n",
    "for in_id in input_ids:\n",
    "    x_train[j]=M_GLOVE_space[in_id]\n",
    "    j+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2123, 33, 200)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids_val=val['tweet_id']\n",
    "tws_val=val['Tweet']\n",
    "labels_val=[d_lab[lb] for lb in val['Label']]\n",
    "\n",
    "etiq_v=labels_val.copy()\n",
    "etiq_v=np.asarray(etiq_v)\n",
    "sentences_val = tws_val\n",
    "n_labels_val = np.array(etiq_v)\n",
    "y_val=to_categorical(n_labels_val,num_classes=4)\n",
    "\n",
    "input_ids_val=[]\n",
    "for sent in sentences_val:\n",
    "    encoded_sent, sent_valid =my_tokenizer(cleaner(sent)[1])  \n",
    "    input_ids_val.append(encoded_sent)\n",
    "\n",
    "ids_test=test['tweet_id']\n",
    "tws_test=test['Tweet']\n",
    "labels_test=[d_lab[lb] for lb in test['Label']]\n",
    "\n",
    "etiq_t=labels_test.copy()\n",
    "etiq_t=np.asarray(etiq_t)\n",
    "sentences_test = tws_test\n",
    "n_labels_test = np.array(etiq_t)\n",
    "y_test=to_categorical(n_labels_test,num_classes=4)\n",
    "\n",
    "input_ids_test=[]\n",
    "for sent in sentences_test:\n",
    "    encoded_sent, sent_valid = my_tokenizer(cleaner(sent)[1])\n",
    "    input_ids_test.append(encoded_sent)\n",
    "    \n",
    "input_ids_val = pad_sequences(input_ids_val,maxlen=MAX_LEN,dtype=\"long\",value=0, truncating=\"post\", padding=\"post\")\n",
    "input_ids_test = pad_sequences(input_ids_test,maxlen=MAX_LEN,dtype=\"long\",value=0, truncating=\"post\", padding=\"post\")\n",
    "\n",
    "\n",
    "shape_val=np.asarray(input_ids_val).shape\n",
    "x_val=np.zeros((shape_val[0], shape_val[1], 200))\n",
    "i=0\n",
    "for in_id in input_ids_val:\n",
    "    x_val[i]=M_GLOVE_space[in_id]\n",
    "    i+=1\n",
    "    \n",
    "shape_test=np.asarray(input_ids_test).shape\n",
    "x_test=np.zeros((shape_test[0], shape_test[1], 200))\n",
    "i=0\n",
    "for in_id in input_ids_test:\n",
    "    x_test[i]=M_GLOVE_space[in_id]\n",
    "    i+=1\n",
    "\n",
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predecir_modelos(bs,predichos_all):  #best fit\n",
    "    temp=predichos_all.copy()\n",
    "    final=[np.argmax(pred) for pred in temp]  \n",
    "    confianzas=[temp[i][final[i]] for i in range(len(final))]\n",
    "    predichos_new=[]\n",
    "    for i in range(len(temp)):\n",
    "        indice=final[i]%4\n",
    "        predichos_new.append(int(indice))\n",
    "    return predichos_new,final,confianzas    \n",
    "\n",
    "def predecir_modelos_norm(bs,predichos_all):  #normalizado\n",
    "    temp=predichos_all.copy()\n",
    "    final=[]\n",
    "    confianzas=[]\n",
    "    for pred in temp:   \n",
    "        aux=np.ones(4)\n",
    "        for i in range(1,6): #for machine pred\n",
    "            sub=pred[(4*(i-1)):(4*i)]\n",
    "            aux=aux*np.asarray(sub)\n",
    "            \n",
    "        aux=aux/np.sum(aux)\n",
    "        final.append(np.argmax(aux))\n",
    "        confianzas.append(aux[np.argmax(aux)])  \n",
    "           \n",
    "    predichos_new=[]\n",
    "    for i in range(len(temp)):\n",
    "        predichos_new.append(final[i])\n",
    "    return predichos_new,final,confianzas\n",
    "    \n",
    "def predecir_modelos_average(bs,predichos_all): #average\n",
    "    temp=predichos_all.copy()\n",
    "    final=[]\n",
    "    confianzas=[]\n",
    "    for pred in temp:\n",
    "        aux=np.zeros(4)\n",
    "        for i in range(1,6):\n",
    "            sub=pred[(4*(i-1)):(4*i)]\n",
    "            aux=aux+np.asarray(sub)\n",
    "            \n",
    "        aux=aux/5.0 #dividido en el total de machines\n",
    "        final.append(np.argmax(aux))\n",
    "        confianzas.append(aux[np.argmax(aux)])  \n",
    "           \n",
    "    predichos_new=[]\n",
    "    for i in range(len(temp)):\n",
    "        predichos_new.append(final[i])\n",
    "    return predichos_new,final,confianzas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "del embeddings_index\n",
    "del emb_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1.])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.0 sin pesos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------- EJECUCIÓN #1----------------\n",
      "Agregando predicciones Test set\n",
      "--------TESTING SET--------\n",
      "\n",
      "Committee Best Fit\n",
      "Accuracy: 0.8412623645784267\n",
      "F1-macro: 0.42037599767404776\n",
      "F1-score SDQC: [0.90909091 0.03703704 0.         0.73537604]\n",
      "\n",
      "Committee Norm\n",
      "Accuracy: 0.8586905322656618\n",
      "F1-macro: 0.42520389989438656\n",
      "F1-score SDQC: [0.92075695 0.         0.         0.78005865]\n",
      "\n",
      "Committee Voting\n",
      "Accuracy: 0.8652849740932642\n",
      "F1-macro: 0.4306337411217853\n",
      "F1-score SDQC: [0.92493377 0.         0.         0.7976012 ]\n",
      "\n",
      "---------------- EJECUCIÓN #2----------------\n",
      "Agregando predicciones Test set\n",
      "--------TESTING SET--------\n",
      "\n",
      "Committee Best Fit\n",
      "Accuracy: 0.855864342910975\n",
      "F1-macro: 0.4303700262206065\n",
      "F1-score SDQC: [0.91947291 0.01869159 0.02597403 0.75734158]\n",
      "\n",
      "Committee Norm\n",
      "Accuracy: 0.8615167216203485\n",
      "F1-macro: 0.42566195994739064\n",
      "F1-score SDQC: [0.92298682 0.         0.         0.77966102]\n",
      "\n",
      "Committee Voting\n",
      "Accuracy: 0.8634008478568064\n",
      "F1-macro: 0.42773115621476276\n",
      "F1-score SDQC: [0.92388759 0.         0.         0.78703704]\n",
      "\n",
      "---------------- EJECUCIÓN #3----------------\n",
      "Agregando predicciones Test set\n",
      "--------TESTING SET--------\n",
      "\n",
      "Committee Best Fit\n",
      "Accuracy: 0.8431464908148846\n",
      "F1-macro: 0.4337094797794475\n",
      "F1-score SDQC: [0.91057384 0.03636364 0.07407407 0.71382637]\n",
      "\n",
      "Committee Norm\n",
      "Accuracy: 0.8525671219971738\n",
      "F1-macro: 0.41328558668462373\n",
      "F1-score SDQC: [0.91770141 0.         0.         0.73544093]\n",
      "\n",
      "Committee Voting\n",
      "Accuracy: 0.8544512482336316\n",
      "F1-macro: 0.41572510220256464\n",
      "F1-score SDQC: [0.91832612 0.         0.         0.74457429]\n",
      "\n",
      "---------------- EJECUCIÓN #4----------------\n",
      "Agregando predicciones Test set\n",
      "--------TESTING SET--------\n",
      "\n",
      "Committee Best Fit\n",
      "Accuracy: 0.8629298162976919\n",
      "F1-macro: 0.4349900907164135\n",
      "F1-score SDQC: [0.92375887 0.         0.02564103 0.79056047]\n",
      "\n",
      "Committee Norm\n",
      "Accuracy: 0.8709373528026377\n",
      "F1-macro: 0.43573784506868807\n",
      "F1-score SDQC: [0.92836171 0.         0.         0.81458967]\n",
      "\n",
      "Committee Voting\n",
      "Accuracy: 0.8718794159208667\n",
      "F1-macro: 0.4368765238530196\n",
      "F1-score SDQC: [0.92848769 0.         0.         0.8190184 ]\n",
      "\n",
      "---------------- EJECUCIÓN #5----------------\n",
      "Agregando predicciones Test set\n",
      "--------TESTING SET--------\n",
      "\n",
      "Committee Best Fit\n",
      "Accuracy: 0.8506829957607159\n",
      "F1-macro: 0.4493407185764955\n",
      "F1-score SDQC: [0.91954023 0.09677419 0.02597403 0.75507442]\n",
      "\n",
      "Committee Norm\n",
      "Accuracy: 0.8662270372114932\n",
      "F1-macro: 0.43166643832884466\n",
      "F1-score SDQC: [0.92607924 0.         0.         0.80058651]\n",
      "\n",
      "Committee Voting\n",
      "Accuracy: 0.8723504474799811\n",
      "F1-macro: 0.4363477251339062\n",
      "F1-score SDQC: [0.93046553 0.         0.         0.81492537]\n"
     ]
    }
   ],
   "source": [
    "prob=0.0\n",
    "path='../new_glove_augmented/harassment_baselines/'\n",
    "bs=32\n",
    "    \n",
    "fs_macro_val={'cnn1':[], 'cnn2':[], 'rnn1':[],'rnn2':[],'rnn3':[]}\n",
    "fs_none_val={'cnn1':[], 'cnn2':[], 'rnn1':[],'rnn2':[],'rnn3':[]}\n",
    "accs_val={'cnn1':[], 'cnn2':[], 'rnn1':[],'rnn2':[],'rnn3':[]}\n",
    "confusions_val={'cnn1':[], 'cnn2':[], 'rnn1':[],'rnn2':[],'rnn3':[]}\n",
    "\n",
    "fs_macro_test={'cnn1':[], 'cnn2':[], 'rnn1':[],'rnn2':[],'rnn3':[]}\n",
    "fs_none_test={'cnn1':[], 'cnn2':[], 'rnn1':[],'rnn2':[],'rnn3':[]}\n",
    "accs_test={'cnn1':[], 'cnn2':[], 'rnn1':[],'rnn2':[],'rnn3':[]}\n",
    "confusions_test={'cnn1':[], 'cnn2':[], 'rnn1':[],'rnn2':[],'rnn3':[]}\n",
    "\n",
    "best={'acc':[], 'none':[], 'macro':[]}\n",
    "norm={'acc':[], 'none':[], 'macro':[]}\n",
    "voting={'acc':[], 'none':[], 'macro':[]}\n",
    "best_test={'acc':[], 'none':[], 'macro':[]}\n",
    "norm_test={'acc':[], 'none':[], 'macro':[]}\n",
    "voting_test={'acc':[], 'none':[], 'macro':[]}\n",
    "\n",
    "for j in range(1,6):\n",
    "    print (\"\")\n",
    "    print (\"---------------- EJECUCIÓN #\"+str(j)+'----------------')\n",
    "    cnn1= load_model(path+str(prob)+'/cnn1_'+str(j)+'-exec_15.h5', custom_objects={'FocalLoss': focal_loss, 'focal_loss_fixed': focal_loss()})  \n",
    "    cnn2= load_model(path+str(prob)+'/cnn2_'+str(j)+'-exec_15.h5', custom_objects={'FocalLoss': focal_loss, 'focal_loss_fixed': focal_loss()}) \n",
    "    rnn1= load_model(path+str(prob)+'/rnn1_'+str(j)+'-exec_8.h5', custom_objects={'FocalLoss': focal_loss, 'focal_loss_fixed': focal_loss()}) \n",
    "    rnn2= load_model(path+str(prob)+'/rnn2_'+str(j)+'-exec_15.h5') \n",
    "    rnn3= load_model(path+str(prob)+'/rnn3_'+str(j)+'-exec_8.h5', custom_objects={'FocalLoss': focal_loss, 'focal_loss_fixed': focal_loss()}) \n",
    "    \n",
    "    list_models=['cnn1', 'cnn2', 'rnn1', 'rnn2', 'rnn3']\n",
    "    index_models=np.arange(5)\n",
    "    dict_models=dict((key, value) for (key, value) in zip(index_models,list_models))\n",
    "    modelos=[cnn1, cnn2, rnn1, rnn2, rnn3]\n",
    "    ind=np.arange(5)\n",
    "    dict_trainedModel=dict((key, value) for (key, value) in zip(ind,modelos))\n",
    "    \n",
    "    \"\"\"print (\"Agregando predicciones Val set\")\n",
    "    for i in dict_trainedModel.keys(): \n",
    "        a_evaluar=dict_models[i]\n",
    "        predicho=dict_trainedModel[i].predict(x_val, batch_size=bs)\n",
    "        predicho=[np.argmax(pred) for pred in predicho]\n",
    "        acc= accuracy_score(etiq_v, predicho)  \n",
    "        f1_ma=f1_score(etiq_v, predicho, average='macro')\n",
    "        f1_no=f1_score(etiq_v, predicho, average=None)\n",
    "        matriz=normalize(confusion_matrix(etiq_v, predicho))\n",
    "        fs_macro_val[dict_models[i]].append(f1_ma)\n",
    "        fs_none_val[dict_models[i]].append(f1_no)\n",
    "        accs_val[dict_models[i]].append(acc)\n",
    "        confusions_val[dict_models[i]].append(matriz)\n",
    "        \n",
    "    predicciones_all_val=[]\n",
    "    for i in dict_trainedModel.keys(): \n",
    "        a_evaluar=dict_models[i]\n",
    "        predicciones_all_val.append(dict_trainedModel[i].predict(x_val, batch_size=bs))\n",
    "    predichos_all_val=np.concatenate(np.asarray(predicciones_all_val),axis=-1)\"\"\"\n",
    "    \n",
    "    print (\"Agregando predicciones Test set\")    \n",
    "    for i in dict_trainedModel.keys(): \n",
    "        a_evaluar=dict_models[i]\n",
    "        predicho=dict_trainedModel[i].predict(x_test, batch_size=bs)\n",
    "        predicho=[np.argmax(pred) for pred in predicho]\n",
    "        acc= accuracy_score(etiq_t, predicho)  \n",
    "        f1_ma=f1_score(etiq_t, predicho, average='macro')\n",
    "        f1_no=f1_score(etiq_t, predicho, average=None)\n",
    "        matriz=normalize(confusion_matrix(etiq_t, predicho))\n",
    "        fs_macro_test[dict_models[i]].append(f1_ma)\n",
    "        fs_none_test[dict_models[i]].append(f1_no)\n",
    "        accs_test[dict_models[i]].append(acc)\n",
    "        confusions_test[dict_models[i]].append(matriz)\n",
    "\n",
    "    predicciones_all_test=[]\n",
    "    bs=32\n",
    "    for i in dict_trainedModel.keys(): \n",
    "        a_evaluar=dict_models[i]\n",
    "        predicciones_all_test.append(dict_trainedModel[i].predict(x_test, batch_size=bs))\n",
    "    predichos_all_test=np.concatenate(np.asarray(predicciones_all_test),axis=-1)\n",
    "    \n",
    "    \"\"\"print (\"--------VALIDATION SET--------\")\n",
    "    \n",
    "    print (\"\\nCommittee Best Fit\")\n",
    "    trainPredicho, maxPredicho, confianzas = predecir_modelos(bs,predichos_all_val)\n",
    "    acc_comite= accuracy_score(etiq_v, trainPredicho)  \n",
    "    print(\"Accuracy:\",acc_comite)\n",
    "    f1=f1_score(etiq_v, trainPredicho, average='macro') \n",
    "    print(\"F1-macro:\",f1)\n",
    "    f1_no=f1_score(etiq_v, trainPredicho, average=None)  \n",
    "    print(\"F1-score SDQC:\",f1)\n",
    "    best['acc'].append(acc_comite)\n",
    "    best['macro'].append(f1)\n",
    "    best['none'].append(f1_no)\n",
    "    \n",
    "    print (\"\\nCommittee Norm\")\n",
    "    trainPredicho, maxPredicho, confianzas = predecir_modelos_norm(bs,predichos_all_val)\n",
    "    acc_comite= accuracy_score(etiq_v, trainPredicho)  \n",
    "    print(\"Accuracy:\",acc_comite)\n",
    "    f1=f1_score(etiq_v, trainPredicho, average='macro') \n",
    "    print(\"F1-macro:\",f1)\n",
    "    f1_no=f1_score(etiq_v, trainPredicho, average=None) \n",
    "    print(\"F1-score SDQC:\",f1)\n",
    "    norm['acc'].append(acc_comite)\n",
    "    norm['macro'].append(f1)\n",
    "    norm['none'].append(f1_no)\n",
    "\n",
    "    print (\"\\nCommittee Voting\")\n",
    "    trainPredicho, maxPredicho, confianzas = predecir_modelos_average(bs,predichos_all_val)\n",
    "    acc_comite= accuracy_score(etiq_v, trainPredicho)  \n",
    "    print(\"Accuracy:\",acc_comite)\n",
    "    f1=f1_score(etiq_v, trainPredicho, average='macro') \n",
    "    print(\"F1-macro:\",f1)\n",
    "    f1_no=f1_score(etiq_v, trainPredicho, average=None)  \n",
    "    print(\"F1-score SDQC:\",f1)\n",
    "    voting['acc'].append(acc_comite)\n",
    "    voting['macro'].append(f1)\n",
    "    voting['none'].append(f1_no)\"\"\"\n",
    "    \n",
    "    print (\"--------TESTING SET--------\")\n",
    "    \n",
    "    print (\"\\nCommittee Best Fit\")\n",
    "    trainPredicho, maxPredicho, confianzas = predecir_modelos(bs,predichos_all_test)\n",
    "    acc_comite= accuracy_score(etiq_t, trainPredicho)  \n",
    "    print(\"Accuracy:\",acc_comite)\n",
    "    f1=f1_score(etiq_t, trainPredicho, average='macro') \n",
    "    print(\"F1-macro:\",f1)\n",
    "    f1_no=f1_score(etiq_t, trainPredicho, average=None)  \n",
    "    print(\"F1-score SDQC:\",f1_no)\n",
    "    best_test['acc'].append(acc_comite)\n",
    "    best_test['macro'].append(f1)\n",
    "    best_test['none'].append(f1_no)\n",
    "\n",
    "    print (\"\\nCommittee Norm\")\n",
    "    trainPredicho, maxPredicho, confianzas = predecir_modelos_norm(bs,predichos_all_test)\n",
    "    acc_comite= accuracy_score(etiq_t, trainPredicho)  \n",
    "    print(\"Accuracy:\",acc_comite)\n",
    "    f1=f1_score(etiq_t, trainPredicho, average='macro') \n",
    "    print(\"F1-macro:\",f1)\n",
    "    f1_no=f1_score(etiq_t, trainPredicho, average=None) \n",
    "    print(\"F1-score SDQC:\",f1_no)\n",
    "    norm_test['acc'].append(acc_comite)\n",
    "    norm_test['macro'].append(f1)\n",
    "    norm_test['none'].append(f1_no)\n",
    "\n",
    "    print (\"\\nCommittee Voting\")\n",
    "    trainPredicho, maxPredicho, confianzas = predecir_modelos_average(bs,predichos_all_test)\n",
    "    acc_comite= accuracy_score(etiq_t, trainPredicho)  \n",
    "    print(\"Accuracy:\",acc_comite)\n",
    "    f1=f1_score(etiq_t, trainPredicho, average='macro') \n",
    "    print(\"F1-macro:\",f1)\n",
    "    f1_no=f1_score(etiq_t, trainPredicho, average=None)  \n",
    "    print(\"F1-score SDQC:\",f1_no)\n",
    "    voting_test['acc'].append(acc_comite)\n",
    "    voting_test['macro'].append(f1)\n",
    "    voting_test['none'].append(f1_no)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TEST] Acc Best comité: 0.8507772020725389\n",
      "[TEST] F1-score NIPS Best comité: [0.91648735 0.03777329 0.03033263 0.75043578]\n",
      "[TEST] F1 macro Best comité: 0.43375726259340214\n",
      "-----------\n",
      "[TEST] Acc Norm comité: 0.861987753179463\n",
      "[TEST] F1-score NIPS Norm comité: [0.92317723 0.         0.         0.78206736]\n",
      "[TEST] F1 macro Norm comité: 0.4263111459847867\n",
      "-----------\n",
      "[TEST] Acc Voting comité: 0.86547338671691\n",
      "[TEST] F1-score NIPS Voting comité: [0.92522014 0.         0.         0.79263126]\n",
      "[TEST] F1 macro Voting comité: 0.4294628497052077\n"
     ]
    }
   ],
   "source": [
    "#### 15-8 epochs\n",
    "\n",
    "m=5.0\n",
    "\n",
    "print (\"[TEST] Acc Best comité:\",np.sum(np.asarray(best_test['acc'])/m))\n",
    "temp=np.zeros(4)\n",
    "for result in best_test['none']:\n",
    "    temp+=result\n",
    "print (\"[TEST] F1-score NIPS Best comité:\" ,temp/m)\n",
    "print (\"[TEST] F1 macro Best comité:\",np.sum(np.asarray(best_test['macro'])/m))\n",
    "print (\"-----------\")\n",
    "print (\"[TEST] Acc Norm comité:\",np.sum(np.asarray(norm_test['acc'])/m))\n",
    "temp=np.zeros(4)\n",
    "for result in norm_test['none']:\n",
    "    temp+=result\n",
    "print (\"[TEST] F1-score NIPS Norm comité:\" ,temp/m)\n",
    "print (\"[TEST] F1 macro Norm comité:\",np.sum(np.asarray(norm_test['macro'])/m))\n",
    "print (\"-----------\")\n",
    "print (\"[TEST] Acc Voting comité:\",np.sum(np.asarray(voting_test['acc'])/m))\n",
    "temp=np.zeros(4)\n",
    "for result in voting_test['none']:\n",
    "    temp+=result\n",
    "print (\"[TEST] F1-score NIPS Voting comité:\" ,temp/m)\n",
    "print (\"[TEST] F1 macro Voting comité:\",np.sum(np.asarray(voting_test['macro'])/m))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TEST] Acc Best comité: 0.8468205369759774\n",
      "[TEST] F1-score NIPS Best comité: [0.91406858 0.04135088 0.05606891 0.75117568]\n",
      "[TEST] F1 macro Best comité: 0.4406660129178516\n",
      "-----------\n",
      "[TEST] Acc Norm comité: 0.8606688648139427\n",
      "[TEST] F1-score NIPS Norm comité: [0.92220497 0.         0.         0.78497927]\n",
      "[TEST] F1 macro Norm comité: 0.42679605912453833\n",
      "-----------\n",
      "[TEST] Acc Voting comité: 0.8653791804050872\n",
      "[TEST] F1-score NIPS Voting comité: [0.92485427 0.         0.00512821 0.79685099]\n",
      "[TEST] F1 macro Voting comité: 0.4317083656478837\n"
     ]
    }
   ],
   "source": [
    "m=5.0\n",
    "\n",
    "print (\"[TEST] Acc Best comité:\",np.sum(np.asarray(best_test['acc'])/m))\n",
    "temp=np.zeros(4)\n",
    "for result in best_test['none']:\n",
    "    temp+=result\n",
    "print (\"[TEST] F1-score NIPS Best comité:\" ,temp/m)\n",
    "print (\"[TEST] F1 macro Best comité:\",np.sum(np.asarray(best_test['macro'])/m))\n",
    "print (\"-----------\")\n",
    "print (\"[TEST] Acc Norm comité:\",np.sum(np.asarray(norm_test['acc'])/m))\n",
    "temp=np.zeros(4)\n",
    "for result in norm_test['none']:\n",
    "    temp+=result\n",
    "print (\"[TEST] F1-score NIPS Norm comité:\" ,temp/m)\n",
    "print (\"[TEST] F1 macro Norm comité:\",np.sum(np.asarray(norm_test['macro'])/m))\n",
    "print (\"-----------\")\n",
    "print (\"[TEST] Acc Voting comité:\",np.sum(np.asarray(voting_test['acc'])/m))\n",
    "temp=np.zeros(4)\n",
    "for result in voting_test['none']:\n",
    "    temp+=result\n",
    "print (\"[TEST] F1-score NIPS Voting comité:\" ,temp/m)\n",
    "print (\"[TEST] F1 macro Voting comité:\",np.sum(np.asarray(voting_test['macro'])/m))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.0 cw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------- EJECUCIÓN #1----------------\n",
      "Agregando predicciones Test set rnn3\n",
      "--------TESTING SET--------\n",
      "\n",
      "Committee Best Fit\n",
      "Accuracy: 0.8516250588789449\n",
      "F1-macro: 0.46930185603667507\n",
      "F1-score NIPS: [0.91748983 0.11290323 0.11494253 0.73187184]\n",
      "\n",
      "Committee Norm\n",
      "Accuracy: 0.8601036269430051\n",
      "F1-macro: 0.4582785508143068\n",
      "F1-score NIPS: [0.92001155 0.07079646 0.07228916 0.77001704]\n",
      "\n",
      "Committee Voting\n",
      "Accuracy: 0.8591615638247763\n",
      "F1-macro: 0.483824932617313\n",
      "F1-score NIPS: [0.91868512 0.12173913 0.13793103 0.75694444]\n",
      "\n",
      "---------------- EJECUCIÓN #2----------------\n",
      "Agregando predicciones Test set rnn3\n",
      "--------TESTING SET--------\n",
      "\n",
      "Committee Best Fit\n",
      "Accuracy: 0.8327837965143665\n",
      "F1-macro: 0.5037770191998525\n",
      "F1-score NIPS: [0.91159251 0.26506024 0.18644068 0.65201465]\n",
      "\n",
      "Committee Norm\n",
      "Accuracy: 0.8610456900612341\n",
      "F1-macro: 0.5288769375844222\n",
      "F1-score NIPS: [0.92186589 0.21374046 0.22222222 0.75767918]\n",
      "\n",
      "Committee Voting\n",
      "Accuracy: 0.8497409326424871\n",
      "F1-macro: 0.5243393354734585\n",
      "F1-score NIPS: [0.91756481 0.25352113 0.21428571 0.71198569]\n",
      "\n",
      "---------------- EJECUCIÓN #3----------------\n",
      "Agregando predicciones Test set rnn3\n",
      "--------TESTING SET--------\n",
      "\n",
      "Committee Best Fit\n",
      "Accuracy: 0.8478568064060292\n",
      "F1-macro: 0.4798504300270242\n",
      "F1-score NIPS: [0.91651543 0.0952381  0.15730337 0.75034483]\n",
      "\n",
      "Committee Norm\n",
      "Accuracy: 0.8709373528026377\n",
      "F1-macro: 0.47649281781752795\n",
      "F1-score NIPS: [0.92833779 0.         0.16666667 0.81096681]\n",
      "\n",
      "Committee Voting\n",
      "Accuracy: 0.8728214790390956\n",
      "F1-macro: 0.5165758401055975\n",
      "F1-score NIPS: [0.92999702 0.05504587 0.27659574 0.80466472]\n",
      "\n",
      "---------------- EJECUCIÓN #4----------------\n",
      "Agregando predicciones Test set rnn3\n",
      "--------TESTING SET--------\n",
      "\n",
      "Committee Best Fit\n",
      "Accuracy: 0.8492699010833726\n",
      "F1-macro: 0.45345591731120627\n",
      "F1-score NIPS: [0.91464821 0.08928571 0.09756098 0.71232877]\n",
      "\n",
      "Committee Norm\n",
      "Accuracy: 0.8535091851154027\n",
      "F1-macro: 0.4262004259096595\n",
      "F1-score NIPS: [0.9160437  0.01869159 0.02564103 0.74442539]\n",
      "\n",
      "Committee Voting\n",
      "Accuracy: 0.8544512482336316\n",
      "F1-macro: 0.44672089648927193\n",
      "F1-score NIPS: [0.91709845 0.10526316 0.02531646 0.73920553]\n",
      "\n",
      "---------------- EJECUCIÓN #5----------------\n",
      "Agregando predicciones Test set rnn3\n",
      "--------TESTING SET--------\n",
      "\n",
      "Committee Best Fit\n",
      "Accuracy: 0.8497409326424871\n",
      "F1-macro: 0.5442229391273977\n",
      "F1-score NIPS: [0.91659084 0.26751592 0.24242424 0.75036075]\n",
      "\n",
      "Committee Norm\n",
      "Accuracy: 0.8756476683937824\n",
      "F1-macro: 0.5209194142370025\n",
      "F1-score NIPS: [0.93139293 0.13559322 0.2        0.81669151]\n",
      "\n",
      "Committee Voting\n",
      "Accuracy: 0.8761186999528968\n",
      "F1-macro: 0.5383057378387583\n",
      "F1-score NIPS: [0.93179122 0.19834711 0.21276596 0.81031866]\n"
     ]
    }
   ],
   "source": [
    "prob=0.0\n",
    "path='../new_glove_augmented/harassment_baselines/'\n",
    "bs=32\n",
    "    \n",
    "fs_macro_val={'cnn1':[], 'cnn2':[], 'rnn1':[],'rnn2':[],'rnn3':[]}\n",
    "fs_none_val={'cnn1':[], 'cnn2':[], 'rnn1':[],'rnn2':[],'rnn3':[]}\n",
    "accs_val={'cnn1':[], 'cnn2':[], 'rnn1':[],'rnn2':[],'rnn3':[]}\n",
    "confusions_val={'cnn1':[], 'cnn2':[], 'rnn1':[],'rnn2':[],'rnn3':[]}\n",
    "\n",
    "fs_macro_test={'cnn1':[], 'cnn2':[], 'rnn1':[],'rnn2':[],'rnn3':[]}\n",
    "fs_none_test={'cnn1':[], 'cnn2':[], 'rnn1':[],'rnn2':[],'rnn3':[]}\n",
    "accs_test={'cnn1':[], 'cnn2':[], 'rnn1':[],'rnn2':[],'rnn3':[]}\n",
    "confusions_test={'cnn1':[], 'cnn2':[], 'rnn1':[],'rnn2':[],'rnn3':[]}\n",
    "\n",
    "best={'acc':[], 'none':[], 'macro':[]}\n",
    "norm={'acc':[], 'none':[], 'macro':[]}\n",
    "voting={'acc':[], 'none':[], 'macro':[]}\n",
    "best_test={'acc':[], 'none':[], 'macro':[]}\n",
    "norm_test={'acc':[], 'none':[], 'macro':[]}\n",
    "voting_test={'acc':[], 'none':[], 'macro':[]}\n",
    "\n",
    "for j in range(1,6):\n",
    "    print (\"\")\n",
    "    print (\"---------------- EJECUCIÓN #\"+str(j)+'----------------')\n",
    "    cnn1= load_model(path+str(prob)+'_cw/cnn1_'+str(j)+'-exec_15e.h5', custom_objects={'FocalLoss': focal_loss, 'focal_loss_fixed': focal_loss()})  \n",
    "    cnn2= load_model(path+str(prob)+'_cw/cnn2_'+str(j)+'-exec_15e.h5', custom_objects={'FocalLoss': focal_loss, 'focal_loss_fixed': focal_loss()}) \n",
    "    rnn1= load_model(path+str(prob)+'_cw/rnn1_'+str(j)+'-exec_8e.h5', custom_objects={'FocalLoss': focal_loss, 'focal_loss_fixed': focal_loss()}) \n",
    "    rnn2= load_model(path+str(prob)+'_cw/rnn2_'+str(j)+'-exec_15e.h5') \n",
    "    rnn3= load_model(path+str(prob)+'_cw/rnn3_'+str(j)+'-exec_8e.h5', custom_objects={'FocalLoss': focal_loss, 'focal_loss_fixed': focal_loss()}) \n",
    "    \n",
    "    list_models=['cnn1', 'cnn2', 'rnn1', 'rnn2', 'rnn3']\n",
    "    index_models=np.arange(5)\n",
    "    dict_models=dict((key, value) for (key, value) in zip(index_models,list_models))\n",
    "    modelos=[cnn1, cnn2, rnn1, rnn2, rnn3]\n",
    "    ind=np.arange(5)\n",
    "    dict_trainedModel=dict((key, value) for (key, value) in zip(ind,modelos))\n",
    "    \n",
    "    \"\"\" print (\"Agregando predicciones Val set\")\n",
    "    for i in dict_trainedModel.keys(): \n",
    "        a_evaluar=dict_models[i]\n",
    "        predicho=dict_trainedModel[i].predict(x_val, batch_size=bs)\n",
    "        predicho=[np.argmax(pred) for pred in predicho]\n",
    "        acc= accuracy_score(etiq_v, predicho)  \n",
    "        f1_ma=f1_score(etiq_v, predicho, average='macro')\n",
    "        f1_no=f1_score(etiq_v, predicho, average=None)\n",
    "        matriz=normalize(confusion_matrix(etiq_v, predicho))\n",
    "        fs_macro_val[dict_models[i]].append(f1_ma)\n",
    "        fs_none_val[dict_models[i]].append(f1_no)\n",
    "        accs_val[dict_models[i]].append(acc)\n",
    "        confusions_val[dict_models[i]].append(matriz)\n",
    "        \n",
    "    predicciones_all_val=[]\n",
    "    for i in dict_trainedModel.keys(): \n",
    "        a_evaluar=dict_models[i]\n",
    "        predicciones_all_val.append(dict_trainedModel[i].predict(x_val, batch_size=bs))\n",
    "    predichos_all_val=np.concatenate(np.asarray(predicciones_all_val),axis=-1)\n",
    "    \"\"\"\n",
    "    print (\"Agregando predicciones Test set\", dict_models[i])    \n",
    "    for i in dict_trainedModel.keys(): \n",
    "        a_evaluar=dict_models[i]\n",
    "        predicho=dict_trainedModel[i].predict(x_test, batch_size=bs)\n",
    "        predicho=[np.argmax(pred) for pred in predicho]\n",
    "        acc= accuracy_score(etiq_t, predicho)  \n",
    "        f1_ma=f1_score(etiq_t, predicho, average='macro')\n",
    "        f1_no=f1_score(etiq_t, predicho, average=None)\n",
    "        matriz=normalize(confusion_matrix(etiq_t, predicho))\n",
    "        fs_macro_test[dict_models[i]].append(f1_ma)\n",
    "        fs_none_test[dict_models[i]].append(f1_no)\n",
    "        accs_test[dict_models[i]].append(acc)\n",
    "        confusions_test[dict_models[i]].append(matriz)\n",
    "\n",
    "    predicciones_all_test=[]\n",
    "    bs=32\n",
    "    for i in dict_trainedModel.keys(): \n",
    "        a_evaluar=dict_models[i]\n",
    "        predicciones_all_test.append(dict_trainedModel[i].predict(x_test, batch_size=bs))\n",
    "    predichos_all_test=np.concatenate(np.asarray(predicciones_all_test),axis=-1)\n",
    "    \"\"\"\n",
    "    print (\"--------VALIDATION SET--------\")\n",
    "    \n",
    "    print (\"\\nCommittee Best Fit\")\n",
    "    trainPredicho, maxPredicho, confianzas = predecir_modelos(bs,predichos_all_val)\n",
    "    acc_comite= accuracy_score(etiq_v, trainPredicho)  \n",
    "    print(\"Accuracy:\",acc_comite)\n",
    "    f1=f1_score(etiq_v, trainPredicho, average='macro') \n",
    "    print(\"F1-macro:\",f1)\n",
    "    f1_no=f1_score(etiq_v, trainPredicho, average=None)  \n",
    "    print(\"F1-score NIPS:\",f1)\n",
    "    best['acc'].append(acc_comite)\n",
    "    best['macro'].append(f1)\n",
    "    best['none'].append(f1_no)\n",
    "    \n",
    "    print (\"\\nCommittee Norm\")\n",
    "    trainPredicho, maxPredicho, confianzas = predecir_modelos_norm(bs,predichos_all_val)\n",
    "    acc_comite= accuracy_score(etiq_v, trainPredicho)  \n",
    "    print(\"Accuracy:\",acc_comite)\n",
    "    f1=f1_score(etiq_v, trainPredicho, average='macro') \n",
    "    print(\"F1-macro:\",f1)\n",
    "    f1_no=f1_score(etiq_v, trainPredicho, average=None) \n",
    "    print(\"F1-score NIPS:\",f1)\n",
    "    norm['acc'].append(acc_comite)\n",
    "    norm['macro'].append(f1)\n",
    "    norm['none'].append(f1_no)\n",
    "\n",
    "    print (\"\\nCommittee Voting\")\n",
    "    trainPredicho, maxPredicho, confianzas = predecir_modelos_average(bs,predichos_all_val)\n",
    "    acc_comite= accuracy_score(etiq_v, trainPredicho)  \n",
    "    print(\"Accuracy:\",acc_comite)\n",
    "    f1=f1_score(etiq_v, trainPredicho, average='macro') \n",
    "    print(\"F1-macro:\",f1)\n",
    "    f1_no=f1_score(etiq_v, trainPredicho, average=None)  \n",
    "    print(\"F1-score NIPS:\",f1)\n",
    "    voting['acc'].append(acc_comite)\n",
    "    voting['macro'].append(f1)\n",
    "    voting['none'].append(f1_no)\"\"\"\n",
    "    \n",
    "    print (\"--------TESTING SET--------\")\n",
    "    \n",
    "    print (\"\\nCommittee Best Fit\")\n",
    "    trainPredicho, maxPredicho, confianzas = predecir_modelos(bs,predichos_all_test)\n",
    "    acc_comite= accuracy_score(etiq_t, trainPredicho)  \n",
    "    print(\"Accuracy:\",acc_comite)\n",
    "    f1=f1_score(etiq_t, trainPredicho, average='macro') \n",
    "    print(\"F1-macro:\",f1)\n",
    "    f1_no=f1_score(etiq_t, trainPredicho, average=None)  \n",
    "    print(\"F1-score NIPS:\",f1_no)\n",
    "    best_test['acc'].append(acc_comite)\n",
    "    best_test['macro'].append(f1)\n",
    "    best_test['none'].append(f1_no)\n",
    "\n",
    "    print (\"\\nCommittee Norm\")\n",
    "    trainPredicho, maxPredicho, confianzas = predecir_modelos_norm(bs,predichos_all_test)\n",
    "    acc_comite= accuracy_score(etiq_t, trainPredicho)  \n",
    "    print(\"Accuracy:\",acc_comite)\n",
    "    f1=f1_score(etiq_t, trainPredicho, average='macro') \n",
    "    print(\"F1-macro:\",f1)\n",
    "    f1_no=f1_score(etiq_t, trainPredicho, average=None) \n",
    "    print(\"F1-score NIPS:\",f1_no)\n",
    "    norm_test['acc'].append(acc_comite)\n",
    "    norm_test['macro'].append(f1)\n",
    "    norm_test['none'].append(f1_no)\n",
    "\n",
    "    print (\"\\nCommittee Voting\")\n",
    "    trainPredicho, maxPredicho, confianzas = predecir_modelos_average(bs,predichos_all_test)\n",
    "    acc_comite= accuracy_score(etiq_t, trainPredicho)  \n",
    "    print(\"Accuracy:\",acc_comite)\n",
    "    f1=f1_score(etiq_t, trainPredicho, average='macro') \n",
    "    print(\"F1-macro:\",f1)\n",
    "    f1_no=f1_score(etiq_t, trainPredicho, average=None)  \n",
    "    print(\"F1-score NIPS:\",f1_no)\n",
    "    voting_test['acc'].append(acc_comite)\n",
    "    voting_test['macro'].append(f1)\n",
    "    voting_test['none'].append(f1_no)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Con 15 y 8 epochs...\n",
      "\n",
      "[TEST] Acc Best comité: 0.84625529910504\n",
      "[TEST] F1-score NIPS Best comité: [0.91536736 0.16600064 0.15973436 0.71938417]\n",
      "[TEST] F1 macro Best comité: 0.49012163234043116\n",
      "-----------\n",
      "[TEST] Acc Norm comité: 0.8642487046632125\n",
      "[TEST] F1-score NIPS Norm comité: [0.92353037 0.08776435 0.13736381 0.77995598]\n",
      "[TEST] F1 macro Norm comité: 0.4821536292725838\n",
      "-----------\n",
      "[TEST] Acc Voting comité: 0.8624587847385776\n",
      "[TEST] F1-score NIPS Voting comité: [0.92302732 0.14678328 0.17337898 0.76462381]\n",
      "[TEST] F1 macro Voting comité: 0.5019533485048798\n"
     ]
    }
   ],
   "source": [
    "print (\"Con 15 y 8 epochs...\\n\")\n",
    "m=5.0\n",
    "\n",
    "print (\"[TEST] Acc Best comité:\",np.sum(np.asarray(best_test['acc'])/m))\n",
    "temp=np.zeros(4)\n",
    "for result in best_test['none']:\n",
    "    temp+=result\n",
    "print (\"[TEST] F1-score NIPS Best comité:\" ,temp/m)\n",
    "print (\"[TEST] F1 macro Best comité:\",np.sum(np.asarray(best_test['macro'])/m))\n",
    "print (\"-----------\")\n",
    "print (\"[TEST] Acc Norm comité:\",np.sum(np.asarray(norm_test['acc'])/m))\n",
    "temp=np.zeros(4)\n",
    "for result in norm_test['none']:\n",
    "    temp+=result\n",
    "print (\"[TEST] F1-score NIPS Norm comité:\" ,temp/m)\n",
    "print (\"[TEST] F1 macro Norm comité:\",np.sum(np.asarray(norm_test['macro'])/m))\n",
    "print (\"-----------\")\n",
    "print (\"[TEST] Acc Voting comité:\",np.sum(np.asarray(voting_test['acc'])/m))\n",
    "temp=np.zeros(4)\n",
    "for result in voting_test['none']:\n",
    "    temp+=result\n",
    "print (\"[TEST] F1-score NIPS Voting comité:\" ,temp/m)\n",
    "print (\"[TEST] F1 macro Voting comité:\",np.sum(np.asarray(voting_test['macro'])/m))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TEST] Acc Best comité: 0.8424870466321244\n",
      "[TEST] F1-score NIPS Best comité: [0.91344209 0.15978576 0.1698437  0.73865242]\n",
      "[TEST] F1 macro Best comité: 0.4954309951957939\n",
      "-----------\n",
      "[TEST] Acc Norm comité: 0.8696184644371172\n",
      "[TEST] F1-score NIPS Norm comité: [0.92761455 0.14946848 0.11116994 0.80520837]\n",
      "[TEST] F1 macro Norm comité: 0.49836533854148113\n",
      "-----------\n",
      "[TEST] Acc Voting comité: 0.8700894959962318\n",
      "[TEST] F1-score NIPS Voting comité: [0.92713953 0.22937507 0.18281202 0.81110401]\n",
      "[TEST] F1 macro Voting comité: 0.5376076561310092\n"
     ]
    }
   ],
   "source": [
    "m=5.0\n",
    "\n",
    "print (\"[TEST] Acc Best comité:\",np.sum(np.asarray(best_test['acc'])/m))\n",
    "temp=np.zeros(4)\n",
    "for result in best_test['none']:\n",
    "    temp+=result\n",
    "print (\"[TEST] F1-score NIPS Best comité:\" ,temp/m)\n",
    "print (\"[TEST] F1 macro Best comité:\",np.sum(np.asarray(best_test['macro'])/m))\n",
    "print (\"-----------\")\n",
    "print (\"[TEST] Acc Norm comité:\",np.sum(np.asarray(norm_test['acc'])/m))\n",
    "temp=np.zeros(4)\n",
    "for result in norm_test['none']:\n",
    "    temp+=result\n",
    "print (\"[TEST] F1-score NIPS Norm comité:\" ,temp/m)\n",
    "print (\"[TEST] F1 macro Norm comité:\",np.sum(np.asarray(norm_test['macro'])/m))\n",
    "print (\"-----------\")\n",
    "print (\"[TEST] Acc Voting comité:\",np.sum(np.asarray(voting_test['acc'])/m))\n",
    "temp=np.zeros(4)\n",
    "for result in voting_test['none']:\n",
    "    temp+=result\n",
    "print (\"[TEST] F1-score NIPS Voting comité:\" ,temp/m)\n",
    "print (\"[TEST] F1 macro Voting comité:\",np.sum(np.asarray(voting_test['macro'])/m))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------- EJECUCIÓN #1----------------\n",
      "Agregando predicciones Test set\n",
      "--------TESTING SET--------\n",
      "\n",
      "Committee Best Fit\n",
      "Accuracy: 0.8186528497409327\n",
      "F1-macro: 0.5266927336555304\n",
      "F1-score NIPS: [0.89801155 0.23589744 0.24074074 0.73212121]\n",
      "\n",
      "Committee Norm\n",
      "Accuracy: 0.8506829957607159\n",
      "F1-macro: 0.4730888141701334\n",
      "F1-score NIPS: [0.92008639 0.10606061 0.09756098 0.76864728]\n",
      "\n",
      "Committee Voting\n",
      "Accuracy: 0.8539802166745172\n",
      "F1-macro: 0.48171642863397834\n",
      "F1-score NIPS: [0.92236599 0.13333333 0.0952381  0.7759283 ]\n",
      "\n",
      "---------------- EJECUCIÓN #2----------------\n",
      "Agregando predicciones Test set\n",
      "--------TESTING SET--------\n",
      "\n",
      "Committee Best Fit\n",
      "Accuracy: 0.8459726801695714\n",
      "F1-macro: 0.4766334958570899\n",
      "F1-score NIPS: [0.91756053 0.1369863  0.08791209 0.76407507]\n",
      "\n",
      "Committee Norm\n",
      "Accuracy: 0.8662270372114932\n",
      "F1-macro: 0.44744623774865466\n",
      "F1-score NIPS: [0.92831325 0.03669725 0.02531646 0.79945799]\n",
      "\n",
      "Committee Voting\n",
      "Accuracy: 0.8648139425341498\n",
      "F1-macro: 0.4473615881127515\n",
      "F1-score NIPS: [0.92678518 0.03571429 0.02531646 0.80163043]\n",
      "\n",
      "---------------- EJECUCIÓN #3----------------\n",
      "Agregando predicciones Test set\n",
      "--------TESTING SET--------\n",
      "\n",
      "Committee Best Fit\n",
      "Accuracy: 0.8502119642016015\n",
      "F1-macro: 0.5051824000610848\n",
      "F1-score NIPS: [0.91923194 0.13636364 0.21276596 0.75236806]\n",
      "\n",
      "Committee Norm\n",
      "Accuracy: 0.8634008478568064\n",
      "F1-macro: 0.457217996118521\n",
      "F1-score NIPS: [0.92698983 0.07079646 0.05128205 0.77980365]\n",
      "\n",
      "Committee Voting\n",
      "Accuracy: 0.8662270372114932\n",
      "F1-macro: 0.47370332277906885\n",
      "F1-score NIPS: [0.92799522 0.10169492 0.07594937 0.78917379]\n",
      "\n",
      "---------------- EJECUCIÓN #4----------------\n",
      "Agregando predicciones Test set\n",
      "--------TESTING SET--------\n",
      "\n",
      "Committee Best Fit\n",
      "Accuracy: 0.8577484691474329\n",
      "F1-macro: 0.49458950514186534\n",
      "F1-score NIPS: [0.92147531 0.09677419 0.18367347 0.77643505]\n",
      "\n",
      "Committee Norm\n",
      "Accuracy: 0.8657560056523788\n",
      "F1-macro: 0.4525991716930228\n",
      "F1-score NIPS: [0.92497793 0.03603604 0.04938272 0.8       ]\n",
      "\n",
      "Committee Voting\n",
      "Accuracy: 0.867169100329722\n",
      "F1-macro: 0.47004763379043807\n",
      "F1-score NIPS: [0.92556634 0.03636364 0.11764706 0.8006135 ]\n",
      "\n",
      "---------------- EJECUCIÓN #5----------------\n",
      "Agregando predicciones Test set\n",
      "--------TESTING SET--------\n",
      "\n",
      "Committee Best Fit\n",
      "Accuracy: 0.8403203014601979\n",
      "F1-macro: 0.5212537071382115\n",
      "F1-score NIPS: [0.91111799 0.18918919 0.22608696 0.75862069]\n",
      "\n",
      "Committee Norm\n",
      "Accuracy: 0.8563353744700894\n",
      "F1-macro: 0.4540775034881126\n",
      "F1-score NIPS: [0.92205438 0.06896552 0.04705882 0.77823129]\n",
      "\n",
      "Committee Voting\n",
      "Accuracy: 0.8601036269430051\n",
      "F1-macro: 0.472085219285265\n",
      "F1-score NIPS: [0.92484153 0.06956522 0.10989011 0.78404402]\n"
     ]
    }
   ],
   "source": [
    "prob=0.15\n",
    "path='../new_glove_augmented/harassment_baselines/'\n",
    "bs=32\n",
    "    \n",
    "fs_macro_val={'cnn1':[], 'cnn2':[], 'rnn1':[],'rnn2':[],'rnn3':[]}\n",
    "fs_none_val={'cnn1':[], 'cnn2':[], 'rnn1':[],'rnn2':[],'rnn3':[]}\n",
    "accs_val={'cnn1':[], 'cnn2':[], 'rnn1':[],'rnn2':[],'rnn3':[]}\n",
    "confusions_val={'cnn1':[], 'cnn2':[], 'rnn1':[],'rnn2':[],'rnn3':[]}\n",
    "\n",
    "fs_macro_test={'cnn1':[], 'cnn2':[], 'rnn1':[],'rnn2':[],'rnn3':[]}\n",
    "fs_none_test={'cnn1':[], 'cnn2':[], 'rnn1':[],'rnn2':[],'rnn3':[]}\n",
    "accs_test={'cnn1':[], 'cnn2':[], 'rnn1':[],'rnn2':[],'rnn3':[]}\n",
    "confusions_test={'cnn1':[], 'cnn2':[], 'rnn1':[],'rnn2':[],'rnn3':[]}\n",
    "\n",
    "best={'acc':[], 'none':[], 'macro':[]}\n",
    "norm={'acc':[], 'none':[], 'macro':[]}\n",
    "voting={'acc':[], 'none':[], 'macro':[]}\n",
    "best_test={'acc':[], 'none':[], 'macro':[]}\n",
    "norm_test={'acc':[], 'none':[], 'macro':[]}\n",
    "voting_test={'acc':[], 'none':[], 'macro':[]}\n",
    "\n",
    "for j in range(1,6):\n",
    "    print (\"\")\n",
    "    print (\"---------------- EJECUCIÓN #\"+str(j)+'----------------')\n",
    "    cnn1= load_model(path+str(prob)+'/cnn1_'+str(j)+'-exec_app1_Top_1_7e.h5', custom_objects={'FocalLoss': focal_loss, 'focal_loss_fixed': focal_loss()})  \n",
    "    cnn2= load_model(path+str(prob)+'/cnn2_'+str(j)+'-exec_app1_Top_1_7e.h5', custom_objects={'FocalLoss': focal_loss, 'focal_loss_fixed': focal_loss()}) \n",
    "    rnn1= load_model(path+str(prob)+'/rnn1_'+str(j)+'-exec_app1_Top_1_4e.h5', custom_objects={'FocalLoss': focal_loss, 'focal_loss_fixed': focal_loss()}) \n",
    "    rnn2= load_model(path+str(prob)+'/rnn2_'+str(j)+'-exec_app1_Top_1_7e.h5') \n",
    "    rnn3= load_model(path+str(prob)+'/rnn3_'+str(j)+'-exec_app1_Top_1_4e.h5', custom_objects={'FocalLoss': focal_loss, 'focal_loss_fixed': focal_loss()}) \n",
    "    \n",
    "    list_models=['cnn1', 'cnn2', 'rnn1', 'rnn2', 'rnn3']\n",
    "    index_models=np.arange(5)\n",
    "    dict_models=dict((key, value) for (key, value) in zip(index_models,list_models))\n",
    "    modelos=[cnn1, cnn2, rnn1, rnn2, rnn3]\n",
    "    ind=np.arange(5)\n",
    "    dict_trainedModel=dict((key, value) for (key, value) in zip(ind,modelos))\n",
    "        \n",
    "    print (\"Agregando predicciones Test set\")    \n",
    "    for i in dict_trainedModel.keys(): \n",
    "        a_evaluar=dict_models[i]\n",
    "        predicho=dict_trainedModel[i].predict(x_test, batch_size=bs)\n",
    "        predicho=[np.argmax(pred) for pred in predicho]\n",
    "        acc= accuracy_score(etiq_t, predicho)  \n",
    "        f1_ma=f1_score(etiq_t, predicho, average='macro')\n",
    "        f1_no=f1_score(etiq_t, predicho, average=None)\n",
    "        matriz=normalize(confusion_matrix(etiq_t, predicho))\n",
    "        fs_macro_test[dict_models[i]].append(f1_ma)\n",
    "        fs_none_test[dict_models[i]].append(f1_no)\n",
    "        accs_test[dict_models[i]].append(acc)\n",
    "        confusions_test[dict_models[i]].append(matriz)\n",
    "\n",
    "    predicciones_all_test=[]\n",
    "    bs=32\n",
    "    for i in dict_trainedModel.keys(): \n",
    "        a_evaluar=dict_models[i]\n",
    "        predicciones_all_test.append(dict_trainedModel[i].predict(x_test, batch_size=bs))\n",
    "    predichos_all_test=np.concatenate(np.asarray(predicciones_all_test),axis=-1)\n",
    "    \n",
    "       \n",
    "    print (\"--------TESTING SET--------\")\n",
    "    \n",
    "    print (\"\\nCommittee Best Fit\")\n",
    "    trainPredicho, maxPredicho, confianzas = predecir_modelos(bs,predichos_all_test)\n",
    "    acc_comite= accuracy_score(etiq_t, trainPredicho)  \n",
    "    print(\"Accuracy:\",acc_comite)\n",
    "    f1=f1_score(etiq_t, trainPredicho, average='macro') \n",
    "    print(\"F1-macro:\",f1)\n",
    "    f1_no=f1_score(etiq_t, trainPredicho, average=None)  \n",
    "    print(\"F1-score NIPS:\",f1_no)\n",
    "    best_test['acc'].append(acc_comite)\n",
    "    best_test['macro'].append(f1)\n",
    "    best_test['none'].append(f1_no)\n",
    "\n",
    "    print (\"\\nCommittee Norm\")\n",
    "    trainPredicho, maxPredicho, confianzas = predecir_modelos_norm(bs,predichos_all_test)\n",
    "    acc_comite= accuracy_score(etiq_t, trainPredicho)  \n",
    "    print(\"Accuracy:\",acc_comite)\n",
    "    f1=f1_score(etiq_t, trainPredicho, average='macro') \n",
    "    print(\"F1-macro:\",f1)\n",
    "    f1_no=f1_score(etiq_t, trainPredicho, average=None) \n",
    "    print(\"F1-score NIPS:\",f1_no)\n",
    "    norm_test['acc'].append(acc_comite)\n",
    "    norm_test['macro'].append(f1)\n",
    "    norm_test['none'].append(f1_no)\n",
    "\n",
    "    print (\"\\nCommittee Voting\")\n",
    "    trainPredicho, maxPredicho, confianzas = predecir_modelos_average(bs,predichos_all_test)\n",
    "    acc_comite= accuracy_score(etiq_t, trainPredicho)  \n",
    "    print(\"Accuracy:\",acc_comite)\n",
    "    f1=f1_score(etiq_t, trainPredicho, average='macro') \n",
    "    print(\"F1-macro:\",f1)\n",
    "    f1_no=f1_score(etiq_t, trainPredicho, average=None)  \n",
    "    print(\"F1-score NIPS:\",f1_no)\n",
    "    voting_test['acc'].append(acc_comite)\n",
    "    voting_test['macro'].append(f1)\n",
    "    voting_test['none'].append(f1_no)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "con 7 y 4 epochs... \n",
      "\n",
      "[TEST] Acc Best comité: 0.8425812529439473\n",
      "[TEST] F1-score NIPS Best comité: [0.91347946 0.15904215 0.19023584 0.75672402]\n",
      "[TEST] F1 macro Best comité: 0.5048703683707564\n",
      "-----------\n",
      "[TEST] Acc Norm comité: 0.8604804521902967\n",
      "[TEST] F1-score NIPS Norm comité: [0.92448436 0.06371117 0.0541202  0.78522804]\n",
      "[TEST] F1 macro Norm comité: 0.45688594464368887\n",
      "-----------\n",
      "[TEST] Acc Voting comité: 0.8624587847385775\n",
      "[TEST] F1-score NIPS Voting comité: [0.92551085 0.07533428 0.08480822 0.79027801]\n",
      "[TEST] F1 macro Voting comité: 0.4689828385203003\n"
     ]
    }
   ],
   "source": [
    "print (\"con 7 y 4 epochs... \\n\")\n",
    "\n",
    "m=5.0\n",
    "\n",
    "print (\"[TEST] Acc Best comité:\",np.sum(np.asarray(best_test['acc'])/m))\n",
    "temp=np.zeros(4)\n",
    "for result in best_test['none']:\n",
    "    temp+=result\n",
    "print (\"[TEST] F1-score NIPS Best comité:\" ,temp/m)\n",
    "print (\"[TEST] F1 macro Best comité:\",np.sum(np.asarray(best_test['macro'])/m))\n",
    "print (\"-----------\")\n",
    "print (\"[TEST] Acc Norm comité:\",np.sum(np.asarray(norm_test['acc'])/m))\n",
    "temp=np.zeros(4)\n",
    "for result in norm_test['none']:\n",
    "    temp+=result\n",
    "print (\"[TEST] F1-score NIPS Norm comité:\" ,temp/m)\n",
    "print (\"[TEST] F1 macro Norm comité:\",np.sum(np.asarray(norm_test['macro'])/m))\n",
    "print (\"-----------\")\n",
    "print (\"[TEST] Acc Voting comité:\",np.sum(np.asarray(voting_test['acc'])/m))\n",
    "temp=np.zeros(4)\n",
    "for result in voting_test['none']:\n",
    "    temp+=result\n",
    "print (\"[TEST] F1-score NIPS Voting comité:\" ,temp/m)\n",
    "print (\"[TEST] F1 macro Voting comité:\",np.sum(np.asarray(voting_test['macro'])/m))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TEST] Acc Best comité: 0.8399434762129062\n",
      "[TEST] F1-score NIPS Best comité: [0.91021617 0.09467849 0.14992861 0.74687985]\n",
      "[TEST] F1 macro Best comité: 0.4754257831049076\n",
      "-----------\n",
      "[TEST] Acc Norm comité: 0.856806406029204\n",
      "[TEST] F1-score NIPS Norm comité: [0.92056639 0.04679765 0.03509661 0.77534944]\n",
      "[TEST] F1 macro Norm comité: 0.4444525226332764\n",
      "-----------\n",
      "[TEST] Acc Voting comité: 0.8598210080075365\n",
      "[TEST] F1-score NIPS Voting comité: [0.92231087 0.05602408 0.0385772  0.7842442 ]\n",
      "[TEST] F1 macro Voting comité: 0.4502890872724369\n"
     ]
    }
   ],
   "source": [
    "m=5.0\n",
    "\n",
    "print (\"[TEST] Acc Best comité:\",np.sum(np.asarray(best_test['acc'])/m))\n",
    "temp=np.zeros(4)\n",
    "for result in best_test['none']:\n",
    "    temp+=result\n",
    "print (\"[TEST] F1-score NIPS Best comité:\" ,temp/m)\n",
    "print (\"[TEST] F1 macro Best comité:\",np.sum(np.asarray(best_test['macro'])/m))\n",
    "print (\"-----------\")\n",
    "print (\"[TEST] Acc Norm comité:\",np.sum(np.asarray(norm_test['acc'])/m))\n",
    "temp=np.zeros(4)\n",
    "for result in norm_test['none']:\n",
    "    temp+=result\n",
    "print (\"[TEST] F1-score NIPS Norm comité:\" ,temp/m)\n",
    "print (\"[TEST] F1 macro Norm comité:\",np.sum(np.asarray(norm_test['macro'])/m))\n",
    "print (\"-----------\")\n",
    "print (\"[TEST] Acc Voting comité:\",np.sum(np.asarray(voting_test['acc'])/m))\n",
    "temp=np.zeros(4)\n",
    "for result in voting_test['none']:\n",
    "    temp+=result\n",
    "print (\"[TEST] F1-score NIPS Voting comité:\" ,temp/m)\n",
    "print (\"[TEST] F1 macro Voting comité:\",np.sum(np.asarray(voting_test['macro'])/m))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------- EJECUCIÓN #1----------------\n",
      "Agregando predicciones Test set\n",
      "--------TESTING SET--------\n",
      "\n",
      "Committee Best Fit\n",
      "Accuracy: 0.795572303344324\n",
      "F1-macro: 0.45053968785636234\n",
      "F1-score NIPS: [0.88336026 0.12820513 0.11363636 0.676957  ]\n",
      "\n",
      "Committee Norm\n",
      "Accuracy: 0.8337258596325954\n",
      "F1-macro: 0.44327893971762566\n",
      "F1-score NIPS: [0.90897717 0.08064516 0.05063291 0.73286052]\n",
      "\n",
      "Committee Voting\n",
      "Accuracy: 0.8356099858690532\n",
      "F1-macro: 0.4596241741530389\n",
      "F1-score NIPS: [0.90994371 0.09230769 0.0952381  0.74100719]\n",
      "\n",
      "---------------- EJECUCIÓN #2----------------\n",
      "Agregando predicciones Test set\n",
      "--------TESTING SET--------\n",
      "\n",
      "Committee Best Fit\n",
      "Accuracy: 0.8097032501177579\n",
      "F1-macro: 0.48226138971559585\n",
      "F1-score NIPS: [0.89225908 0.01652893 0.32653061 0.69372694]\n",
      "\n",
      "Committee Norm\n",
      "Accuracy: 0.8276024493641074\n",
      "F1-macro: 0.4660125270317209\n",
      "F1-score NIPS: [0.90328638 0.         0.24       0.72076372]\n",
      "\n",
      "Committee Voting\n",
      "Accuracy: 0.8341968911917098\n",
      "F1-macro: 0.49217972383261177\n",
      "F1-score NIPS: [0.90698403 0.01769912 0.31372549 0.73031026]\n",
      "\n",
      "---------------- EJECUCIÓN #3----------------\n",
      "Agregando predicciones Test set\n",
      "--------TESTING SET--------\n",
      "\n",
      "Committee Best Fit\n",
      "Accuracy: 0.8346679227508244\n",
      "F1-macro: 0.49032760753582394\n",
      "F1-score NIPS: [0.90791897 0.03478261 0.28148148 0.73712737]\n",
      "\n",
      "Committee Norm\n",
      "Accuracy: 0.8577484691474329\n",
      "F1-macro: 0.4915610835818792\n",
      "F1-score NIPS: [0.91983122 0.01818182 0.24489796 0.78333333]\n",
      "\n",
      "Committee Voting\n",
      "Accuracy: 0.8577484691474329\n",
      "F1-macro: 0.499328672111255\n",
      "F1-score NIPS: [0.92061576 0.03571429 0.25225225 0.78873239]\n",
      "\n",
      "---------------- EJECUCIÓN #4----------------\n",
      "Agregando predicciones Test set\n",
      "--------TESTING SET--------\n",
      "\n",
      "Committee Best Fit\n",
      "Accuracy: 0.8341968911917098\n",
      "F1-macro: 0.5254797542961643\n",
      "F1-score NIPS: [0.90914748 0.17834395 0.25899281 0.75543478]\n",
      "\n",
      "Committee Norm\n",
      "Accuracy: 0.8572774375883184\n",
      "F1-macro: 0.48977580977727975\n",
      "F1-score NIPS: [0.92106861 0.07079646 0.18556701 0.78167116]\n",
      "\n",
      "Committee Voting\n",
      "Accuracy: 0.8506829957607159\n",
      "F1-macro: 0.5108469404565342\n",
      "F1-score NIPS: [0.91641245 0.13846154 0.21782178 0.77069199]\n",
      "\n",
      "---------------- EJECUCIÓN #5----------------\n",
      "Agregando predicciones Test set\n",
      "--------TESTING SET--------\n",
      "\n",
      "Committee Best Fit\n",
      "Accuracy: 0.8483278379651437\n",
      "F1-macro: 0.4701291957094543\n",
      "F1-score NIPS: [0.91569157 0.16793893 0.04938272 0.74750357]\n",
      "\n",
      "Committee Norm\n",
      "Accuracy: 0.8539802166745172\n",
      "F1-macro: 0.4413334720387485\n",
      "F1-score NIPS: [0.91817911 0.05128205 0.02597403 0.7698987 ]\n",
      "\n",
      "Committee Voting\n",
      "Accuracy: 0.8563353744700894\n",
      "F1-macro: 0.44584020579259687\n",
      "F1-score NIPS: [0.91946508 0.03418803 0.05128205 0.77842566]\n"
     ]
    }
   ],
   "source": [
    "prob=0.5\n",
    "path='../new_glove_augmented/harassment_baselines/'\n",
    "bs=32\n",
    "    \n",
    "fs_macro_val={'cnn1':[], 'cnn2':[], 'rnn1':[],'rnn2':[],'rnn3':[]}\n",
    "fs_none_val={'cnn1':[], 'cnn2':[], 'rnn1':[],'rnn2':[],'rnn3':[]}\n",
    "accs_val={'cnn1':[], 'cnn2':[], 'rnn1':[],'rnn2':[],'rnn3':[]}\n",
    "confusions_val={'cnn1':[], 'cnn2':[], 'rnn1':[],'rnn2':[],'rnn3':[]}\n",
    "\n",
    "fs_macro_test={'cnn1':[], 'cnn2':[], 'rnn1':[],'rnn2':[],'rnn3':[]}\n",
    "fs_none_test={'cnn1':[], 'cnn2':[], 'rnn1':[],'rnn2':[],'rnn3':[]}\n",
    "accs_test={'cnn1':[], 'cnn2':[], 'rnn1':[],'rnn2':[],'rnn3':[]}\n",
    "confusions_test={'cnn1':[], 'cnn2':[], 'rnn1':[],'rnn2':[],'rnn3':[]}\n",
    "\n",
    "best={'acc':[], 'none':[], 'macro':[]}\n",
    "norm={'acc':[], 'none':[], 'macro':[]}\n",
    "voting={'acc':[], 'none':[], 'macro':[]}\n",
    "best_test={'acc':[], 'none':[], 'macro':[]}\n",
    "norm_test={'acc':[], 'none':[], 'macro':[]}\n",
    "voting_test={'acc':[], 'none':[], 'macro':[]}\n",
    "\n",
    "for j in range(1,6):\n",
    "    print (\"\")\n",
    "    print (\"---------------- EJECUCIÓN #\"+str(j)+'----------------')\n",
    "    cnn1= load_model(path+str(prob)+'/cnn1_'+str(j)+'-exec_app1_Top_1_7e.h5', custom_objects={'FocalLoss': focal_loss, 'focal_loss_fixed': focal_loss()})  \n",
    "    cnn2= load_model(path+str(prob)+'/cnn2_'+str(j)+'-exec_app1_Top_1_7e.h5', custom_objects={'FocalLoss': focal_loss, 'focal_loss_fixed': focal_loss()}) \n",
    "    rnn1= load_model(path+str(prob)+'/rnn1_'+str(j)+'-exec_app1_Top_1_4e.h5', custom_objects={'FocalLoss': focal_loss, 'focal_loss_fixed': focal_loss()}) \n",
    "    rnn2= load_model(path+str(prob)+'/rnn2_'+str(j)+'-exec_app1_Top_1_7e.h5') \n",
    "    rnn3= load_model(path+str(prob)+'/rnn3_'+str(j)+'-exec_app1_Top_1_4e.h5', custom_objects={'FocalLoss': focal_loss, 'focal_loss_fixed': focal_loss()}) \n",
    "    \n",
    "    list_models=['cnn1', 'cnn2', 'rnn1', 'rnn2', 'rnn3']\n",
    "    index_models=np.arange(5)\n",
    "    dict_models=dict((key, value) for (key, value) in zip(index_models,list_models))\n",
    "    modelos=[cnn1, cnn2, rnn1, rnn2, rnn3]\n",
    "    ind=np.arange(5)\n",
    "    dict_trainedModel=dict((key, value) for (key, value) in zip(ind,modelos))\n",
    "    \n",
    "    \n",
    "    print (\"Agregando predicciones Test set\")    \n",
    "    for i in dict_trainedModel.keys(): \n",
    "        a_evaluar=dict_models[i]\n",
    "        predicho=dict_trainedModel[i].predict(x_test, batch_size=bs)\n",
    "        predicho=[np.argmax(pred) for pred in predicho]\n",
    "        acc= accuracy_score(etiq_t, predicho)  \n",
    "        f1_ma=f1_score(etiq_t, predicho, average='macro')\n",
    "        f1_no=f1_score(etiq_t, predicho, average=None)\n",
    "        matriz=normalize(confusion_matrix(etiq_t, predicho))\n",
    "        fs_macro_test[dict_models[i]].append(f1_ma)\n",
    "        fs_none_test[dict_models[i]].append(f1_no)\n",
    "        accs_test[dict_models[i]].append(acc)\n",
    "        confusions_test[dict_models[i]].append(matriz)\n",
    "\n",
    "    predicciones_all_test=[]\n",
    "    bs=32\n",
    "    for i in dict_trainedModel.keys(): \n",
    "        a_evaluar=dict_models[i]\n",
    "        predicciones_all_test.append(dict_trainedModel[i].predict(x_test, batch_size=bs))\n",
    "    predichos_all_test=np.concatenate(np.asarray(predicciones_all_test),axis=-1)\n",
    "    \n",
    "        \n",
    "    print (\"--------TESTING SET--------\")\n",
    "    \n",
    "    print (\"\\nCommittee Best Fit\")\n",
    "    trainPredicho, maxPredicho, confianzas = predecir_modelos(bs,predichos_all_test)\n",
    "    acc_comite= accuracy_score(etiq_t, trainPredicho)  \n",
    "    print(\"Accuracy:\",acc_comite)\n",
    "    f1=f1_score(etiq_t, trainPredicho, average='macro') \n",
    "    print(\"F1-macro:\",f1)\n",
    "    f1_no=f1_score(etiq_t, trainPredicho, average=None)  \n",
    "    print(\"F1-score NIPS:\",f1_no)\n",
    "    best_test['acc'].append(acc_comite)\n",
    "    best_test['macro'].append(f1)\n",
    "    best_test['none'].append(f1_no)\n",
    "\n",
    "    print (\"\\nCommittee Norm\")\n",
    "    trainPredicho, maxPredicho, confianzas = predecir_modelos_norm(bs,predichos_all_test)\n",
    "    acc_comite= accuracy_score(etiq_t, trainPredicho)  \n",
    "    print(\"Accuracy:\",acc_comite)\n",
    "    f1=f1_score(etiq_t, trainPredicho, average='macro') \n",
    "    print(\"F1-macro:\",f1)\n",
    "    f1_no=f1_score(etiq_t, trainPredicho, average=None) \n",
    "    print(\"F1-score NIPS:\",f1_no)\n",
    "    norm_test['acc'].append(acc_comite)\n",
    "    norm_test['macro'].append(f1)\n",
    "    norm_test['none'].append(f1_no)\n",
    "\n",
    "    print (\"\\nCommittee Voting\")\n",
    "    trainPredicho, maxPredicho, confianzas = predecir_modelos_average(bs,predichos_all_test)\n",
    "    acc_comite= accuracy_score(etiq_t, trainPredicho)  \n",
    "    print(\"Accuracy:\",acc_comite)\n",
    "    f1=f1_score(etiq_t, trainPredicho, average='macro') \n",
    "    print(\"F1-macro:\",f1)\n",
    "    f1_no=f1_score(etiq_t, trainPredicho, average=None)  \n",
    "    print(\"F1-score NIPS:\",f1_no)\n",
    "    voting_test['acc'].append(acc_comite)\n",
    "    voting_test['macro'].append(f1)\n",
    "    voting_test['none'].append(f1_no)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "con 7 y 4 epochs...\n",
      "\n",
      "[TEST] Acc Best comité: 0.824493641073952\n",
      "[TEST] F1-score NIPS Best comité: [0.90167547 0.10515991 0.2060048  0.72214993]\n",
      "[TEST] F1 macro Best comité: 0.4837475270226802\n",
      "-----------\n",
      "[TEST] Acc Norm comité: 0.8460668864813943\n",
      "[TEST] F1-score NIPS Norm comité: [0.9142685  0.0441811  0.14941438 0.75770549]\n",
      "[TEST] F1 macro Norm comité: 0.4663923664294508\n",
      "-----------\n",
      "[TEST] Acc Voting comité: 0.8469147432878004\n",
      "[TEST] F1-score NIPS Voting comité: [0.91468421 0.06367413 0.18606393 0.7618335 ]\n",
      "[TEST] F1 macro Voting comité: 0.4815639432692074\n"
     ]
    }
   ],
   "source": [
    "print (\"con 7 y 4 epochs...\\n\")\n",
    "\n",
    "m=5.0\n",
    "\n",
    "print (\"[TEST] Acc Best comité:\",np.sum(np.asarray(best_test['acc'])/m))\n",
    "temp=np.zeros(4)\n",
    "for result in best_test['none']:\n",
    "    temp+=result\n",
    "print (\"[TEST] F1-score NIPS Best comité:\" ,temp/m)\n",
    "print (\"[TEST] F1 macro Best comité:\",np.sum(np.asarray(best_test['macro'])/m))\n",
    "print (\"-----------\")\n",
    "print (\"[TEST] Acc Norm comité:\",np.sum(np.asarray(norm_test['acc'])/m))\n",
    "temp=np.zeros(4)\n",
    "for result in norm_test['none']:\n",
    "    temp+=result\n",
    "print (\"[TEST] F1-score NIPS Norm comité:\" ,temp/m)\n",
    "print (\"[TEST] F1 macro Norm comité:\",np.sum(np.asarray(norm_test['macro'])/m))\n",
    "print (\"-----------\")\n",
    "print (\"[TEST] Acc Voting comité:\",np.sum(np.asarray(voting_test['acc'])/m))\n",
    "temp=np.zeros(4)\n",
    "for result in voting_test['none']:\n",
    "    temp+=result\n",
    "print (\"[TEST] F1-score NIPS Voting comité:\" ,temp/m)\n",
    "print (\"[TEST] F1 macro Voting comité:\",np.sum(np.asarray(voting_test['macro'])/m))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TEST] Acc Best comité: 0.8071596796985397\n",
      "[TEST] F1-score NIPS Best comité: [0.89049212 0.11073239 0.17162255 0.70024128]\n",
      "[TEST] F1 macro Best comité: 0.4682720840623157\n",
      "-----------\n",
      "[TEST] Acc Norm comité: 0.8355157795572303\n",
      "[TEST] F1-score NIPS Norm comité: [0.90888367 0.05757599 0.11769108 0.73535642]\n",
      "[TEST] F1 macro Norm comité: 0.4548767893352969\n",
      "-----------\n",
      "[TEST] Acc Voting comité: 0.8416391898257184\n",
      "[TEST] F1-score NIPS Voting comité: [0.91301854 0.06599851 0.12760449 0.7500492 ]\n",
      "[TEST] F1 macro Voting comité: 0.4641676870158177\n"
     ]
    }
   ],
   "source": [
    "m=5.0\n",
    "\n",
    "print (\"[TEST] Acc Best comité:\",np.sum(np.asarray(best_test['acc'])/m))\n",
    "temp=np.zeros(4)\n",
    "for result in best_test['none']:\n",
    "    temp+=result\n",
    "print (\"[TEST] F1-score NIPS Best comité:\" ,temp/m)\n",
    "print (\"[TEST] F1 macro Best comité:\",np.sum(np.asarray(best_test['macro'])/m))\n",
    "print (\"-----------\")\n",
    "print (\"[TEST] Acc Norm comité:\",np.sum(np.asarray(norm_test['acc'])/m))\n",
    "temp=np.zeros(4)\n",
    "for result in norm_test['none']:\n",
    "    temp+=result\n",
    "print (\"[TEST] F1-score NIPS Norm comité:\" ,temp/m)\n",
    "print (\"[TEST] F1 macro Norm comité:\",np.sum(np.asarray(norm_test['macro'])/m))\n",
    "print (\"-----------\")\n",
    "print (\"[TEST] Acc Voting comité:\",np.sum(np.asarray(voting_test['acc'])/m))\n",
    "temp=np.zeros(4)\n",
    "for result in voting_test['none']:\n",
    "    temp+=result\n",
    "print (\"[TEST] F1-score NIPS Voting comité:\" ,temp/m)\n",
    "print (\"[TEST] F1 macro Voting comité:\",np.sum(np.asarray(voting_test['macro'])/m))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.85"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------- EJECUCIÓN #1----------------\n",
      "Agregando predicciones Test set\n",
      "--------TESTING SET--------\n",
      "\n",
      "Committee Best Fit\n",
      "Accuracy: 0.8365520489872822\n",
      "F1-macro: 0.47205939195464786\n",
      "F1-score NIPS: [0.90750226 0.11023622 0.13592233 0.73457676]\n",
      "\n",
      "Committee Norm\n",
      "Accuracy: 0.8469147432878003\n",
      "F1-macro: 0.463786817741373\n",
      "F1-score NIPS: [0.91399461 0.05042017 0.14285714 0.74787535]\n",
      "\n",
      "Committee Voting\n",
      "Accuracy: 0.8483278379651437\n",
      "F1-macro: 0.45632648963694733\n",
      "F1-score NIPS: [0.91531532 0.08264463 0.07594937 0.75139665]\n",
      "\n",
      "---------------- EJECUCIÓN #2----------------\n",
      "Agregando predicciones Test set\n",
      "--------TESTING SET--------\n",
      "\n",
      "Committee Best Fit\n",
      "Accuracy: 0.7739048516250588\n",
      "F1-macro: 0.5014794206982005\n",
      "F1-score NIPS: [0.86315087 0.24892704 0.20869565 0.68514412]\n",
      "\n",
      "Committee Norm\n",
      "Accuracy: 0.8134715025906736\n",
      "F1-macro: 0.48626285927733887\n",
      "F1-score NIPS: [0.89252486 0.16759777 0.15384615 0.73108265]\n",
      "\n",
      "Committee Voting\n",
      "Accuracy: 0.8186528497409327\n",
      "F1-macro: 0.49548382246161643\n",
      "F1-score NIPS: [0.89677213 0.17977528 0.16842105 0.73696682]\n",
      "\n",
      "---------------- EJECUCIÓN #3----------------\n",
      "Agregando predicciones Test set\n",
      "--------TESTING SET--------\n",
      "\n",
      "Committee Best Fit\n",
      "Accuracy: 0.8412623645784267\n",
      "F1-macro: 0.4960411923630431\n",
      "F1-score NIPS: [0.91229154 0.20645161 0.10752688 0.75789474]\n",
      "\n",
      "Committee Norm\n",
      "Accuracy: 0.8483278379651437\n",
      "F1-macro: 0.46101570154937044\n",
      "F1-score NIPS: [0.9173071  0.140625   0.02531646 0.76081425]\n",
      "\n",
      "Committee Voting\n",
      "Accuracy: 0.8469147432878003\n",
      "F1-macro: 0.4718279877805561\n",
      "F1-score NIPS: [0.91592239 0.13740458 0.07317073 0.76081425]\n",
      "\n",
      "---------------- EJECUCIÓN #4----------------\n",
      "Agregando predicciones Test set\n",
      "--------TESTING SET--------\n",
      "\n",
      "Committee Best Fit\n",
      "Accuracy: 0.8144135657089024\n",
      "F1-macro: 0.464438287716764\n",
      "F1-score NIPS: [0.8912081  0.03636364 0.22429907 0.70588235]\n",
      "\n",
      "Committee Norm\n",
      "Accuracy: 0.8464437117286858\n",
      "F1-macro: 0.4226591871825648\n",
      "F1-score NIPS: [0.91589932 0.01851852 0.         0.75621891]\n",
      "\n",
      "Committee Voting\n",
      "Accuracy: 0.8487988695242581\n",
      "F1-macro: 0.4248174682607507\n",
      "F1-score NIPS: [0.91766146 0.01801802 0.         0.76359039]\n",
      "\n",
      "---------------- EJECUCIÓN #5----------------\n",
      "Agregando predicciones Test set\n",
      "--------TESTING SET--------\n",
      "\n",
      "Committee Best Fit\n",
      "Accuracy: 0.8276024493641074\n",
      "F1-macro: 0.5047474868274309\n",
      "F1-score NIPS: [0.90577695 0.16091954 0.20967742 0.74261603]\n",
      "\n",
      "Committee Norm\n",
      "Accuracy: 0.8492699010833726\n",
      "F1-macro: 0.4545074866380632\n",
      "F1-score NIPS: [0.91621459 0.06666667 0.06741573 0.76773296]\n",
      "\n",
      "Committee Voting\n",
      "Accuracy: 0.8516250588789449\n",
      "F1-macro: 0.46456966343146905\n",
      "F1-score NIPS: [0.91732046 0.09375    0.06896552 0.77824268]\n"
     ]
    }
   ],
   "source": [
    "prob=0.85\n",
    "path='../new_glove_augmented/harassment_baselines/'\n",
    "bs=32\n",
    "    \n",
    "fs_macro_val={'cnn1':[], 'cnn2':[], 'rnn1':[],'rnn2':[],'rnn3':[]}\n",
    "fs_none_val={'cnn1':[], 'cnn2':[], 'rnn1':[],'rnn2':[],'rnn3':[]}\n",
    "accs_val={'cnn1':[], 'cnn2':[], 'rnn1':[],'rnn2':[],'rnn3':[]}\n",
    "confusions_val={'cnn1':[], 'cnn2':[], 'rnn1':[],'rnn2':[],'rnn3':[]}\n",
    "\n",
    "fs_macro_test={'cnn1':[], 'cnn2':[], 'rnn1':[],'rnn2':[],'rnn3':[]}\n",
    "fs_none_test={'cnn1':[], 'cnn2':[], 'rnn1':[],'rnn2':[],'rnn3':[]}\n",
    "accs_test={'cnn1':[], 'cnn2':[], 'rnn1':[],'rnn2':[],'rnn3':[]}\n",
    "confusions_test={'cnn1':[], 'cnn2':[], 'rnn1':[],'rnn2':[],'rnn3':[]}\n",
    "\n",
    "best={'acc':[], 'none':[], 'macro':[]}\n",
    "norm={'acc':[], 'none':[], 'macro':[]}\n",
    "voting={'acc':[], 'none':[], 'macro':[]}\n",
    "best_test={'acc':[], 'none':[], 'macro':[]}\n",
    "norm_test={'acc':[], 'none':[], 'macro':[]}\n",
    "voting_test={'acc':[], 'none':[], 'macro':[]}\n",
    "\n",
    "for j in range(1,6):\n",
    "    print (\"\")\n",
    "    print (\"---------------- EJECUCIÓN #\"+str(j)+'----------------')\n",
    "    cnn1= load_model(path+str(prob)+'/cnn1_'+str(j)+'-exec_app1_Top_1_7e.h5', custom_objects={'FocalLoss': focal_loss, 'focal_loss_fixed': focal_loss()})  \n",
    "    cnn2= load_model(path+str(prob)+'/cnn2_'+str(j)+'-exec_app1_Top_1_7e.h5', custom_objects={'FocalLoss': focal_loss, 'focal_loss_fixed': focal_loss()}) \n",
    "    rnn1= load_model(path+str(prob)+'/rnn1_'+str(j)+'-exec_app1_Top_1_4e.h5', custom_objects={'FocalLoss': focal_loss, 'focal_loss_fixed': focal_loss()}) \n",
    "    rnn2= load_model(path+str(prob)+'/rnn2_'+str(j)+'-exec_app1_Top_1_7e.h5') \n",
    "    rnn3= load_model(path+str(prob)+'/rnn3_'+str(j)+'-exec_app1_Top_1_4e.h5', custom_objects={'FocalLoss': focal_loss, 'focal_loss_fixed': focal_loss()}) \n",
    "    \n",
    "    list_models=['cnn1', 'cnn2', 'rnn1', 'rnn2', 'rnn3']\n",
    "    index_models=np.arange(5)\n",
    "    dict_models=dict((key, value) for (key, value) in zip(index_models,list_models))\n",
    "    modelos=[cnn1, cnn2, rnn1, rnn2, rnn3]\n",
    "    ind=np.arange(5)\n",
    "    dict_trainedModel=dict((key, value) for (key, value) in zip(ind,modelos))\n",
    "    \n",
    "    \n",
    "    print (\"Agregando predicciones Test set\")    \n",
    "    for i in dict_trainedModel.keys(): \n",
    "        a_evaluar=dict_models[i]\n",
    "        predicho=dict_trainedModel[i].predict(x_test, batch_size=bs)\n",
    "        predicho=[np.argmax(pred) for pred in predicho]\n",
    "        acc= accuracy_score(etiq_t, predicho)  \n",
    "        f1_ma=f1_score(etiq_t, predicho, average='macro')\n",
    "        f1_no=f1_score(etiq_t, predicho, average=None)\n",
    "        matriz=normalize(confusion_matrix(etiq_t, predicho))\n",
    "        fs_macro_test[dict_models[i]].append(f1_ma)\n",
    "        fs_none_test[dict_models[i]].append(f1_no)\n",
    "        accs_test[dict_models[i]].append(acc)\n",
    "        confusions_test[dict_models[i]].append(matriz)\n",
    "\n",
    "    predicciones_all_test=[]\n",
    "    bs=32\n",
    "    for i in dict_trainedModel.keys(): \n",
    "        a_evaluar=dict_models[i]\n",
    "        predicciones_all_test.append(dict_trainedModel[i].predict(x_test, batch_size=bs))\n",
    "    predichos_all_test=np.concatenate(np.asarray(predicciones_all_test),axis=-1)\n",
    "    \n",
    "        \n",
    "    print (\"--------TESTING SET--------\")\n",
    "    \n",
    "    print (\"\\nCommittee Best Fit\")\n",
    "    trainPredicho, maxPredicho, confianzas = predecir_modelos(bs,predichos_all_test)\n",
    "    acc_comite= accuracy_score(etiq_t, trainPredicho)  \n",
    "    print(\"Accuracy:\",acc_comite)\n",
    "    f1=f1_score(etiq_t, trainPredicho, average='macro') \n",
    "    print(\"F1-macro:\",f1)\n",
    "    f1_no=f1_score(etiq_t, trainPredicho, average=None)  \n",
    "    print(\"F1-score NIPS:\",f1_no)\n",
    "    best_test['acc'].append(acc_comite)\n",
    "    best_test['macro'].append(f1)\n",
    "    best_test['none'].append(f1_no)\n",
    "\n",
    "    print (\"\\nCommittee Norm\")\n",
    "    trainPredicho, maxPredicho, confianzas = predecir_modelos_norm(bs,predichos_all_test)\n",
    "    acc_comite= accuracy_score(etiq_t, trainPredicho)  \n",
    "    print(\"Accuracy:\",acc_comite)\n",
    "    f1=f1_score(etiq_t, trainPredicho, average='macro') \n",
    "    print(\"F1-macro:\",f1)\n",
    "    f1_no=f1_score(etiq_t, trainPredicho, average=None) \n",
    "    print(\"F1-score NIPS:\",f1_no)\n",
    "    norm_test['acc'].append(acc_comite)\n",
    "    norm_test['macro'].append(f1)\n",
    "    norm_test['none'].append(f1_no)\n",
    "\n",
    "    print (\"\\nCommittee Voting\")\n",
    "    trainPredicho, maxPredicho, confianzas = predecir_modelos_average(bs,predichos_all_test)\n",
    "    acc_comite= accuracy_score(etiq_t, trainPredicho)  \n",
    "    print(\"Accuracy:\",acc_comite)\n",
    "    f1=f1_score(etiq_t, trainPredicho, average='macro') \n",
    "    print(\"F1-macro:\",f1)\n",
    "    f1_no=f1_score(etiq_t, trainPredicho, average=None)  \n",
    "    print(\"F1-score NIPS:\",f1_no)\n",
    "    voting_test['acc'].append(acc_comite)\n",
    "    voting_test['macro'].append(f1)\n",
    "    voting_test['none'].append(f1_no)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "con 7 y 4 epochs...\n",
      "\n",
      "[TEST] Acc Best comité: 0.8187470560527554\n",
      "[TEST] F1-score NIPS Best comité: [0.89598594 0.15257961 0.17722427 0.7252228 ]\n",
      "[TEST] F1 macro Best comité: 0.4877531559120173\n",
      "-----------\n",
      "[TEST] Acc Norm comité: 0.8408855393311352\n",
      "[TEST] F1-score NIPS Norm comité: [0.9111881  0.08876562 0.0778871  0.75274483]\n",
      "[TEST] F1 macro Norm comité: 0.4576464104777421\n",
      "-----------\n",
      "[TEST] Acc Voting comité: 0.8428638718794159\n",
      "[TEST] F1-score NIPS Voting comité: [0.91259835 0.1023185  0.07730133 0.75820216]\n",
      "[TEST] F1 macro Voting comité: 0.4626050863142679\n"
     ]
    }
   ],
   "source": [
    "print (\"con 7 y 4 epochs...\\n\")\n",
    "\n",
    "m=5.0\n",
    "\n",
    "print (\"[TEST] Acc Best comité:\",np.sum(np.asarray(best_test['acc'])/m))\n",
    "temp=np.zeros(4)\n",
    "for result in best_test['none']:\n",
    "    temp+=result\n",
    "print (\"[TEST] F1-score NIPS Best comité:\" ,temp/m)\n",
    "print (\"[TEST] F1 macro Best comité:\",np.sum(np.asarray(best_test['macro'])/m))\n",
    "print (\"-----------\")\n",
    "print (\"[TEST] Acc Norm comité:\",np.sum(np.asarray(norm_test['acc'])/m))\n",
    "temp=np.zeros(4)\n",
    "for result in norm_test['none']:\n",
    "    temp+=result\n",
    "print (\"[TEST] F1-score NIPS Norm comité:\" ,temp/m)\n",
    "print (\"[TEST] F1 macro Norm comité:\",np.sum(np.asarray(norm_test['macro'])/m))\n",
    "print (\"-----------\")\n",
    "print (\"[TEST] Acc Voting comité:\",np.sum(np.asarray(voting_test['acc'])/m))\n",
    "temp=np.zeros(4)\n",
    "for result in voting_test['none']:\n",
    "    temp+=result\n",
    "print (\"[TEST] F1-score NIPS Voting comité:\" ,temp/m)\n",
    "print (\"[TEST] F1 macro Voting comité:\",np.sum(np.asarray(voting_test['macro'])/m))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TEST] Acc Best comité: 0.81921808761187\n",
      "[TEST] F1-score NIPS Best comité: [0.89791142 0.08312651 0.08559012 0.70417871]\n",
      "[TEST] F1 macro Best comité: 0.4427016889619424\n",
      "-----------\n",
      "[TEST] Acc Norm comité: 0.8373999057936882\n",
      "[TEST] F1-score NIPS Norm comité: [0.9094091  0.040043   0.025      0.73757739]\n",
      "[TEST] F1 macro Norm comité: 0.4280073730785264\n",
      "-----------\n",
      "[TEST] Acc Voting comité: 0.8395666509656147\n",
      "[TEST] F1-score NIPS Voting comité: [0.91089089 0.0426081  0.03826736 0.74392055]\n",
      "[TEST] F1 macro Voting comité: 0.43392172396770323\n"
     ]
    }
   ],
   "source": [
    "m=5.0\n",
    "\n",
    "print (\"[TEST] Acc Best comité:\",np.sum(np.asarray(best_test['acc'])/m))\n",
    "temp=np.zeros(4)\n",
    "for result in best_test['none']:\n",
    "    temp+=result\n",
    "print (\"[TEST] F1-score NIPS Best comité:\" ,temp/m)\n",
    "print (\"[TEST] F1 macro Best comité:\",np.sum(np.asarray(best_test['macro'])/m))\n",
    "print (\"-----------\")\n",
    "print (\"[TEST] Acc Norm comité:\",np.sum(np.asarray(norm_test['acc'])/m))\n",
    "temp=np.zeros(4)\n",
    "for result in norm_test['none']:\n",
    "    temp+=result\n",
    "print (\"[TEST] F1-score NIPS Norm comité:\" ,temp/m)\n",
    "print (\"[TEST] F1 macro Norm comité:\",np.sum(np.asarray(norm_test['macro'])/m))\n",
    "print (\"-----------\")\n",
    "print (\"[TEST] Acc Voting comité:\",np.sum(np.asarray(voting_test['acc'])/m))\n",
    "temp=np.zeros(4)\n",
    "for result in voting_test['none']:\n",
    "    temp+=result\n",
    "print (\"[TEST] F1-score NIPS Voting comité:\" ,temp/m)\n",
    "print (\"[TEST] F1 macro Voting comité:\",np.sum(np.asarray(voting_test['macro'])/m))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:newpy3_tf1]",
   "language": "python",
   "name": "conda-env-newpy3_tf1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
