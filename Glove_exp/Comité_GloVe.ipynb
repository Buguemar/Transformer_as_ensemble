{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/casapanshop/anaconda2/envs/newpy3_tf1/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/casapanshop/anaconda2/envs/newpy3_tf1/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/casapanshop/anaconda2/envs/newpy3_tf1/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/casapanshop/anaconda2/envs/newpy3_tf1/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/casapanshop/anaconda2/envs/newpy3_tf1/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/casapanshop/anaconda2/envs/newpy3_tf1/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/casapanshop/anaconda2/envs/newpy3_tf1/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/casapanshop/anaconda2/envs/newpy3_tf1/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/casapanshop/anaconda2/envs/newpy3_tf1/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/casapanshop/anaconda2/envs/newpy3_tf1/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/casapanshop/anaconda2/envs/newpy3_tf1/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/casapanshop/anaconda2/envs/newpy3_tf1/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: GeForce GTX 1060 6GB\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import pandas as pd\n",
    "from models import *\n",
    "import time\n",
    "import re, io, nltk, torch \n",
    "from nltk.corpus import stopwords\n",
    "from numpy import linalg as LA\n",
    "from numpy.linalg import norm\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm, trange\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from gensim import utils, matutils  \n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from six import string_types, integer_types\n",
    "from six.moves import zip, range\n",
    "from numpy import linalg as LA\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from scipy import stats\n",
    "from gensim.utils import deprecated\n",
    "from numpy import dot, float32 as REAL, memmap as np_memmap, \\\n",
    "    double, array, zeros, vstack, sqrt, newaxis, integer, \\\n",
    "    ndarray, sum as np_sum, prod, argmax\n",
    "from collections import Counter\n",
    "from scipy.spatial import distance\n",
    "from numpy.random import binomial\n",
    "from scipy.stats import bernoulli\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "n_gpu = torch.cuda.device_count()\n",
    "torch.cuda.get_device_name(0)\n",
    "\n",
    "if torch.cuda.is_available():     \n",
    "    device = torch.device(\"cuda\")\n",
    "    print('There are',torch.cuda.device_count(),'GPU(s) available.')\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "dict_cl=dict()\n",
    "dict_cl[0]='support'\n",
    "dict_cl[1]='denying'\n",
    "dict_cl[2]='questioning'\n",
    "dict_cl[3]='commenting'\n",
    "\n",
    "#sdqc\n",
    "d_lab=dict()\n",
    "d_lab[\"support\"]=0\n",
    "d_lab[\"denying\"]=1\n",
    "d_lab[\"questioning\"]=2\n",
    "d_lab[\"commenting\"]=3\n",
    "\n",
    "train =pd.read_csv(\"../Tesis_exp/Fine-Tuning/CSV_Stance/train_semeval_raw.csv\")\n",
    "val=pd.read_csv(\"../Tesis_exp/Fine-Tuning/CSV_Stance/dev_semeval_raw.csv\")\n",
    "test=pd.read_csv(\"../Tesis_exp/Fine-Tuning/CSV_Stance/test_semeval_raw.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se encontraron 1193514 terminos con sus vectores de embedding.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import WordNetLemmatizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "\n",
    "stemmer_sn = SnowballStemmer(\"english\")\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "stoplist = stopwords.words(\"english\")\n",
    "lemmatizer=WordNetLemmatizer()\n",
    "\n",
    "emoji_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "\n",
    "def cleaner(post): \n",
    "    #re.sub(r'([a-z])\\1+', r'\\1', \"user i think that ' s all you loooooove \")\n",
    "    s= re.sub(r\"http\\S+\", \" html \", post)   ##########elimina http    \n",
    "    s= re.sub(r'#\\w+ ?', \" hashtag \", s) ########3\n",
    "    s= re.sub(r'@\\w+ ?', \" user \", s)##############    \n",
    "    s= s.lower()\n",
    "    s=emoji_pattern.sub(r'', s)\n",
    "    s=re.sub(r\"'\\b\", \" ' \", s)\n",
    "    s=re.sub(r\"\\b'\\b\", \" ' \", s)\n",
    "    s=re.sub(r\"“\\b\", \" “ \", s)\n",
    "    #“\n",
    "    s=re.sub(r\"\\b’\", \" ’ \", s)\n",
    "    s=re.sub(r\"‘\\b\", \" ‘ \", s)\n",
    "    s=re.sub(r\"\\b’\\b\", \" ‘ \", s)\n",
    "    s = re.sub(r\"-\", \" - \", s)\n",
    "    s = re.sub(r\"\\(\", \" \", s)\n",
    "    s = re.sub(r\"\\)\", \" \", s)\n",
    "    s = re.sub(r\"\\?\", \" ? \", s)    \n",
    "    s = re.sub(r\"\\/\", \" \", s)\n",
    "    s = re.sub(r\"' \", \" ' \", s)\n",
    "    s = re.sub(r\" '\", \" ' \", s)\n",
    "    s = re.sub(r\"\\!\", \" ! \", s)\n",
    "    s=re.sub(\"[\\.]+\", \" . \", s)\n",
    "    s=re.sub(\"[\\,]+\", \" , \", s)\n",
    "    s=re.sub(\"[\\;]+\", \" ; \", s)\n",
    "    s=re.sub(\"[\\:]+\", \" : \", s)\n",
    "    s=re.sub('[\\\"]+', ' \" ', s)\n",
    "    s=re.sub(r'\\b[0-9]\\b', \" number \",  s)\n",
    "    s=re.sub(r'\\b[0-9]*[0-9]\\b', \" number \",  s)    \n",
    "    s=re.sub(r'\\b”', ' \" ', s)\n",
    "    sl= list(s.split())\n",
    "    sl_2=[]\n",
    "    for wd in sl:\n",
    "        try: \n",
    "            q=Word2Index[wd]\n",
    "            sl_2.append(wd)\n",
    "        except:\n",
    "            try: \n",
    "                if stemmer.stem(wd) in Word2Index.keys():\n",
    "                    sl_2.append(stemmer.stem(wd))\n",
    "                elif lemmatizer.lemmatize(wd) in Word2Index.keys():\n",
    "                    sl_2.append(lemmatizer.lemmatize(wd))\n",
    "                else:\n",
    "                    sl_2.append(wd)\n",
    "            except:\n",
    "                sl_2.append(wd)\n",
    "    sl=sl_2\n",
    "    s=' '.join([word for word in sl])# if word not in stoplist])\n",
    "    return s, sl\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join(\"glove.twitter.27B.200d.txt\"))#'glove.twitter.27B.100d.txt'))\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Se encontraron %s terminos con sus vectores de embedding.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7236"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_index=dict()\n",
    "j=1\n",
    "for frase in train[\"Tweet\"].values:\n",
    "    frase=cleaner(frase)[-1]\n",
    "    #print (frase)\n",
    "    for term in frase:\n",
    "        if term not in word_index.keys():\n",
    "            word_index[term]=j\n",
    "            j+=1\n",
    "\n",
    "for frase in val[\"Tweet\"].values:\n",
    "    frase=cleaner(frase)[-1]\n",
    "    #print (frase)\n",
    "    for term in frase:\n",
    "        if term not in word_index.keys():\n",
    "            word_index[term]=j\n",
    "            j+=1\n",
    "            \n",
    "for frase in test[\"Tweet\"].values:\n",
    "    frase=cleaner(frase)[-1]\n",
    "    #print (frase)\n",
    "    for term in frase:\n",
    "        if term not in word_index.keys():\n",
    "            word_index[term]=j\n",
    "            j+=1\n",
    "            \n",
    "len(word_index.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_found=[]\n",
    "pos_tag_nf=[]\n",
    "Word2Index={}\n",
    "idx_2_token={}\n",
    "\n",
    "g_dim=200\n",
    "emb_matrix = np.zeros((len(word_index.keys())+1, g_dim))  \n",
    "\n",
    "for word, i in word_index.items():\n",
    "    vector = embeddings_index.get(word)\n",
    "    if vector is not None:\n",
    "        # words sin match en Glove, serán vectores de ceros.\n",
    "        emb_matrix[i] = vector\n",
    "        Word2Index[word]=i\n",
    "        idx_2_token[i]=word\n",
    "    else:\n",
    "        Word2Index[word]=i\n",
    "        idx_2_token[i]=word\n",
    "        not_found.append(word)\n",
    "        pos_tag_nf.append(nltk.pos_tag([word])[0][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "transf = Normalizer().fit(emb_matrix) \n",
    "M_GLOVE_space=transf.transform(emb_matrix)\n",
    "\n",
    "def my_tokenizer(lista):\n",
    "    to_return=[]\n",
    "    for l in lista:\n",
    "        if l in word_index.keys():\n",
    "            to_return.append(l)\n",
    "        else:\n",
    "            continue\n",
    "    return to_return\n",
    "\n",
    "def predict_data(trained, x_train, x_val, x_test, etiq, etiq_v, etiq_t, name_model):\n",
    "    etiq = etiq.astype(\"int\")\n",
    "    trainPredict = trained.predict(x_train, batch_size=32)\n",
    "    trainPredict=[np.argmax(pred) for pred in trainPredict]\n",
    "    acc= accuracy_score(etiq, trainPredict)  \n",
    "    f1=f1_score(etiq, trainPredict, average=None)  # labels=np.unique(trainPredict)\n",
    "    f1_ma=f1_score(etiq, trainPredict, average='macro')  # labels=np.unique(trainPredict)\n",
    "    #matriz=normalize(confusion_matrix(etiq, trainPredict))\n",
    "    print (\"\")\n",
    "    print (\"Accuracy sobre Train\", name_model, \":\",acc)  \n",
    "    print (\"F1-score None sobre Train\", name_model, \":\",f1)\n",
    "    print (\"F1-score macro sobre Train\", name_model, \":\",f1_ma)\n",
    "    ########################################\n",
    "    trainPredict = trained.predict(x_val, batch_size=32)\n",
    "    trainPredict=[np.argmax(pred) for pred in trainPredict]\n",
    "    acc= accuracy_score(etiq_v, trainPredict)  \n",
    "    f1=f1_score(etiq_v, trainPredict, average=None)  # labels=np.unique(trainPredict)\n",
    "    f1_ma=f1_score(etiq_v, trainPredict, average='macro')  # labels=np.unique(trainPredict)\n",
    "    #matriz=normalize(confusion_matrix(etiq_v, trainPredict))\n",
    "    print (\"\")\n",
    "    print (\"Accuracy sobre Val\", name_model, \":\",acc)  \n",
    "    print (\"F1-score None sobre Val\", name_model, \":\",f1)\n",
    "    print (\"F1-score macro sobre Val\", name_model, \":\",f1_ma)\n",
    "    ########################################\n",
    "    trainPredict = trained.predict(x_test, batch_size=32)\n",
    "    trainPredict=[np.argmax(pred) for pred in trainPredict]\n",
    "    acc_t= accuracy_score(etiq_t, trainPredict)  \n",
    "    f1_t=f1_score(etiq_t, trainPredict, average=None)  # labels=np.unique(trainPredict)\n",
    "    f1_ma_t=f1_score(etiq_t, trainPredict, average='macro')  # labels=np.unique(trainPredict)\n",
    "    matriz_t=normalize(confusion_matrix(etiq_t, trainPredict))\n",
    "    print (\"\")\n",
    "    print (\"Accuracy sobre Test\", name_model, \":\",acc_t)  \n",
    "    print (\"F1-score None sobre Test\", name_model, \":\",f1_t)\n",
    "    print (\"F1-score macro sobre Test\", name_model, \":\",f1_ma_t)\n",
    "    \n",
    "    return f1_ma_t, f1_t, acc_t, matriz_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conteo por stance val\n",
      " Counter({'commenting': 173, 'support': 69, 'questioning': 28, 'denying': 11})\n",
      "Conteo por stance test\n",
      " Counter({'commenting': 778, 'questioning': 106, 'denying': 69, 'support': 68})\n"
     ]
    }
   ],
   "source": [
    "from models import *\n",
    "\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    \n",
    "class Dummy_Embeddings(nn.Module):\n",
    "    def __init__(self, d_model, dummy_vectors): \n",
    "        super(Dummy_Embeddings, self).__init__()\n",
    "        aux = torch.from_numpy(dummy_vectors)#, dtype='float32'))\n",
    "        self.index2dummy = nn.Embedding(aux.size()[0], d_model)\n",
    "        self.index2dummy.weigth=nn.Parameter(aux)\n",
    "        self.index2dummy.weigth.requires_grad=False\n",
    "        self.d_model = d_model\n",
    "        \n",
    "    def forward(self, x):\n",
    "        aux=x.numpy()\n",
    "        #print (\"aux original\", aux)\n",
    "        #print (\"aux restado...\",aux-np.ones(aux.shape))\n",
    "        new_x= aux #- np.ones(aux.shape)\n",
    "        new_x= torch.from_numpy(new_x)\n",
    "        return self.index2dummy(new_x.long()) * math.sqrt(self.d_model) #debiese retornar matriz de batch_size x [ind_tw, k1,k2,k3,k4,k5,k6] (si son 6 modelos)\n",
    "    \n",
    "def match(objetos,ejemplo):\n",
    "    i=0\n",
    "    for obj in objetos:\n",
    "        if obj==ejemplo:\n",
    "            return i\n",
    "        i+=1\n",
    "        \n",
    "def plot_confusion_matrix(cm, target_names, title='Confusion matrix (f1-score)',cmap=None, normalize=True):    \n",
    "    accuracy = np.trace(cm) / float(np.sum(cm))\n",
    "    misclass = 1 - accuracy\n",
    "    if cmap is None:\n",
    "        cmap = plt.get_cmap('Blues')\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap, vmin=0, vmax=1)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    \n",
    "    if target_names is not None:\n",
    "        tick_marks = np.arange(len(target_names))\n",
    "        plt.xticks(tick_marks, target_names, rotation=45)\n",
    "        plt.yticks(tick_marks, target_names)\n",
    "    thresh = cm.max() / 1.5 if normalize else cm.max() / 2\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        if normalize:\n",
    "            plt.text(j, i, \"{:0.4f}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "        else:\n",
    "            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))\n",
    "    plt.show()\n",
    "    \n",
    "MAX_LEN = 37\n",
    "\n",
    "print (\"Conteo por stance val\\n\", Counter(val['Label']))\n",
    "print (\"Conteo por stance test\\n\", Counter(test['Label']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1021, 37, 200)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids_val=val['Tw_id']\n",
    "tws_val=val['Tweet']\n",
    "labels_val=[d_lab[lb] for lb in val['Label']]\n",
    "\n",
    "etiq_v=labels_val.copy()\n",
    "etiq_v=np.asarray(etiq_v)\n",
    "sentences_val = tws_val\n",
    "n_labels_val = np.array(etiq_v)\n",
    "y_val=to_categorical(n_labels_val,num_classes=4)\n",
    "\n",
    "input_ids_val=[]\n",
    "for sent in sentences_val:\n",
    "    sent_cl=cleaner(sent)[1]\n",
    "    encoded_sent=[]\n",
    "    for wd in sent_cl:\n",
    "        try: \n",
    "            encoded_sent.append(Word2Index[wd])\n",
    "        except: \n",
    "            continue   \n",
    "    input_ids_val.append(encoded_sent)\n",
    "    \n",
    "ids_test=test['Tw_id']\n",
    "tws_test=test['Tweet']\n",
    "labels_test=[d_lab[lb] for lb in test['Label']]\n",
    "\n",
    "etiq_t=labels_test.copy()\n",
    "etiq_t=np.asarray(etiq_t)\n",
    "sentences_test = tws_test\n",
    "n_labels_test = np.array(etiq_t)\n",
    "y_test=to_categorical(n_labels_test,num_classes=4)\n",
    "\n",
    "input_ids_test=[]\n",
    "for sent in sentences_test:\n",
    "    sent_cl=cleaner(sent)[1]\n",
    "    encoded_sent=[]\n",
    "    for wd in sent_cl:\n",
    "        try: \n",
    "            encoded_sent.append(Word2Index[wd])\n",
    "        except: \n",
    "            continue   \n",
    "    input_ids_test.append(encoded_sent)\n",
    "    \n",
    "input_ids_val = pad_sequences(input_ids_val,maxlen=MAX_LEN,dtype=\"long\",value=0, truncating=\"post\", padding=\"post\")\n",
    "input_ids_test = pad_sequences(input_ids_test,maxlen=MAX_LEN,dtype=\"long\",value=0, truncating=\"post\", padding=\"post\")\n",
    "\n",
    "\n",
    "shape_val=np.asarray(input_ids_val).shape\n",
    "x_val=np.zeros((shape_val[0], shape_val[1], 200))\n",
    "i=0\n",
    "for in_id in input_ids_val:\n",
    "    x_val[i]=M_GLOVE_space[in_id]\n",
    "    i+=1\n",
    "    \n",
    "shape_test=np.asarray(input_ids_test).shape\n",
    "x_test=np.zeros((shape_test[0], shape_test[1], 200))\n",
    "i=0\n",
    "for in_id in input_ids_test:\n",
    "    x_test[i]=M_GLOVE_space[in_id]\n",
    "    i+=1\n",
    "\n",
    "x_test.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predecir_modelos(bs,predichos_all):  #best fit\n",
    "    temp=predichos_all.copy()\n",
    "    final=[np.argmax(pred) for pred in temp]  \n",
    "    confianzas=[temp[i][final[i]] for i in range(len(final))]\n",
    "    predichos_new=[]\n",
    "    for i in range(len(temp)):\n",
    "        indice=final[i]%4\n",
    "        predichos_new.append(int(indice))\n",
    "    return predichos_new,final,confianzas    \n",
    "\n",
    "def predecir_modelos_norm(bs,predichos_all):  #normalizado\n",
    "    temp=predichos_all.copy()\n",
    "    final=[]\n",
    "    confianzas=[]\n",
    "    for pred in temp:   \n",
    "        aux=np.ones(4)\n",
    "        for i in range(1,6): #for machine pred\n",
    "            sub=pred[(4*(i-1)):(4*i)]\n",
    "            aux=aux*np.asarray(sub)\n",
    "            \n",
    "        aux=aux/np.sum(aux)\n",
    "        final.append(np.argmax(aux))\n",
    "        confianzas.append(aux[np.argmax(aux)])  \n",
    "           \n",
    "    predichos_new=[]\n",
    "    for i in range(len(temp)):\n",
    "        predichos_new.append(final[i])\n",
    "    return predichos_new,final,confianzas\n",
    "    \n",
    "def predecir_modelos_average(bs,predichos_all): #average\n",
    "    temp=predichos_all.copy()\n",
    "    final=[]\n",
    "    confianzas=[]\n",
    "    for pred in temp:\n",
    "        aux=np.zeros(4)\n",
    "        for i in range(1,6):\n",
    "            sub=pred[(4*(i-1)):(4*i)]\n",
    "            aux=aux+np.asarray(sub)\n",
    "            \n",
    "        aux=aux/5.0 #dividido en el total de machines\n",
    "        final.append(np.argmax(aux))\n",
    "        confianzas.append(aux[np.argmax(aux)])  \n",
    "           \n",
    "    predichos_new=[]\n",
    "    for i in range(len(temp)):\n",
    "        predichos_new.append(final[i])\n",
    "    return predichos_new,final,confianzas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "del embeddings_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.0 sin pesos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------- EJECUCIÓN #1----------------\n",
      "WARNING:tensorflow:From /home/casapanshop/anaconda2/envs/newpy3_tf1/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/casapanshop/anaconda2/envs/newpy3_tf1/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/casapanshop/anaconda2/envs/newpy3_tf1/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/casapanshop/anaconda2/envs/newpy3_tf1/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/casapanshop/anaconda2/envs/newpy3_tf1/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/casapanshop/anaconda2/envs/newpy3_tf1/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:Large dropout rate: 0.65 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.65 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:From /home/casapanshop/anaconda2/envs/newpy3_tf1/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/casapanshop/anaconda2/envs/newpy3_tf1/lib/python3.6/site-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/casapanshop/anaconda2/envs/newpy3_tf1/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1205: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:Large dropout rate: 0.65 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.65 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.65 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "Agregando predicciones Val set\n",
      "Agregando predicciones Test set rnn3\n",
      "--------VALIDATION SET--------\n",
      "\n",
      "Committee Best Fit\n",
      "Accuracy: 0.6868327402135231\n",
      "F1-macro: 0.5842391323878955\n",
      "F1-score SDQC: 0.5842391323878955\n",
      "\n",
      "Committee Norm\n",
      "Accuracy: 0.7046263345195729\n",
      "F1-macro: 0.5722039515517776\n",
      "F1-score SDQC: 0.5722039515517776\n",
      "\n",
      "Committee Voting\n",
      "Accuracy: 0.7153024911032029\n",
      "F1-macro: 0.6020568031173422\n",
      "F1-score SDQC: 0.6020568031173422\n",
      "--------TESTING SET--------\n",
      "\n",
      "Committee Best Fit\n",
      "Accuracy: 0.66307541625857\n",
      "F1-macro: 0.4565804672571612\n",
      "F1-score SDQC: 0.4565804672571612\n",
      "\n",
      "Committee Norm\n",
      "Accuracy: 0.7561214495592556\n",
      "F1-macro: 0.46105320531535177\n",
      "F1-score SDQC: 0.46105320531535177\n",
      "\n",
      "Committee Voting\n",
      "Accuracy: 0.7502448579823702\n",
      "F1-macro: 0.44417138184596283\n",
      "F1-score SDQC: 0.44417138184596283\n",
      "\n",
      "---------------- EJECUCIÓN #2----------------\n",
      "Agregando predicciones Val set\n",
      "Agregando predicciones Test set rnn3\n",
      "--------VALIDATION SET--------\n",
      "\n",
      "Committee Best Fit\n",
      "Accuracy: 0.6476868327402135\n",
      "F1-macro: 0.48085002284347034\n",
      "F1-score SDQC: 0.48085002284347034\n",
      "\n",
      "Committee Norm\n",
      "Accuracy: 0.6903914590747331\n",
      "F1-macro: 0.48905451613529954\n",
      "F1-score SDQC: 0.48905451613529954\n",
      "\n",
      "Committee Voting\n",
      "Accuracy: 0.7046263345195729\n",
      "F1-macro: 0.5203005444874015\n",
      "F1-score SDQC: 0.5203005444874015\n",
      "--------TESTING SET--------\n",
      "\n",
      "Committee Best Fit\n",
      "Accuracy: 0.732615083251714\n",
      "F1-macro: 0.38040790392597384\n",
      "F1-score SDQC: 0.38040790392597384\n",
      "\n",
      "Committee Norm\n",
      "Accuracy: 0.7639569049951028\n",
      "F1-macro: 0.35127155805724347\n",
      "F1-score SDQC: 0.35127155805724347\n",
      "\n",
      "Committee Voting\n",
      "Accuracy: 0.7668952007835456\n",
      "F1-macro: 0.3507193302606625\n",
      "F1-score SDQC: 0.3507193302606625\n",
      "\n",
      "---------------- EJECUCIÓN #3----------------\n",
      "Agregando predicciones Val set\n",
      "Agregando predicciones Test set rnn3\n",
      "--------VALIDATION SET--------\n",
      "\n",
      "Committee Best Fit\n",
      "Accuracy: 0.6192170818505338\n",
      "F1-macro: 0.45267438666502835\n",
      "F1-score SDQC: 0.45267438666502835\n",
      "\n",
      "Committee Norm\n",
      "Accuracy: 0.6868327402135231\n",
      "F1-macro: 0.5066974456268606\n",
      "F1-score SDQC: 0.5066974456268606\n",
      "\n",
      "Committee Voting\n",
      "Accuracy: 0.693950177935943\n",
      "F1-macro: 0.5150933702703657\n",
      "F1-score SDQC: 0.5150933702703657\n",
      "--------TESTING SET--------\n",
      "\n",
      "Committee Best Fit\n",
      "Accuracy: 0.6983349657198825\n",
      "F1-macro: 0.40015987685331234\n",
      "F1-score SDQC: 0.40015987685331234\n",
      "\n",
      "Committee Norm\n",
      "Accuracy: 0.7502448579823702\n",
      "F1-macro: 0.41501792628406453\n",
      "F1-score SDQC: 0.41501792628406453\n",
      "\n",
      "Committee Voting\n",
      "Accuracy: 0.7551420176297747\n",
      "F1-macro: 0.4164788227244347\n",
      "F1-score SDQC: 0.4164788227244347\n",
      "\n",
      "---------------- EJECUCIÓN #4----------------\n",
      "Agregando predicciones Val set\n",
      "Agregando predicciones Test set rnn3\n",
      "--------VALIDATION SET--------\n",
      "\n",
      "Committee Best Fit\n",
      "Accuracy: 0.6441281138790036\n",
      "F1-macro: 0.48239617539285773\n",
      "F1-score SDQC: 0.48239617539285773\n",
      "\n",
      "Committee Norm\n",
      "Accuracy: 0.7153024911032029\n",
      "F1-macro: 0.5236508897926222\n",
      "F1-score SDQC: 0.5236508897926222\n",
      "\n",
      "Committee Voting\n",
      "Accuracy: 0.7153024911032029\n",
      "F1-macro: 0.529336648399973\n",
      "F1-score SDQC: 0.529336648399973\n",
      "--------TESTING SET--------\n",
      "\n",
      "Committee Best Fit\n",
      "Accuracy: 0.6856023506366308\n",
      "F1-macro: 0.43606223389328413\n",
      "F1-score SDQC: 0.43606223389328413\n",
      "\n",
      "Committee Norm\n",
      "Accuracy: 0.7571008814887366\n",
      "F1-macro: 0.4385100415587222\n",
      "F1-score SDQC: 0.4385100415587222\n",
      "\n",
      "Committee Voting\n",
      "Accuracy: 0.7600391772771793\n",
      "F1-macro: 0.4215360527216197\n",
      "F1-score SDQC: 0.4215360527216197\n",
      "\n",
      "---------------- EJECUCIÓN #5----------------\n",
      "Agregando predicciones Val set\n",
      "Agregando predicciones Test set rnn3\n",
      "--------VALIDATION SET--------\n",
      "\n",
      "Committee Best Fit\n",
      "Accuracy: 0.6903914590747331\n",
      "F1-macro: 0.5353325522882151\n",
      "F1-score SDQC: 0.5353325522882151\n",
      "\n",
      "Committee Norm\n",
      "Accuracy: 0.7046263345195729\n",
      "F1-macro: 0.5496672629695886\n",
      "F1-score SDQC: 0.5496672629695886\n",
      "\n",
      "Committee Voting\n",
      "Accuracy: 0.708185053380783\n",
      "F1-macro: 0.5256489248177227\n",
      "F1-score SDQC: 0.5256489248177227\n",
      "--------TESTING SET--------\n",
      "\n",
      "Committee Best Fit\n",
      "Accuracy: 0.7149853085210578\n",
      "F1-macro: 0.41796654091702984\n",
      "F1-score SDQC: 0.41796654091702984\n",
      "\n",
      "Committee Norm\n",
      "Accuracy: 0.7453476983349657\n",
      "F1-macro: 0.3977516058724093\n",
      "F1-score SDQC: 0.3977516058724093\n",
      "\n",
      "Committee Voting\n",
      "Accuracy: 0.7473065621939275\n",
      "F1-macro: 0.4067885858308743\n",
      "F1-score SDQC: 0.4067885858308743\n"
     ]
    }
   ],
   "source": [
    "prob=0.0\n",
    "path='../new_glove_augmented/stance_baselines/'\n",
    "bs=32\n",
    "    \n",
    "fs_macro_val={'cnn1':[], 'cnn2':[], 'rnn1':[],'rnn2':[],'rnn3':[]}\n",
    "fs_none_val={'cnn1':[], 'cnn2':[], 'rnn1':[],'rnn2':[],'rnn3':[]}\n",
    "accs_val={'cnn1':[], 'cnn2':[], 'rnn1':[],'rnn2':[],'rnn3':[]}\n",
    "confusions_val={'cnn1':[], 'cnn2':[], 'rnn1':[],'rnn2':[],'rnn3':[]}\n",
    "\n",
    "fs_macro_test={'cnn1':[], 'cnn2':[], 'rnn1':[],'rnn2':[],'rnn3':[]}\n",
    "fs_none_test={'cnn1':[], 'cnn2':[], 'rnn1':[],'rnn2':[],'rnn3':[]}\n",
    "accs_test={'cnn1':[], 'cnn2':[], 'rnn1':[],'rnn2':[],'rnn3':[]}\n",
    "confusions_test={'cnn1':[], 'cnn2':[], 'rnn1':[],'rnn2':[],'rnn3':[]}\n",
    "\n",
    "best={'acc':[], 'none':[], 'macro':[]}\n",
    "norm={'acc':[], 'none':[], 'macro':[]}\n",
    "voting={'acc':[], 'none':[], 'macro':[]}\n",
    "best_test={'acc':[], 'none':[], 'macro':[]}\n",
    "norm_test={'acc':[], 'none':[], 'macro':[]}\n",
    "voting_test={'acc':[], 'none':[], 'macro':[]}\n",
    "\n",
    "for j in range(1,6):\n",
    "    print (\"\")\n",
    "    print (\"---------------- EJECUCIÓN #\"+str(j)+'----------------')\n",
    "    cnn1= load_model(path+str(prob)+'/cnn1_'+str(j)+'-exec.h5', custom_objects={'FocalLoss': focal_loss, 'focal_loss_fixed': focal_loss()})  \n",
    "    cnn2= load_model(path+str(prob)+'/cnn2_'+str(j)+'-exec.h5', custom_objects={'FocalLoss': focal_loss, 'focal_loss_fixed': focal_loss()}) \n",
    "    rnn1= load_model(path+str(prob)+'/rnn1_'+str(j)+'-exec.h5', custom_objects={'FocalLoss': focal_loss, 'focal_loss_fixed': focal_loss()}) \n",
    "    rnn2= load_model(path+str(prob)+'/rnn2_'+str(j)+'-exec.h5', custom_objects={'FocalLoss': focal_loss, 'focal_loss_fixed': focal_loss()}) \n",
    "    rnn3= load_model(path+str(prob)+'/rnn3_'+str(j)+'-exec.h5') \n",
    "    \n",
    "    list_models=['cnn1', 'cnn2', 'rnn1', 'rnn2', 'rnn3']\n",
    "    index_models=np.arange(5)\n",
    "    dict_models=dict((key, value) for (key, value) in zip(index_models,list_models))\n",
    "    modelos=[cnn1, cnn2, rnn1, rnn2, rnn3]\n",
    "    ind=np.arange(5)\n",
    "    dict_trainedModel=dict((key, value) for (key, value) in zip(ind,modelos))\n",
    "    \n",
    "    print (\"Agregando predicciones Val set\")\n",
    "    for i in dict_trainedModel.keys(): \n",
    "        a_evaluar=dict_models[i]\n",
    "        predicho=dict_trainedModel[i].predict(x_val, batch_size=bs)\n",
    "        predicho=[np.argmax(pred) for pred in predicho]\n",
    "        acc= accuracy_score(etiq_v, predicho)  \n",
    "        f1_ma=f1_score(etiq_v, predicho, average='macro')\n",
    "        f1_no=f1_score(etiq_v, predicho, average=None)\n",
    "        matriz=normalize(confusion_matrix(etiq_v, predicho))\n",
    "        fs_macro_val[dict_models[i]].append(f1_ma)\n",
    "        fs_none_val[dict_models[i]].append(f1_no)\n",
    "        accs_val[dict_models[i]].append(acc)\n",
    "        confusions_val[dict_models[i]].append(matriz)\n",
    "        \n",
    "    predicciones_all_val=[]\n",
    "    for i in dict_trainedModel.keys(): \n",
    "        a_evaluar=dict_models[i]\n",
    "        predicciones_all_val.append(dict_trainedModel[i].predict(x_val, batch_size=bs))\n",
    "    predichos_all_val=np.concatenate(np.asarray(predicciones_all_val),axis=-1)\n",
    "    \n",
    "    print (\"Agregando predicciones Test set\", dict_models[i])    \n",
    "    for i in dict_trainedModel.keys(): \n",
    "        a_evaluar=dict_models[i]\n",
    "        predicho=dict_trainedModel[i].predict(x_test, batch_size=bs)\n",
    "        predicho=[np.argmax(pred) for pred in predicho]\n",
    "        acc= accuracy_score(etiq_t, predicho)  \n",
    "        f1_ma=f1_score(etiq_t, predicho, average='macro')\n",
    "        f1_no=f1_score(etiq_t, predicho, average=None)\n",
    "        matriz=normalize(confusion_matrix(etiq_t, predicho))\n",
    "        fs_macro_test[dict_models[i]].append(f1_ma)\n",
    "        fs_none_test[dict_models[i]].append(f1_no)\n",
    "        accs_test[dict_models[i]].append(acc)\n",
    "        confusions_test[dict_models[i]].append(matriz)\n",
    "\n",
    "    predicciones_all_test=[]\n",
    "    bs=32\n",
    "    for i in dict_trainedModel.keys(): \n",
    "        a_evaluar=dict_models[i]\n",
    "        predicciones_all_test.append(dict_trainedModel[i].predict(x_test, batch_size=bs))\n",
    "    predichos_all_test=np.concatenate(np.asarray(predicciones_all_test),axis=-1)\n",
    "    \n",
    "    print (\"--------VALIDATION SET--------\")\n",
    "    \n",
    "    print (\"\\nCommittee Best Fit\")\n",
    "    trainPredicho, maxPredicho, confianzas = predecir_modelos(bs,predichos_all_val)\n",
    "    acc_comite= accuracy_score(etiq_v, trainPredicho)  \n",
    "    print(\"Accuracy:\",acc_comite)\n",
    "    f1=f1_score(etiq_v, trainPredicho, average='macro') \n",
    "    print(\"F1-macro:\",f1)\n",
    "    f1_no=f1_score(etiq_v, trainPredicho, average=None)  \n",
    "    print(\"F1-score SDQC:\",f1)\n",
    "    best['acc'].append(acc_comite)\n",
    "    best['macro'].append(f1)\n",
    "    best['none'].append(f1_no)\n",
    "    \n",
    "    print (\"\\nCommittee Norm\")\n",
    "    trainPredicho, maxPredicho, confianzas = predecir_modelos_norm(bs,predichos_all_val)\n",
    "    acc_comite= accuracy_score(etiq_v, trainPredicho)  \n",
    "    print(\"Accuracy:\",acc_comite)\n",
    "    f1=f1_score(etiq_v, trainPredicho, average='macro') \n",
    "    print(\"F1-macro:\",f1)\n",
    "    f1_no=f1_score(etiq_v, trainPredicho, average=None) \n",
    "    print(\"F1-score SDQC:\",f1)\n",
    "    norm['acc'].append(acc_comite)\n",
    "    norm['macro'].append(f1)\n",
    "    norm['none'].append(f1_no)\n",
    "\n",
    "    print (\"\\nCommittee Voting\")\n",
    "    trainPredicho, maxPredicho, confianzas = predecir_modelos_average(bs,predichos_all_val)\n",
    "    acc_comite= accuracy_score(etiq_v, trainPredicho)  \n",
    "    print(\"Accuracy:\",acc_comite)\n",
    "    f1=f1_score(etiq_v, trainPredicho, average='macro') \n",
    "    print(\"F1-macro:\",f1)\n",
    "    f1_no=f1_score(etiq_v, trainPredicho, average=None)  \n",
    "    print(\"F1-score SDQC:\",f1)\n",
    "    voting['acc'].append(acc_comite)\n",
    "    voting['macro'].append(f1)\n",
    "    voting['none'].append(f1_no)\n",
    "    \n",
    "    print (\"--------TESTING SET--------\")\n",
    "    \n",
    "    print (\"\\nCommittee Best Fit\")\n",
    "    trainPredicho, maxPredicho, confianzas = predecir_modelos(bs,predichos_all_test)\n",
    "    acc_comite= accuracy_score(etiq_t, trainPredicho)  \n",
    "    print(\"Accuracy:\",acc_comite)\n",
    "    f1=f1_score(etiq_t, trainPredicho, average='macro') \n",
    "    print(\"F1-macro:\",f1)\n",
    "    f1_no=f1_score(etiq_t, trainPredicho, average=None)  \n",
    "    print(\"F1-score SDQC:\",f1)\n",
    "    best_test['acc'].append(acc_comite)\n",
    "    best_test['macro'].append(f1)\n",
    "    best_test['none'].append(f1_no)\n",
    "\n",
    "    print (\"\\nCommittee Norm\")\n",
    "    trainPredicho, maxPredicho, confianzas = predecir_modelos_norm(bs,predichos_all_test)\n",
    "    acc_comite= accuracy_score(etiq_t, trainPredicho)  \n",
    "    print(\"Accuracy:\",acc_comite)\n",
    "    f1=f1_score(etiq_t, trainPredicho, average='macro') \n",
    "    print(\"F1-macro:\",f1)\n",
    "    f1_no=f1_score(etiq_t, trainPredicho, average=None) \n",
    "    print(\"F1-score SDQC:\",f1)\n",
    "    norm_test['acc'].append(acc_comite)\n",
    "    norm_test['macro'].append(f1)\n",
    "    norm_test['none'].append(f1_no)\n",
    "\n",
    "    print (\"\\nCommittee Voting\")\n",
    "    trainPredicho, maxPredicho, confianzas = predecir_modelos_average(bs,predichos_all_test)\n",
    "    acc_comite= accuracy_score(etiq_t, trainPredicho)  \n",
    "    print(\"Accuracy:\",acc_comite)\n",
    "    f1=f1_score(etiq_t, trainPredicho, average='macro') \n",
    "    print(\"F1-macro:\",f1)\n",
    "    f1_no=f1_score(etiq_t, trainPredicho, average=None)  \n",
    "    print(\"F1-score SDQC:\",f1)\n",
    "    voting_test['acc'].append(acc_comite)\n",
    "    voting_test['macro'].append(f1)\n",
    "    voting_test['none'].append(f1_no)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0.24285714, 0.15909091, 0.58883249, 0.85343228]),\n",
       " array([0.02380952, 0.10126582, 0.41772152, 0.86228937]),\n",
       " array([0.12612613, 0.24761905, 0.43312102, 0.85320551]),\n",
       " array([0.22033898, 0.17391304, 0.50574713, 0.85404101]),\n",
       " array([0.21052632, 0.09876543, 0.43312102, 0.84859366])]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_test['none']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TEST] Acc Best comité: 0.6989226248775711\n",
      "[TEST] F1-score SDQC Best comité: [0.18813701 0.20515302 0.46382251 0.81582908]\n",
      "[TEST] F1 macro Best comité: 0.41823540456935226\n",
      "-----------\n",
      "[TEST] Acc Norm comité: 0.7545543584720862\n",
      "[TEST] F1-score SDQC Norm comité: [0.16473162 0.15613085 0.47570863 0.85431237]\n",
      "[TEST] F1 macro Norm comité: 0.41272086741755826\n",
      "-----------\n",
      "[TEST] Acc Voting comité: 0.7559255631733595\n",
      "[TEST] F1-score SDQC Voting comité: [0.1224249  0.16118902 0.49242577 0.85571564]\n",
      "[TEST] F1 macro Voting comité: 0.4079388346767108\n"
     ]
    }
   ],
   "source": [
    "m=5.0\n",
    "\n",
    "print (\"[TEST] Acc Best comité:\",np.sum(np.asarray(best_test['acc'])/m))\n",
    "temp=np.zeros(4)\n",
    "for result in best_test['none']:\n",
    "    temp+=result\n",
    "print (\"[TEST] F1-score SDQC Best comité:\" ,temp/m)\n",
    "print (\"[TEST] F1 macro Best comité:\",np.sum(np.asarray(best_test['macro'])/m))\n",
    "print (\"-----------\")\n",
    "print (\"[TEST] Acc Norm comité:\",np.sum(np.asarray(norm_test['acc'])/m))\n",
    "temp=np.zeros(4)\n",
    "for result in norm_test['none']:\n",
    "    temp+=result\n",
    "print (\"[TEST] F1-score SDQC Norm comité:\" ,temp/m)\n",
    "print (\"[TEST] F1 macro Norm comité:\",np.sum(np.asarray(norm_test['macro'])/m))\n",
    "print (\"-----------\")\n",
    "print (\"[TEST] Acc Voting comité:\",np.sum(np.asarray(voting_test['acc'])/m))\n",
    "temp=np.zeros(4)\n",
    "for result in voting_test['none']:\n",
    "    temp+=result\n",
    "print (\"[TEST] F1-score SDQC Voting comité:\" ,temp/m)\n",
    "print (\"[TEST] F1 macro Voting comité:\",np.sum(np.asarray(voting_test['macro'])/m))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.0 cw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------- EJECUCIÓN #1----------------\n",
      "Agregando predicciones Val set\n",
      "Agregando predicciones Test set rnn3\n",
      "--------VALIDATION SET--------\n",
      "\n",
      "Committee Best Fit\n",
      "Accuracy: 0.604982206405694\n",
      "F1-macro: 0.5101315017126793\n",
      "F1-score SDQC: 0.5101315017126793\n",
      "\n",
      "Committee Norm\n",
      "Accuracy: 0.6797153024911032\n",
      "F1-macro: 0.5667279611882622\n",
      "F1-score SDQC: 0.5667279611882622\n",
      "\n",
      "Committee Voting\n",
      "Accuracy: 0.6548042704626335\n",
      "F1-macro: 0.540050066768629\n",
      "F1-score SDQC: 0.540050066768629\n",
      "--------TESTING SET--------\n",
      "\n",
      "Committee Best Fit\n",
      "Accuracy: 0.6072477962781586\n",
      "F1-macro: 0.4080003675688219\n",
      "F1-score SDQC: 0.4080003675688219\n",
      "\n",
      "Committee Norm\n",
      "Accuracy: 0.7159647404505387\n",
      "F1-macro: 0.4583285235820511\n",
      "F1-score SDQC: 0.4583285235820511\n",
      "\n",
      "Committee Voting\n",
      "Accuracy: 0.693437806072478\n",
      "F1-macro: 0.4594721958182371\n",
      "F1-score SDQC: 0.4594721958182371\n",
      "\n",
      "---------------- EJECUCIÓN #2----------------\n",
      "Agregando predicciones Val set\n",
      "Agregando predicciones Test set rnn3\n",
      "--------VALIDATION SET--------\n",
      "\n",
      "Committee Best Fit\n",
      "Accuracy: 0.6619217081850534\n",
      "F1-macro: 0.5417529330572808\n",
      "F1-score SDQC: 0.5417529330572808\n",
      "\n",
      "Committee Norm\n",
      "Accuracy: 0.7046263345195729\n",
      "F1-macro: 0.6213218026200096\n",
      "F1-score SDQC: 0.6213218026200096\n",
      "\n",
      "Committee Voting\n",
      "Accuracy: 0.697508896797153\n",
      "F1-macro: 0.5995864762550498\n",
      "F1-score SDQC: 0.5995864762550498\n",
      "--------TESTING SET--------\n",
      "\n",
      "Committee Best Fit\n",
      "Accuracy: 0.6356513222331048\n",
      "F1-macro: 0.40944900994344297\n",
      "F1-score SDQC: 0.40944900994344297\n",
      "\n",
      "Committee Norm\n",
      "Accuracy: 0.7081292850146915\n",
      "F1-macro: 0.45377217902062594\n",
      "F1-score SDQC: 0.45377217902062594\n",
      "\n",
      "Committee Voting\n",
      "Accuracy: 0.6953966699314398\n",
      "F1-macro: 0.4640946704232791\n",
      "F1-score SDQC: 0.4640946704232791\n",
      "\n",
      "---------------- EJECUCIÓN #3----------------\n",
      "Agregando predicciones Val set\n",
      "Agregando predicciones Test set rnn3\n",
      "--------VALIDATION SET--------\n",
      "\n",
      "Committee Best Fit\n",
      "Accuracy: 0.6868327402135231\n",
      "F1-macro: 0.5589427508146529\n",
      "F1-score SDQC: 0.5589427508146529\n",
      "\n",
      "Committee Norm\n",
      "Accuracy: 0.7295373665480427\n",
      "F1-macro: 0.6171021957738545\n",
      "F1-score SDQC: 0.6171021957738545\n",
      "\n",
      "Committee Voting\n",
      "Accuracy: 0.7117437722419929\n",
      "F1-macro: 0.5984731985412353\n",
      "F1-score SDQC: 0.5984731985412353\n",
      "--------TESTING SET--------\n",
      "\n",
      "Committee Best Fit\n",
      "Accuracy: 0.7316356513222331\n",
      "F1-macro: 0.45764631610219847\n",
      "F1-score SDQC: 0.45764631610219847\n",
      "\n",
      "Committee Norm\n",
      "Accuracy: 0.7561214495592556\n",
      "F1-macro: 0.47638190047238554\n",
      "F1-score SDQC: 0.47638190047238554\n",
      "\n",
      "Committee Voting\n",
      "Accuracy: 0.7502448579823702\n",
      "F1-macro: 0.48241793835819874\n",
      "F1-score SDQC: 0.48241793835819874\n",
      "\n",
      "---------------- EJECUCIÓN #4----------------\n",
      "Agregando predicciones Val set\n",
      "Agregando predicciones Test set rnn3\n",
      "--------VALIDATION SET--------\n",
      "\n",
      "Committee Best Fit\n",
      "Accuracy: 0.6797153024911032\n",
      "F1-macro: 0.5672596221592429\n",
      "F1-score SDQC: 0.5672596221592429\n",
      "\n",
      "Committee Norm\n",
      "Accuracy: 0.701067615658363\n",
      "F1-macro: 0.5557078108885632\n",
      "F1-score SDQC: 0.5557078108885632\n",
      "\n",
      "Committee Voting\n",
      "Accuracy: 0.708185053380783\n",
      "F1-macro: 0.5913315505905473\n",
      "F1-score SDQC: 0.5913315505905473\n",
      "--------TESTING SET--------\n",
      "\n",
      "Committee Best Fit\n",
      "Accuracy: 0.6787463271302644\n",
      "F1-macro: 0.3966170115215336\n",
      "F1-score SDQC: 0.3966170115215336\n",
      "\n",
      "Committee Norm\n",
      "Accuracy: 0.742409402546523\n",
      "F1-macro: 0.44119141071920864\n",
      "F1-score SDQC: 0.44119141071920864\n",
      "\n",
      "Committee Voting\n",
      "Accuracy: 0.7463271302644466\n",
      "F1-macro: 0.4786363468165876\n",
      "F1-score SDQC: 0.4786363468165876\n",
      "\n",
      "---------------- EJECUCIÓN #5----------------\n",
      "Agregando predicciones Val set\n",
      "Agregando predicciones Test set rnn3\n",
      "--------VALIDATION SET--------\n",
      "\n",
      "Committee Best Fit\n",
      "Accuracy: 0.6120996441281139\n",
      "F1-macro: 0.5055713915380216\n",
      "F1-score SDQC: 0.5055713915380216\n",
      "\n",
      "Committee Norm\n",
      "Accuracy: 0.6654804270462633\n",
      "F1-macro: 0.5563772564083251\n",
      "F1-score SDQC: 0.5563772564083251\n",
      "\n",
      "Committee Voting\n",
      "Accuracy: 0.6263345195729537\n",
      "F1-macro: 0.5257664657094625\n",
      "F1-score SDQC: 0.5257664657094625\n",
      "--------TESTING SET--------\n",
      "\n",
      "Committee Best Fit\n",
      "Accuracy: 0.6366307541625857\n",
      "F1-macro: 0.42554573637473614\n",
      "F1-score SDQC: 0.42554573637473614\n",
      "\n",
      "Committee Norm\n",
      "Accuracy: 0.703232125367287\n",
      "F1-macro: 0.4748503396778849\n",
      "F1-score SDQC: 0.4748503396778849\n",
      "\n",
      "Committee Voting\n",
      "Accuracy: 0.6758080313418218\n",
      "F1-macro: 0.46907025676136543\n",
      "F1-score SDQC: 0.46907025676136543\n"
     ]
    }
   ],
   "source": [
    "prob=0.0\n",
    "path='../new_glove_augmented/stance_baselines/'\n",
    "bs=32\n",
    "    \n",
    "fs_macro_val={'cnn1':[], 'cnn2':[], 'rnn1':[],'rnn2':[],'rnn3':[]}\n",
    "fs_none_val={'cnn1':[], 'cnn2':[], 'rnn1':[],'rnn2':[],'rnn3':[]}\n",
    "accs_val={'cnn1':[], 'cnn2':[], 'rnn1':[],'rnn2':[],'rnn3':[]}\n",
    "confusions_val={'cnn1':[], 'cnn2':[], 'rnn1':[],'rnn2':[],'rnn3':[]}\n",
    "\n",
    "fs_macro_test={'cnn1':[], 'cnn2':[], 'rnn1':[],'rnn2':[],'rnn3':[]}\n",
    "fs_none_test={'cnn1':[], 'cnn2':[], 'rnn1':[],'rnn2':[],'rnn3':[]}\n",
    "accs_test={'cnn1':[], 'cnn2':[], 'rnn1':[],'rnn2':[],'rnn3':[]}\n",
    "confusions_test={'cnn1':[], 'cnn2':[], 'rnn1':[],'rnn2':[],'rnn3':[]}\n",
    "\n",
    "best={'acc':[], 'none':[], 'macro':[]}\n",
    "norm={'acc':[], 'none':[], 'macro':[]}\n",
    "voting={'acc':[], 'none':[], 'macro':[]}\n",
    "best_test={'acc':[], 'none':[], 'macro':[]}\n",
    "norm_test={'acc':[], 'none':[], 'macro':[]}\n",
    "voting_test={'acc':[], 'none':[], 'macro':[]}\n",
    "\n",
    "for j in range(1,6):\n",
    "    print (\"\")\n",
    "    print (\"---------------- EJECUCIÓN #\"+str(j)+'----------------')\n",
    "    cnn1= load_model(path+str(prob)+'_cw/cnn1_'+str(j)+'-exec.h5', custom_objects={'FocalLoss': focal_loss, 'focal_loss_fixed': focal_loss()})  \n",
    "    cnn2= load_model(path+str(prob)+'_cw/cnn2_'+str(j)+'-exec.h5', custom_objects={'FocalLoss': focal_loss, 'focal_loss_fixed': focal_loss()}) \n",
    "    rnn1= load_model(path+str(prob)+'_cw/rnn1_'+str(j)+'-exec.h5', custom_objects={'FocalLoss': focal_loss, 'focal_loss_fixed': focal_loss()}) \n",
    "    rnn2= load_model(path+str(prob)+'_cw/rnn2_'+str(j)+'-exec.h5', custom_objects={'FocalLoss': focal_loss, 'focal_loss_fixed': focal_loss()}) \n",
    "    rnn3= load_model(path+str(prob)+'_cw/rnn3_'+str(j)+'-exec.h5') \n",
    "    \n",
    "    list_models=['cnn1', 'cnn2', 'rnn1', 'rnn2', 'rnn3']\n",
    "    index_models=np.arange(5)\n",
    "    dict_models=dict((key, value) for (key, value) in zip(index_models,list_models))\n",
    "    modelos=[cnn1, cnn2, rnn1, rnn2, rnn3]\n",
    "    ind=np.arange(5)\n",
    "    dict_trainedModel=dict((key, value) for (key, value) in zip(ind,modelos))\n",
    "    \n",
    "    print (\"Agregando predicciones Val set\")\n",
    "    for i in dict_trainedModel.keys(): \n",
    "        a_evaluar=dict_models[i]\n",
    "        predicho=dict_trainedModel[i].predict(x_val, batch_size=bs)\n",
    "        predicho=[np.argmax(pred) for pred in predicho]\n",
    "        acc= accuracy_score(etiq_v, predicho)  \n",
    "        f1_ma=f1_score(etiq_v, predicho, average='macro')\n",
    "        f1_no=f1_score(etiq_v, predicho, average=None)\n",
    "        matriz=normalize(confusion_matrix(etiq_v, predicho))\n",
    "        fs_macro_val[dict_models[i]].append(f1_ma)\n",
    "        fs_none_val[dict_models[i]].append(f1_no)\n",
    "        accs_val[dict_models[i]].append(acc)\n",
    "        confusions_val[dict_models[i]].append(matriz)\n",
    "        \n",
    "    predicciones_all_val=[]\n",
    "    for i in dict_trainedModel.keys(): \n",
    "        a_evaluar=dict_models[i]\n",
    "        predicciones_all_val.append(dict_trainedModel[i].predict(x_val, batch_size=bs))\n",
    "    predichos_all_val=np.concatenate(np.asarray(predicciones_all_val),axis=-1)\n",
    "    \n",
    "    print (\"Agregando predicciones Test set\", dict_models[i])    \n",
    "    for i in dict_trainedModel.keys(): \n",
    "        a_evaluar=dict_models[i]\n",
    "        predicho=dict_trainedModel[i].predict(x_test, batch_size=bs)\n",
    "        predicho=[np.argmax(pred) for pred in predicho]\n",
    "        acc= accuracy_score(etiq_t, predicho)  \n",
    "        f1_ma=f1_score(etiq_t, predicho, average='macro')\n",
    "        f1_no=f1_score(etiq_t, predicho, average=None)\n",
    "        matriz=normalize(confusion_matrix(etiq_t, predicho))\n",
    "        fs_macro_test[dict_models[i]].append(f1_ma)\n",
    "        fs_none_test[dict_models[i]].append(f1_no)\n",
    "        accs_test[dict_models[i]].append(acc)\n",
    "        confusions_test[dict_models[i]].append(matriz)\n",
    "\n",
    "    predicciones_all_test=[]\n",
    "    bs=32\n",
    "    for i in dict_trainedModel.keys(): \n",
    "        a_evaluar=dict_models[i]\n",
    "        predicciones_all_test.append(dict_trainedModel[i].predict(x_test, batch_size=bs))\n",
    "    predichos_all_test=np.concatenate(np.asarray(predicciones_all_test),axis=-1)\n",
    "    \n",
    "    print (\"--------VALIDATION SET--------\")\n",
    "    \n",
    "    print (\"\\nCommittee Best Fit\")\n",
    "    trainPredicho, maxPredicho, confianzas = predecir_modelos(bs,predichos_all_val)\n",
    "    acc_comite= accuracy_score(etiq_v, trainPredicho)  \n",
    "    print(\"Accuracy:\",acc_comite)\n",
    "    f1=f1_score(etiq_v, trainPredicho, average='macro') \n",
    "    print(\"F1-macro:\",f1)\n",
    "    f1_no=f1_score(etiq_v, trainPredicho, average=None)  \n",
    "    print(\"F1-score SDQC:\",f1)\n",
    "    best['acc'].append(acc_comite)\n",
    "    best['macro'].append(f1)\n",
    "    best['none'].append(f1_no)\n",
    "    \n",
    "    print (\"\\nCommittee Norm\")\n",
    "    trainPredicho, maxPredicho, confianzas = predecir_modelos_norm(bs,predichos_all_val)\n",
    "    acc_comite= accuracy_score(etiq_v, trainPredicho)  \n",
    "    print(\"Accuracy:\",acc_comite)\n",
    "    f1=f1_score(etiq_v, trainPredicho, average='macro') \n",
    "    print(\"F1-macro:\",f1)\n",
    "    f1_no=f1_score(etiq_v, trainPredicho, average=None) \n",
    "    print(\"F1-score SDQC:\",f1)\n",
    "    norm['acc'].append(acc_comite)\n",
    "    norm['macro'].append(f1)\n",
    "    norm['none'].append(f1_no)\n",
    "\n",
    "    print (\"\\nCommittee Voting\")\n",
    "    trainPredicho, maxPredicho, confianzas = predecir_modelos_average(bs,predichos_all_val)\n",
    "    acc_comite= accuracy_score(etiq_v, trainPredicho)  \n",
    "    print(\"Accuracy:\",acc_comite)\n",
    "    f1=f1_score(etiq_v, trainPredicho, average='macro') \n",
    "    print(\"F1-macro:\",f1)\n",
    "    f1_no=f1_score(etiq_v, trainPredicho, average=None)  \n",
    "    print(\"F1-score SDQC:\",f1)\n",
    "    voting['acc'].append(acc_comite)\n",
    "    voting['macro'].append(f1)\n",
    "    voting['none'].append(f1_no)\n",
    "    \n",
    "    print (\"--------TESTING SET--------\")\n",
    "    \n",
    "    print (\"\\nCommittee Best Fit\")\n",
    "    trainPredicho, maxPredicho, confianzas = predecir_modelos(bs,predichos_all_test)\n",
    "    acc_comite= accuracy_score(etiq_t, trainPredicho)  \n",
    "    print(\"Accuracy:\",acc_comite)\n",
    "    f1=f1_score(etiq_t, trainPredicho, average='macro') \n",
    "    print(\"F1-macro:\",f1)\n",
    "    f1_no=f1_score(etiq_t, trainPredicho, average=None)  \n",
    "    print(\"F1-score SDQC:\",f1)\n",
    "    best_test['acc'].append(acc_comite)\n",
    "    best_test['macro'].append(f1)\n",
    "    best_test['none'].append(f1_no)\n",
    "\n",
    "    print (\"\\nCommittee Norm\")\n",
    "    trainPredicho, maxPredicho, confianzas = predecir_modelos_norm(bs,predichos_all_test)\n",
    "    acc_comite= accuracy_score(etiq_t, trainPredicho)  \n",
    "    print(\"Accuracy:\",acc_comite)\n",
    "    f1=f1_score(etiq_t, trainPredicho, average='macro') \n",
    "    print(\"F1-macro:\",f1)\n",
    "    f1_no=f1_score(etiq_t, trainPredicho, average=None) \n",
    "    print(\"F1-score SDQC:\",f1)\n",
    "    norm_test['acc'].append(acc_comite)\n",
    "    norm_test['macro'].append(f1)\n",
    "    norm_test['none'].append(f1_no)\n",
    "\n",
    "    print (\"\\nCommittee Voting\")\n",
    "    trainPredicho, maxPredicho, confianzas = predecir_modelos_average(bs,predichos_all_test)\n",
    "    acc_comite= accuracy_score(etiq_t, trainPredicho)  \n",
    "    print(\"Accuracy:\",acc_comite)\n",
    "    f1=f1_score(etiq_t, trainPredicho, average='macro') \n",
    "    print(\"F1-macro:\",f1)\n",
    "    f1_no=f1_score(etiq_t, trainPredicho, average=None)  \n",
    "    print(\"F1-score SDQC:\",f1)\n",
    "    voting_test['acc'].append(acc_comite)\n",
    "    voting_test['macro'].append(f1)\n",
    "    voting_test['none'].append(f1_no)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0.13559322, 0.28742515, 0.58653846, 0.82375726]),\n",
       " array([0.17218543, 0.24460432, 0.58      , 0.81829897]),\n",
       " array([0.18518519, 0.28125   , 0.58585859, 0.85323383]),\n",
       " array([0.15267176, 0.22018349, 0.54651163, 0.84539877]),\n",
       " array([0.17322835, 0.33898305, 0.57627119, 0.81091877])]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_test['none']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TEST] Acc Best comité: 0.6579823702252694\n",
      "[TEST] F1-score SDQC Best comité: [0.14356289 0.2336784  0.51980306 0.7807624 ]\n",
      "[TEST] F1 macro Best comité: 0.41945168830214663\n",
      "-----------\n",
      "[TEST] Acc Norm comité: 0.725171400587659\n",
      "[TEST] F1-score SDQC Norm comité: [0.16377279 0.2744892  0.57503597 0.83032152]\n",
      "[TEST] F1 macro Norm comité: 0.4609048706944312\n",
      "-----------\n",
      "[TEST] Acc Voting comité: 0.7122428991185114\n",
      "[TEST] F1-score SDQC Voting comité: [0.17422817 0.27404515 0.61543953 0.81924028]\n",
      "[TEST] F1 macro Voting comité: 0.47073828163553355\n"
     ]
    }
   ],
   "source": [
    "m=5.0\n",
    "\n",
    "print (\"[TEST] Acc Best comité:\",np.sum(np.asarray(best_test['acc'])/m))\n",
    "temp=np.zeros(4)\n",
    "for result in best_test['none']:\n",
    "    temp+=result\n",
    "print (\"[TEST] F1-score SDQC Best comité:\" ,temp/m)\n",
    "print (\"[TEST] F1 macro Best comité:\",np.sum(np.asarray(best_test['macro'])/m))\n",
    "print (\"-----------\")\n",
    "print (\"[TEST] Acc Norm comité:\",np.sum(np.asarray(norm_test['acc'])/m))\n",
    "temp=np.zeros(4)\n",
    "for result in norm_test['none']:\n",
    "    temp+=result\n",
    "print (\"[TEST] F1-score SDQC Norm comité:\" ,temp/m)\n",
    "print (\"[TEST] F1 macro Norm comité:\",np.sum(np.asarray(norm_test['macro'])/m))\n",
    "print (\"-----------\")\n",
    "print (\"[TEST] Acc Voting comité:\",np.sum(np.asarray(voting_test['acc'])/m))\n",
    "temp=np.zeros(4)\n",
    "for result in voting_test['none']:\n",
    "    temp+=result\n",
    "print (\"[TEST] F1-score SDQC Voting comité:\" ,temp/m)\n",
    "print (\"[TEST] F1 macro Voting comité:\",np.sum(np.asarray(voting_test['macro'])/m))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------- EJECUCIÓN #1----------------\n",
      "Agregando predicciones Val set\n",
      "Agregando predicciones Test set rnn3\n",
      "--------VALIDATION SET--------\n",
      "\n",
      "Committee Best Fit\n",
      "Accuracy: 0.608540925266904\n",
      "F1-macro: 0.49764221479337756\n",
      "F1-score SDQC: 0.49764221479337756\n",
      "\n",
      "Committee Norm\n",
      "Accuracy: 0.6654804270462633\n",
      "F1-macro: 0.5370115995115995\n",
      "F1-score SDQC: 0.5370115995115995\n",
      "\n",
      "Committee Voting\n",
      "Accuracy: 0.6548042704626335\n",
      "F1-macro: 0.5323753530494866\n",
      "F1-score SDQC: 0.5323753530494866\n",
      "--------TESTING SET--------\n",
      "\n",
      "Committee Best Fit\n",
      "Accuracy: 0.6679725759059746\n",
      "F1-macro: 0.39625098831448763\n",
      "F1-score SDQC: 0.39625098831448763\n",
      "\n",
      "Committee Norm\n",
      "Accuracy: 0.7375122428991185\n",
      "F1-macro: 0.4188669091935424\n",
      "F1-score SDQC: 0.4188669091935424\n",
      "\n",
      "Committee Voting\n",
      "Accuracy: 0.7316356513222331\n",
      "F1-macro: 0.420100160414801\n",
      "F1-score SDQC: 0.420100160414801\n",
      "\n",
      "---------------- EJECUCIÓN #2----------------\n",
      "Agregando predicciones Val set\n",
      "Agregando predicciones Test set rnn3\n",
      "--------VALIDATION SET--------\n",
      "\n",
      "Committee Best Fit\n",
      "Accuracy: 0.6405693950177936\n",
      "F1-macro: 0.5196126188003654\n",
      "F1-score SDQC: 0.5196126188003654\n",
      "\n",
      "Committee Norm\n",
      "Accuracy: 0.6832740213523132\n",
      "F1-macro: 0.559220268143082\n",
      "F1-score SDQC: 0.559220268143082\n",
      "\n",
      "Committee Voting\n",
      "Accuracy: 0.7046263345195729\n",
      "F1-macro: 0.5667026044129284\n",
      "F1-score SDQC: 0.5667026044129284\n",
      "--------TESTING SET--------\n",
      "\n",
      "Committee Best Fit\n",
      "Accuracy: 0.653281096963761\n",
      "F1-macro: 0.4044703938353404\n",
      "F1-score SDQC: 0.4044703938353404\n",
      "\n",
      "Committee Norm\n",
      "Accuracy: 0.7120470127326151\n",
      "F1-macro: 0.4250566012500441\n",
      "F1-score SDQC: 0.4250566012500441\n",
      "\n",
      "Committee Voting\n",
      "Accuracy: 0.7110675808031341\n",
      "F1-macro: 0.4130663445622351\n",
      "F1-score SDQC: 0.4130663445622351\n",
      "\n",
      "---------------- EJECUCIÓN #3----------------\n",
      "Agregando predicciones Val set\n",
      "Agregando predicciones Test set rnn3\n",
      "--------VALIDATION SET--------\n",
      "\n",
      "Committee Best Fit\n",
      "Accuracy: 0.5871886120996441\n",
      "F1-macro: 0.4424280319703748\n",
      "F1-score SDQC: 0.4424280319703748\n",
      "\n",
      "Committee Norm\n",
      "Accuracy: 0.6476868327402135\n",
      "F1-macro: 0.49429906518392486\n",
      "F1-score SDQC: 0.49429906518392486\n",
      "\n",
      "Committee Voting\n",
      "Accuracy: 0.6548042704626335\n",
      "F1-macro: 0.4777071196893071\n",
      "F1-score SDQC: 0.4777071196893071\n",
      "--------TESTING SET--------\n",
      "\n",
      "Committee Best Fit\n",
      "Accuracy: 0.6699314397649363\n",
      "F1-macro: 0.425215810668104\n",
      "F1-score SDQC: 0.425215810668104\n",
      "\n",
      "Committee Norm\n",
      "Accuracy: 0.7257590597453477\n",
      "F1-macro: 0.4356828082242963\n",
      "F1-score SDQC: 0.4356828082242963\n",
      "\n",
      "Committee Voting\n",
      "Accuracy: 0.7296767874632712\n",
      "F1-macro: 0.43975327092777483\n",
      "F1-score SDQC: 0.43975327092777483\n",
      "\n",
      "---------------- EJECUCIÓN #4----------------\n",
      "Agregando predicciones Val set\n",
      "Agregando predicciones Test set rnn3\n",
      "--------VALIDATION SET--------\n",
      "\n",
      "Committee Best Fit\n",
      "Accuracy: 0.6120996441281139\n",
      "F1-macro: 0.48008385658127417\n",
      "F1-score SDQC: 0.48008385658127417\n",
      "\n",
      "Committee Norm\n",
      "Accuracy: 0.6690391459074733\n",
      "F1-macro: 0.5206882191763075\n",
      "F1-score SDQC: 0.5206882191763075\n",
      "\n",
      "Committee Voting\n",
      "Accuracy: 0.6690391459074733\n",
      "F1-macro: 0.5212152627852895\n",
      "F1-score SDQC: 0.5212152627852895\n",
      "--------TESTING SET--------\n",
      "\n",
      "Committee Best Fit\n",
      "Accuracy: 0.6816846229187071\n",
      "F1-macro: 0.3704097141372222\n",
      "F1-score SDQC: 0.3704097141372222\n",
      "\n",
      "Committee Norm\n",
      "Accuracy: 0.7375122428991185\n",
      "F1-macro: 0.3949315341192562\n",
      "F1-score SDQC: 0.3949315341192562\n",
      "\n",
      "Committee Voting\n",
      "Accuracy: 0.7394711067580804\n",
      "F1-macro: 0.41236339479100337\n",
      "F1-score SDQC: 0.41236339479100337\n",
      "\n",
      "---------------- EJECUCIÓN #5----------------\n",
      "Agregando predicciones Val set\n",
      "Agregando predicciones Test set rnn3\n",
      "--------VALIDATION SET--------\n",
      "\n",
      "Committee Best Fit\n",
      "Accuracy: 0.6476868327402135\n",
      "F1-macro: 0.471542168212712\n",
      "F1-score SDQC: 0.471542168212712\n",
      "\n",
      "Committee Norm\n",
      "Accuracy: 0.6725978647686833\n",
      "F1-macro: 0.46728757514421\n",
      "F1-score SDQC: 0.46728757514421\n",
      "\n",
      "Committee Voting\n",
      "Accuracy: 0.6583629893238434\n",
      "F1-macro: 0.4491260781560569\n",
      "F1-score SDQC: 0.4491260781560569\n",
      "--------TESTING SET--------\n",
      "\n",
      "Committee Best Fit\n",
      "Accuracy: 0.7140058765915769\n",
      "F1-macro: 0.3999397992195445\n",
      "F1-score SDQC: 0.3999397992195445\n",
      "\n",
      "Committee Norm\n",
      "Accuracy: 0.7502448579823702\n",
      "F1-macro: 0.40662595942610696\n",
      "F1-score SDQC: 0.40662595942610696\n",
      "\n",
      "Committee Voting\n",
      "Accuracy: 0.7531831537708129\n",
      "F1-macro: 0.4369519544173546\n",
      "F1-score SDQC: 0.4369519544173546\n"
     ]
    }
   ],
   "source": [
    "prob=0.15\n",
    "path='../new_glove_augmented/stance_baselines/'\n",
    "bs=32\n",
    "    \n",
    "fs_macro_val={'cnn1':[], 'cnn2':[], 'rnn1':[],'rnn2':[],'rnn3':[]}\n",
    "fs_none_val={'cnn1':[], 'cnn2':[], 'rnn1':[],'rnn2':[],'rnn3':[]}\n",
    "accs_val={'cnn1':[], 'cnn2':[], 'rnn1':[],'rnn2':[],'rnn3':[]}\n",
    "confusions_val={'cnn1':[], 'cnn2':[], 'rnn1':[],'rnn2':[],'rnn3':[]}\n",
    "\n",
    "fs_macro_test={'cnn1':[], 'cnn2':[], 'rnn1':[],'rnn2':[],'rnn3':[]}\n",
    "fs_none_test={'cnn1':[], 'cnn2':[], 'rnn1':[],'rnn2':[],'rnn3':[]}\n",
    "accs_test={'cnn1':[], 'cnn2':[], 'rnn1':[],'rnn2':[],'rnn3':[]}\n",
    "confusions_test={'cnn1':[], 'cnn2':[], 'rnn1':[],'rnn2':[],'rnn3':[]}\n",
    "\n",
    "best={'acc':[], 'none':[], 'macro':[]}\n",
    "norm={'acc':[], 'none':[], 'macro':[]}\n",
    "voting={'acc':[], 'none':[], 'macro':[]}\n",
    "best_test={'acc':[], 'none':[], 'macro':[]}\n",
    "norm_test={'acc':[], 'none':[], 'macro':[]}\n",
    "voting_test={'acc':[], 'none':[], 'macro':[]}\n",
    "\n",
    "for j in range(1,6):\n",
    "    print (\"\")\n",
    "    print (\"---------------- EJECUCIÓN #\"+str(j)+'----------------')\n",
    "    cnn1= load_model(path+str(prob)+'/cnn1_'+str(j)+'-exec_app1_Top_1.h5', custom_objects={'FocalLoss': focal_loss, 'focal_loss_fixed': focal_loss()})  \n",
    "    cnn2= load_model(path+str(prob)+'/cnn2_'+str(j)+'-exec_app1_Top_1.h5', custom_objects={'FocalLoss': focal_loss, 'focal_loss_fixed': focal_loss()}) \n",
    "    rnn1= load_model(path+str(prob)+'/rnn1_'+str(j)+'-exec_app1_Top_1.h5', custom_objects={'FocalLoss': focal_loss, 'focal_loss_fixed': focal_loss()}) \n",
    "    rnn2= load_model(path+str(prob)+'/rnn2_'+str(j)+'-exec_app1_Top_1.h5', custom_objects={'FocalLoss': focal_loss, 'focal_loss_fixed': focal_loss()}) \n",
    "    rnn3= load_model(path+str(prob)+'/rnn3_'+str(j)+'-exec_app1_Top_1.h5') \n",
    "    \n",
    "    list_models=['cnn1', 'cnn2', 'rnn1', 'rnn2', 'rnn3']\n",
    "    index_models=np.arange(5)\n",
    "    dict_models=dict((key, value) for (key, value) in zip(index_models,list_models))\n",
    "    modelos=[cnn1, cnn2, rnn1, rnn2, rnn3]\n",
    "    ind=np.arange(5)\n",
    "    dict_trainedModel=dict((key, value) for (key, value) in zip(ind,modelos))\n",
    "    \n",
    "    print (\"Agregando predicciones Val set\")\n",
    "    for i in dict_trainedModel.keys(): \n",
    "        a_evaluar=dict_models[i]\n",
    "        predicho=dict_trainedModel[i].predict(x_val, batch_size=bs)\n",
    "        predicho=[np.argmax(pred) for pred in predicho]\n",
    "        acc= accuracy_score(etiq_v, predicho)  \n",
    "        f1_ma=f1_score(etiq_v, predicho, average='macro')\n",
    "        f1_no=f1_score(etiq_v, predicho, average=None)\n",
    "        matriz=normalize(confusion_matrix(etiq_v, predicho))\n",
    "        fs_macro_val[dict_models[i]].append(f1_ma)\n",
    "        fs_none_val[dict_models[i]].append(f1_no)\n",
    "        accs_val[dict_models[i]].append(acc)\n",
    "        confusions_val[dict_models[i]].append(matriz)\n",
    "        \n",
    "    predicciones_all_val=[]\n",
    "    for i in dict_trainedModel.keys(): \n",
    "        a_evaluar=dict_models[i]\n",
    "        predicciones_all_val.append(dict_trainedModel[i].predict(x_val, batch_size=bs))\n",
    "    predichos_all_val=np.concatenate(np.asarray(predicciones_all_val),axis=-1)\n",
    "    \n",
    "    print (\"Agregando predicciones Test set\", dict_models[i])    \n",
    "    for i in dict_trainedModel.keys(): \n",
    "        a_evaluar=dict_models[i]\n",
    "        predicho=dict_trainedModel[i].predict(x_test, batch_size=bs)\n",
    "        predicho=[np.argmax(pred) for pred in predicho]\n",
    "        acc= accuracy_score(etiq_t, predicho)  \n",
    "        f1_ma=f1_score(etiq_t, predicho, average='macro')\n",
    "        f1_no=f1_score(etiq_t, predicho, average=None)\n",
    "        matriz=normalize(confusion_matrix(etiq_t, predicho))\n",
    "        fs_macro_test[dict_models[i]].append(f1_ma)\n",
    "        fs_none_test[dict_models[i]].append(f1_no)\n",
    "        accs_test[dict_models[i]].append(acc)\n",
    "        confusions_test[dict_models[i]].append(matriz)\n",
    "\n",
    "    predicciones_all_test=[]\n",
    "    bs=32\n",
    "    for i in dict_trainedModel.keys(): \n",
    "        a_evaluar=dict_models[i]\n",
    "        predicciones_all_test.append(dict_trainedModel[i].predict(x_test, batch_size=bs))\n",
    "    predichos_all_test=np.concatenate(np.asarray(predicciones_all_test),axis=-1)\n",
    "    \n",
    "    print (\"--------VALIDATION SET--------\")\n",
    "    \n",
    "    print (\"\\nCommittee Best Fit\")\n",
    "    trainPredicho, maxPredicho, confianzas = predecir_modelos(bs,predichos_all_val)\n",
    "    acc_comite= accuracy_score(etiq_v, trainPredicho)  \n",
    "    print(\"Accuracy:\",acc_comite)\n",
    "    f1=f1_score(etiq_v, trainPredicho, average='macro') \n",
    "    print(\"F1-macro:\",f1)\n",
    "    f1_no=f1_score(etiq_v, trainPredicho, average=None)  \n",
    "    print(\"F1-score SDQC:\",f1)\n",
    "    best['acc'].append(acc_comite)\n",
    "    best['macro'].append(f1)\n",
    "    best['none'].append(f1_no)\n",
    "    \n",
    "    print (\"\\nCommittee Norm\")\n",
    "    trainPredicho, maxPredicho, confianzas = predecir_modelos_norm(bs,predichos_all_val)\n",
    "    acc_comite= accuracy_score(etiq_v, trainPredicho)  \n",
    "    print(\"Accuracy:\",acc_comite)\n",
    "    f1=f1_score(etiq_v, trainPredicho, average='macro') \n",
    "    print(\"F1-macro:\",f1)\n",
    "    f1_no=f1_score(etiq_v, trainPredicho, average=None) \n",
    "    print(\"F1-score SDQC:\",f1)\n",
    "    norm['acc'].append(acc_comite)\n",
    "    norm['macro'].append(f1)\n",
    "    norm['none'].append(f1_no)\n",
    "\n",
    "    print (\"\\nCommittee Voting\")\n",
    "    trainPredicho, maxPredicho, confianzas = predecir_modelos_average(bs,predichos_all_val)\n",
    "    acc_comite= accuracy_score(etiq_v, trainPredicho)  \n",
    "    print(\"Accuracy:\",acc_comite)\n",
    "    f1=f1_score(etiq_v, trainPredicho, average='macro') \n",
    "    print(\"F1-macro:\",f1)\n",
    "    f1_no=f1_score(etiq_v, trainPredicho, average=None)  \n",
    "    print(\"F1-score SDQC:\",f1)\n",
    "    voting['acc'].append(acc_comite)\n",
    "    voting['macro'].append(f1)\n",
    "    voting['none'].append(f1_no)\n",
    "    \n",
    "    print (\"--------TESTING SET--------\")\n",
    "    \n",
    "    print (\"\\nCommittee Best Fit\")\n",
    "    trainPredicho, maxPredicho, confianzas = predecir_modelos(bs,predichos_all_test)\n",
    "    acc_comite= accuracy_score(etiq_t, trainPredicho)  \n",
    "    print(\"Accuracy:\",acc_comite)\n",
    "    f1=f1_score(etiq_t, trainPredicho, average='macro') \n",
    "    print(\"F1-macro:\",f1)\n",
    "    f1_no=f1_score(etiq_t, trainPredicho, average=None)  \n",
    "    print(\"F1-score SDQC:\",f1)\n",
    "    best_test['acc'].append(acc_comite)\n",
    "    best_test['macro'].append(f1)\n",
    "    best_test['none'].append(f1_no)\n",
    "\n",
    "    print (\"\\nCommittee Norm\")\n",
    "    trainPredicho, maxPredicho, confianzas = predecir_modelos_norm(bs,predichos_all_test)\n",
    "    acc_comite= accuracy_score(etiq_t, trainPredicho)  \n",
    "    print(\"Accuracy:\",acc_comite)\n",
    "    f1=f1_score(etiq_t, trainPredicho, average='macro') \n",
    "    print(\"F1-macro:\",f1)\n",
    "    f1_no=f1_score(etiq_t, trainPredicho, average=None) \n",
    "    print(\"F1-score SDQC:\",f1)\n",
    "    norm_test['acc'].append(acc_comite)\n",
    "    norm_test['macro'].append(f1)\n",
    "    norm_test['none'].append(f1_no)\n",
    "\n",
    "    print (\"\\nCommittee Voting\")\n",
    "    trainPredicho, maxPredicho, confianzas = predecir_modelos_average(bs,predichos_all_test)\n",
    "    acc_comite= accuracy_score(etiq_t, trainPredicho)  \n",
    "    print(\"Accuracy:\",acc_comite)\n",
    "    f1=f1_score(etiq_t, trainPredicho, average='macro') \n",
    "    print(\"F1-macro:\",f1)\n",
    "    f1_no=f1_score(etiq_t, trainPredicho, average=None)  \n",
    "    print(\"F1-score SDQC:\",f1)\n",
    "    voting_test['acc'].append(acc_comite)\n",
    "    voting_test['macro'].append(f1)\n",
    "    voting_test['none'].append(f1_no)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0.03846154, 0.17460317, 0.61818182, 0.84422111]),\n",
       " array([0.09150327, 0.24793388, 0.53333333, 0.82745592]),\n",
       " array([0.13533835, 0.23188406, 0.53631285, 0.83919598]),\n",
       " array([0.05769231, 0.14285714, 0.53475936, 0.84441733]),\n",
       " array([0.0990099 , 0.15217391, 0.5257732 , 0.84954683])]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_test['none']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TEST] Acc Best comité: 0.6773751224289912\n",
      "[TEST] F1-score SDQC Best comité: [0.0985273  0.20060576 0.49484766 0.80304865]\n",
      "[TEST] F1 macro Best comité: 0.39925734123493967\n",
      "-----------\n",
      "[TEST] Acc Norm comité: 0.7326150832517139\n",
      "[TEST] F1-score SDQC Norm comité: [0.08440107 0.18989043 0.54967211 0.84096743]\n",
      "[TEST] F1 macro Norm comité: 0.4162327624426492\n",
      "-----------\n",
      "[TEST] Acc Voting comité: 0.7330068560235062\n",
      "[TEST] F1-score SDQC Voting comité: [0.09291343 0.20018159 0.56438427 0.84030882]\n",
      "[TEST] F1 macro Voting comité: 0.42444702502263376\n"
     ]
    }
   ],
   "source": [
    "m=5.0\n",
    "\n",
    "print (\"[TEST] Acc Best comité:\",np.sum(np.asarray(best_test['acc'])/m))\n",
    "temp=np.zeros(4)\n",
    "for result in best_test['none']:\n",
    "    temp+=result\n",
    "print (\"[TEST] F1-score SDQC Best comité:\" ,temp/m)\n",
    "print (\"[TEST] F1 macro Best comité:\",np.sum(np.asarray(best_test['macro'])/m))\n",
    "print (\"-----------\")\n",
    "print (\"[TEST] Acc Norm comité:\",np.sum(np.asarray(norm_test['acc'])/m))\n",
    "temp=np.zeros(4)\n",
    "for result in norm_test['none']:\n",
    "    temp+=result\n",
    "print (\"[TEST] F1-score SDQC Norm comité:\" ,temp/m)\n",
    "print (\"[TEST] F1 macro Norm comité:\",np.sum(np.asarray(norm_test['macro'])/m))\n",
    "print (\"-----------\")\n",
    "print (\"[TEST] Acc Voting comité:\",np.sum(np.asarray(voting_test['acc'])/m))\n",
    "temp=np.zeros(4)\n",
    "for result in voting_test['none']:\n",
    "    temp+=result\n",
    "print (\"[TEST] F1-score SDQC Voting comité:\" ,temp/m)\n",
    "print (\"[TEST] F1 macro Voting comité:\",np.sum(np.asarray(voting_test['macro'])/m))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------- EJECUCIÓN #1----------------\n",
      "Agregando predicciones Val set\n",
      "Agregando predicciones Test set rnn3\n",
      "--------VALIDATION SET--------\n",
      "\n",
      "Committee Best Fit\n",
      "Accuracy: 0.5551601423487544\n",
      "F1-macro: 0.49275237282543705\n",
      "F1-score SDQC: 0.49275237282543705\n",
      "\n",
      "Committee Norm\n",
      "Accuracy: 0.6227758007117438\n",
      "F1-macro: 0.5302195340501793\n",
      "F1-score SDQC: 0.5302195340501793\n",
      "\n",
      "Committee Voting\n",
      "Accuracy: 0.6512455516014235\n",
      "F1-macro: 0.5443253199483385\n",
      "F1-score SDQC: 0.5443253199483385\n",
      "--------TESTING SET--------\n",
      "\n",
      "Committee Best Fit\n",
      "Accuracy: 0.435847208619001\n",
      "F1-macro: 0.34583873946675536\n",
      "F1-score SDQC: 0.34583873946675536\n",
      "\n",
      "Committee Norm\n",
      "Accuracy: 0.5886385896180215\n",
      "F1-macro: 0.4071890253255982\n",
      "F1-score SDQC: 0.4071890253255982\n",
      "\n",
      "Committee Voting\n",
      "Accuracy: 0.6307541625857003\n",
      "F1-macro: 0.417344804389277\n",
      "F1-score SDQC: 0.417344804389277\n",
      "\n",
      "---------------- EJECUCIÓN #2----------------\n",
      "Agregando predicciones Val set\n",
      "Agregando predicciones Test set rnn3\n",
      "--------VALIDATION SET--------\n",
      "\n",
      "Committee Best Fit\n",
      "Accuracy: 0.6014234875444839\n",
      "F1-macro: 0.40326574396696346\n",
      "F1-score SDQC: 0.40326574396696346\n",
      "\n",
      "Committee Norm\n",
      "Accuracy: 0.6263345195729537\n",
      "F1-macro: 0.41666704047541303\n",
      "F1-score SDQC: 0.41666704047541303\n",
      "\n",
      "Committee Voting\n",
      "Accuracy: 0.6263345195729537\n",
      "F1-macro: 0.41966244998063773\n",
      "F1-score SDQC: 0.41966244998063773\n",
      "--------TESTING SET--------\n",
      "\n",
      "Committee Best Fit\n",
      "Accuracy: 0.6914789422135161\n",
      "F1-macro: 0.3925781718121268\n",
      "F1-score SDQC: 0.3925781718121268\n",
      "\n",
      "Committee Norm\n",
      "Accuracy: 0.7120470127326151\n",
      "F1-macro: 0.3881188977660526\n",
      "F1-score SDQC: 0.3881188977660526\n",
      "\n",
      "Committee Voting\n",
      "Accuracy: 0.7100881488736532\n",
      "F1-macro: 0.39758091780479843\n",
      "F1-score SDQC: 0.39758091780479843\n",
      "\n",
      "---------------- EJECUCIÓN #3----------------\n",
      "Agregando predicciones Val set\n",
      "Agregando predicciones Test set rnn3\n",
      "--------VALIDATION SET--------\n",
      "\n",
      "Committee Best Fit\n",
      "Accuracy: 0.5693950177935944\n",
      "F1-macro: 0.44271776638347243\n",
      "F1-score SDQC: 0.44271776638347243\n",
      "\n",
      "Committee Norm\n",
      "Accuracy: 0.6156583629893239\n",
      "F1-macro: 0.4692550037658446\n",
      "F1-score SDQC: 0.4692550037658446\n",
      "\n",
      "Committee Voting\n",
      "Accuracy: 0.6120996441281139\n",
      "F1-macro: 0.46836470215562154\n",
      "F1-score SDQC: 0.46836470215562154\n",
      "--------TESTING SET--------\n",
      "\n",
      "Committee Best Fit\n",
      "Accuracy: 0.6317335945151812\n",
      "F1-macro: 0.39198390042604414\n",
      "F1-score SDQC: 0.39198390042604414\n",
      "\n",
      "Committee Norm\n",
      "Accuracy: 0.6895200783545543\n",
      "F1-macro: 0.4176460894344356\n",
      "F1-score SDQC: 0.4176460894344356\n",
      "\n",
      "Committee Voting\n",
      "Accuracy: 0.6846229187071499\n",
      "F1-macro: 0.4203101900092733\n",
      "F1-score SDQC: 0.4203101900092733\n",
      "\n",
      "---------------- EJECUCIÓN #4----------------\n",
      "Agregando predicciones Val set\n",
      "Agregando predicciones Test set rnn3\n",
      "--------VALIDATION SET--------\n",
      "\n",
      "Committee Best Fit\n",
      "Accuracy: 0.6441281138790036\n",
      "F1-macro: 0.5088149313813031\n",
      "F1-score SDQC: 0.5088149313813031\n",
      "\n",
      "Committee Norm\n",
      "Accuracy: 0.6583629893238434\n",
      "F1-macro: 0.49651547836373255\n",
      "F1-score SDQC: 0.49651547836373255\n",
      "\n",
      "Committee Voting\n",
      "Accuracy: 0.6868327402135231\n",
      "F1-macro: 0.545536865104802\n",
      "F1-score SDQC: 0.545536865104802\n",
      "--------TESTING SET--------\n",
      "\n",
      "Committee Best Fit\n",
      "Accuracy: 0.6297747306562194\n",
      "F1-macro: 0.3739046743414164\n",
      "F1-score SDQC: 0.3739046743414164\n",
      "\n",
      "Committee Norm\n",
      "Accuracy: 0.6846229187071499\n",
      "F1-macro: 0.3842002925427406\n",
      "F1-score SDQC: 0.3842002925427406\n",
      "\n",
      "Committee Voting\n",
      "Accuracy: 0.7002938295788442\n",
      "F1-macro: 0.3885644159299968\n",
      "F1-score SDQC: 0.3885644159299968\n",
      "\n",
      "---------------- EJECUCIÓN #5----------------\n",
      "Agregando predicciones Val set\n",
      "Agregando predicciones Test set rnn3\n",
      "--------VALIDATION SET--------\n",
      "\n",
      "Committee Best Fit\n",
      "Accuracy: 0.5765124555160143\n",
      "F1-macro: 0.41523227173788524\n",
      "F1-score SDQC: 0.41523227173788524\n",
      "\n",
      "Committee Norm\n",
      "Accuracy: 0.6441281138790036\n",
      "F1-macro: 0.4927078587275353\n",
      "F1-score SDQC: 0.4927078587275353\n",
      "\n",
      "Committee Voting\n",
      "Accuracy: 0.6512455516014235\n",
      "F1-macro: 0.49192897497982246\n",
      "F1-score SDQC: 0.49192897497982246\n",
      "--------TESTING SET--------\n",
      "\n",
      "Committee Best Fit\n",
      "Accuracy: 0.6552399608227228\n",
      "F1-macro: 0.4134172357856568\n",
      "F1-score SDQC: 0.4134172357856568\n",
      "\n",
      "Committee Norm\n",
      "Accuracy: 0.7012732615083251\n",
      "F1-macro: 0.4220089746285509\n",
      "F1-score SDQC: 0.4220089746285509\n",
      "\n",
      "Committee Voting\n",
      "Accuracy: 0.7061704211557297\n",
      "F1-macro: 0.42844766750363417\n",
      "F1-score SDQC: 0.42844766750363417\n"
     ]
    }
   ],
   "source": [
    "prob=0.5\n",
    "path='../new_glove_augmented/stance_baselines/'\n",
    "bs=32\n",
    "    \n",
    "fs_macro_val={'cnn1':[], 'cnn2':[], 'rnn1':[],'rnn2':[],'rnn3':[]}\n",
    "fs_none_val={'cnn1':[], 'cnn2':[], 'rnn1':[],'rnn2':[],'rnn3':[]}\n",
    "accs_val={'cnn1':[], 'cnn2':[], 'rnn1':[],'rnn2':[],'rnn3':[]}\n",
    "confusions_val={'cnn1':[], 'cnn2':[], 'rnn1':[],'rnn2':[],'rnn3':[]}\n",
    "\n",
    "fs_macro_test={'cnn1':[], 'cnn2':[], 'rnn1':[],'rnn2':[],'rnn3':[]}\n",
    "fs_none_test={'cnn1':[], 'cnn2':[], 'rnn1':[],'rnn2':[],'rnn3':[]}\n",
    "accs_test={'cnn1':[], 'cnn2':[], 'rnn1':[],'rnn2':[],'rnn3':[]}\n",
    "confusions_test={'cnn1':[], 'cnn2':[], 'rnn1':[],'rnn2':[],'rnn3':[]}\n",
    "\n",
    "best={'acc':[], 'none':[], 'macro':[]}\n",
    "norm={'acc':[], 'none':[], 'macro':[]}\n",
    "voting={'acc':[], 'none':[], 'macro':[]}\n",
    "best_test={'acc':[], 'none':[], 'macro':[]}\n",
    "norm_test={'acc':[], 'none':[], 'macro':[]}\n",
    "voting_test={'acc':[], 'none':[], 'macro':[]}\n",
    "\n",
    "for j in range(1,6):\n",
    "    print (\"\")\n",
    "    print (\"---------------- EJECUCIÓN #\"+str(j)+'----------------')\n",
    "    cnn1= load_model(path+str(prob)+'/cnn1_'+str(j)+'-exec_app1_Top_1.h5', custom_objects={'FocalLoss': focal_loss, 'focal_loss_fixed': focal_loss()})  \n",
    "    cnn2= load_model(path+str(prob)+'/cnn2_'+str(j)+'-exec_app1_Top_1.h5', custom_objects={'FocalLoss': focal_loss, 'focal_loss_fixed': focal_loss()}) \n",
    "    rnn1= load_model(path+str(prob)+'/rnn1_'+str(j)+'-exec_app1_Top_1.h5', custom_objects={'FocalLoss': focal_loss, 'focal_loss_fixed': focal_loss()}) \n",
    "    rnn2= load_model(path+str(prob)+'/rnn2_'+str(j)+'-exec_app1_Top_1.h5', custom_objects={'FocalLoss': focal_loss, 'focal_loss_fixed': focal_loss()}) \n",
    "    rnn3= load_model(path+str(prob)+'/rnn3_'+str(j)+'-exec_app1_Top_1.h5') \n",
    "    \n",
    "    list_models=['cnn1', 'cnn2', 'rnn1', 'rnn2', 'rnn3']\n",
    "    index_models=np.arange(5)\n",
    "    dict_models=dict((key, value) for (key, value) in zip(index_models,list_models))\n",
    "    modelos=[cnn1, cnn2, rnn1, rnn2, rnn3]\n",
    "    ind=np.arange(5)\n",
    "    dict_trainedModel=dict((key, value) for (key, value) in zip(ind,modelos))\n",
    "    \n",
    "    print (\"Agregando predicciones Val set\")\n",
    "    for i in dict_trainedModel.keys(): \n",
    "        a_evaluar=dict_models[i]\n",
    "        predicho=dict_trainedModel[i].predict(x_val, batch_size=bs)\n",
    "        predicho=[np.argmax(pred) for pred in predicho]\n",
    "        acc= accuracy_score(etiq_v, predicho)  \n",
    "        f1_ma=f1_score(etiq_v, predicho, average='macro')\n",
    "        f1_no=f1_score(etiq_v, predicho, average=None)\n",
    "        matriz=normalize(confusion_matrix(etiq_v, predicho))\n",
    "        fs_macro_val[dict_models[i]].append(f1_ma)\n",
    "        fs_none_val[dict_models[i]].append(f1_no)\n",
    "        accs_val[dict_models[i]].append(acc)\n",
    "        confusions_val[dict_models[i]].append(matriz)\n",
    "        \n",
    "    predicciones_all_val=[]\n",
    "    for i in dict_trainedModel.keys(): \n",
    "        a_evaluar=dict_models[i]\n",
    "        predicciones_all_val.append(dict_trainedModel[i].predict(x_val, batch_size=bs))\n",
    "    predichos_all_val=np.concatenate(np.asarray(predicciones_all_val),axis=-1)\n",
    "    \n",
    "    print (\"Agregando predicciones Test set\", dict_models[i])    \n",
    "    for i in dict_trainedModel.keys(): \n",
    "        a_evaluar=dict_models[i]\n",
    "        predicho=dict_trainedModel[i].predict(x_test, batch_size=bs)\n",
    "        predicho=[np.argmax(pred) for pred in predicho]\n",
    "        acc= accuracy_score(etiq_t, predicho)  \n",
    "        f1_ma=f1_score(etiq_t, predicho, average='macro')\n",
    "        f1_no=f1_score(etiq_t, predicho, average=None)\n",
    "        matriz=normalize(confusion_matrix(etiq_t, predicho))\n",
    "        fs_macro_test[dict_models[i]].append(f1_ma)\n",
    "        fs_none_test[dict_models[i]].append(f1_no)\n",
    "        accs_test[dict_models[i]].append(acc)\n",
    "        confusions_test[dict_models[i]].append(matriz)\n",
    "\n",
    "    predicciones_all_test=[]\n",
    "    bs=32\n",
    "    for i in dict_trainedModel.keys(): \n",
    "        a_evaluar=dict_models[i]\n",
    "        predicciones_all_test.append(dict_trainedModel[i].predict(x_test, batch_size=bs))\n",
    "    predichos_all_test=np.concatenate(np.asarray(predicciones_all_test),axis=-1)\n",
    "    \n",
    "    print (\"--------VALIDATION SET--------\")\n",
    "    \n",
    "    print (\"\\nCommittee Best Fit\")\n",
    "    trainPredicho, maxPredicho, confianzas = predecir_modelos(bs,predichos_all_val)\n",
    "    acc_comite= accuracy_score(etiq_v, trainPredicho)  \n",
    "    print(\"Accuracy:\",acc_comite)\n",
    "    f1=f1_score(etiq_v, trainPredicho, average='macro') \n",
    "    print(\"F1-macro:\",f1)\n",
    "    f1_no=f1_score(etiq_v, trainPredicho, average=None)  \n",
    "    print(\"F1-score SDQC:\",f1)\n",
    "    best['acc'].append(acc_comite)\n",
    "    best['macro'].append(f1)\n",
    "    best['none'].append(f1_no)\n",
    "    \n",
    "    print (\"\\nCommittee Norm\")\n",
    "    trainPredicho, maxPredicho, confianzas = predecir_modelos_norm(bs,predichos_all_val)\n",
    "    acc_comite= accuracy_score(etiq_v, trainPredicho)  \n",
    "    print(\"Accuracy:\",acc_comite)\n",
    "    f1=f1_score(etiq_v, trainPredicho, average='macro') \n",
    "    print(\"F1-macro:\",f1)\n",
    "    f1_no=f1_score(etiq_v, trainPredicho, average=None) \n",
    "    print(\"F1-score SDQC:\",f1)\n",
    "    norm['acc'].append(acc_comite)\n",
    "    norm['macro'].append(f1)\n",
    "    norm['none'].append(f1_no)\n",
    "\n",
    "    print (\"\\nCommittee Voting\")\n",
    "    trainPredicho, maxPredicho, confianzas = predecir_modelos_average(bs,predichos_all_val)\n",
    "    acc_comite= accuracy_score(etiq_v, trainPredicho)  \n",
    "    print(\"Accuracy:\",acc_comite)\n",
    "    f1=f1_score(etiq_v, trainPredicho, average='macro') \n",
    "    print(\"F1-macro:\",f1)\n",
    "    f1_no=f1_score(etiq_v, trainPredicho, average=None)  \n",
    "    print(\"F1-score SDQC:\",f1)\n",
    "    voting['acc'].append(acc_comite)\n",
    "    voting['macro'].append(f1)\n",
    "    voting['none'].append(f1_no)\n",
    "    \n",
    "    print (\"--------TESTING SET--------\")\n",
    "    \n",
    "    print (\"\\nCommittee Best Fit\")\n",
    "    trainPredicho, maxPredicho, confianzas = predecir_modelos(bs,predichos_all_test)\n",
    "    acc_comite= accuracy_score(etiq_t, trainPredicho)  \n",
    "    print(\"Accuracy:\",acc_comite)\n",
    "    f1=f1_score(etiq_t, trainPredicho, average='macro') \n",
    "    print(\"F1-macro:\",f1)\n",
    "    f1_no=f1_score(etiq_t, trainPredicho, average=None)  \n",
    "    print(\"F1-score SDQC:\",f1)\n",
    "    best_test['acc'].append(acc_comite)\n",
    "    best_test['macro'].append(f1)\n",
    "    best_test['none'].append(f1_no)\n",
    "\n",
    "    print (\"\\nCommittee Norm\")\n",
    "    trainPredicho, maxPredicho, confianzas = predecir_modelos_norm(bs,predichos_all_test)\n",
    "    acc_comite= accuracy_score(etiq_t, trainPredicho)  \n",
    "    print(\"Accuracy:\",acc_comite)\n",
    "    f1=f1_score(etiq_t, trainPredicho, average='macro') \n",
    "    print(\"F1-macro:\",f1)\n",
    "    f1_no=f1_score(etiq_t, trainPredicho, average=None) \n",
    "    print(\"F1-score SDQC:\",f1)\n",
    "    norm_test['acc'].append(acc_comite)\n",
    "    norm_test['macro'].append(f1)\n",
    "    norm_test['none'].append(f1_no)\n",
    "\n",
    "    print (\"\\nCommittee Voting\")\n",
    "    trainPredicho, maxPredicho, confianzas = predecir_modelos_average(bs,predichos_all_test)\n",
    "    acc_comite= accuracy_score(etiq_t, trainPredicho)  \n",
    "    print(\"Accuracy:\",acc_comite)\n",
    "    f1=f1_score(etiq_t, trainPredicho, average='macro') \n",
    "    print(\"F1-macro:\",f1)\n",
    "    f1_no=f1_score(etiq_t, trainPredicho, average=None)  \n",
    "    print(\"F1-score SDQC:\",f1)\n",
    "    voting_test['acc'].append(acc_comite)\n",
    "    voting_test['macro'].append(f1)\n",
    "    voting_test['none'].append(f1_no)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0.10810811, 0.24460432, 0.5483871 , 0.72765658]),\n",
       " array([0.10071942, 0.14814815, 0.47674419, 0.82686383]),\n",
       " array([0.08433735, 0.23703704, 0.53763441, 0.81157556]),\n",
       " array([0.08484848, 0.17094017, 0.47191011, 0.8091024 ]),\n",
       " array([0.14173228, 0.22222222, 0.50549451, 0.81858689])]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_test['none']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TEST] Acc Best comité: 0.6088148873653281\n",
      "[TEST] F1-score SDQC Best comité: [0.10545953 0.19415911 0.48948341 0.74507613]\n",
      "[TEST] F1 macro Best comité: 0.38354454436639995\n",
      "-----------\n",
      "[TEST] Acc Norm comité: 0.6752203721841332\n",
      "[TEST] F1-score SDQC Norm comité: [0.10394913 0.20459038 0.50803406 0.79875705]\n",
      "[TEST] F1 macro Norm comité: 0.4038326559394756\n",
      "-----------\n",
      "[TEST] Acc Voting comité: 0.6863858961802154\n",
      "[TEST] F1-score SDQC Voting comité: [0.09682584 0.21001821 0.52714224 0.8078121 ]\n",
      "[TEST] F1 macro Voting comité: 0.410449599127396\n"
     ]
    }
   ],
   "source": [
    "m=5.0\n",
    "\n",
    "print (\"[TEST] Acc Best comité:\",np.sum(np.asarray(best_test['acc'])/m))\n",
    "temp=np.zeros(4)\n",
    "for result in best_test['none']:\n",
    "    temp+=result\n",
    "print (\"[TEST] F1-score SDQC Best comité:\" ,temp/m)\n",
    "print (\"[TEST] F1 macro Best comité:\",np.sum(np.asarray(best_test['macro'])/m))\n",
    "print (\"-----------\")\n",
    "print (\"[TEST] Acc Norm comité:\",np.sum(np.asarray(norm_test['acc'])/m))\n",
    "temp=np.zeros(4)\n",
    "for result in norm_test['none']:\n",
    "    temp+=result\n",
    "print (\"[TEST] F1-score SDQC Norm comité:\" ,temp/m)\n",
    "print (\"[TEST] F1 macro Norm comité:\",np.sum(np.asarray(norm_test['macro'])/m))\n",
    "print (\"-----------\")\n",
    "print (\"[TEST] Acc Voting comité:\",np.sum(np.asarray(voting_test['acc'])/m))\n",
    "temp=np.zeros(4)\n",
    "for result in voting_test['none']:\n",
    "    temp+=result\n",
    "print (\"[TEST] F1-score SDQC Voting comité:\" ,temp/m)\n",
    "print (\"[TEST] F1 macro Voting comité:\",np.sum(np.asarray(voting_test['macro'])/m))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.85"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------- EJECUCIÓN #1----------------\n",
      "Agregando predicciones Val set\n",
      "Agregando predicciones Test set rnn3\n",
      "--------VALIDATION SET--------\n",
      "\n",
      "Committee Best Fit\n",
      "Accuracy: 0.6227758007117438\n",
      "F1-macro: 0.5064119675711178\n",
      "F1-score SDQC: 0.5064119675711178\n",
      "\n",
      "Committee Norm\n",
      "Accuracy: 0.6797153024911032\n",
      "F1-macro: 0.5390506630155558\n",
      "F1-score SDQC: 0.5390506630155558\n",
      "\n",
      "Committee Voting\n",
      "Accuracy: 0.6725978647686833\n",
      "F1-macro: 0.5293481122370457\n",
      "F1-score SDQC: 0.5293481122370457\n",
      "--------TESTING SET--------\n",
      "\n",
      "Committee Best Fit\n",
      "Accuracy: 0.6748285994123409\n",
      "F1-macro: 0.39340850114673864\n",
      "F1-score SDQC: 0.39340850114673864\n",
      "\n",
      "Committee Norm\n",
      "Accuracy: 0.7081292850146915\n",
      "F1-macro: 0.3958321036867881\n",
      "F1-score SDQC: 0.3958321036867881\n",
      "\n",
      "Committee Voting\n",
      "Accuracy: 0.7110675808031341\n",
      "F1-macro: 0.41116646570331356\n",
      "F1-score SDQC: 0.41116646570331356\n",
      "\n",
      "---------------- EJECUCIÓN #2----------------\n",
      "Agregando predicciones Val set\n",
      "Agregando predicciones Test set rnn3\n",
      "--------VALIDATION SET--------\n",
      "\n",
      "Committee Best Fit\n",
      "Accuracy: 0.6405693950177936\n",
      "F1-macro: 0.4958690870714483\n",
      "F1-score SDQC: 0.4958690870714483\n",
      "\n",
      "Committee Norm\n",
      "Accuracy: 0.6761565836298933\n",
      "F1-macro: 0.5074119668694367\n",
      "F1-score SDQC: 0.5074119668694367\n",
      "\n",
      "Committee Voting\n",
      "Accuracy: 0.6797153024911032\n",
      "F1-macro: 0.5291862593256965\n",
      "F1-score SDQC: 0.5291862593256965\n",
      "--------TESTING SET--------\n",
      "\n",
      "Committee Best Fit\n",
      "Accuracy: 0.7061704211557297\n",
      "F1-macro: 0.39334244321966083\n",
      "F1-score SDQC: 0.39334244321966083\n",
      "\n",
      "Committee Norm\n",
      "Accuracy: 0.7306562193927522\n",
      "F1-macro: 0.40388248983079134\n",
      "F1-score SDQC: 0.40388248983079134\n",
      "\n",
      "Committee Voting\n",
      "Accuracy: 0.7267384916748286\n",
      "F1-macro: 0.4103637426109582\n",
      "F1-score SDQC: 0.4103637426109582\n",
      "\n",
      "---------------- EJECUCIÓN #3----------------\n",
      "Agregando predicciones Val set\n",
      "Agregando predicciones Test set rnn3\n",
      "--------VALIDATION SET--------\n",
      "\n",
      "Committee Best Fit\n",
      "Accuracy: 0.5338078291814946\n",
      "F1-macro: 0.4070026955675903\n",
      "F1-score SDQC: 0.4070026955675903\n",
      "\n",
      "Committee Norm\n",
      "Accuracy: 0.597864768683274\n",
      "F1-macro: 0.43862169020263997\n",
      "F1-score SDQC: 0.43862169020263997\n",
      "\n",
      "Committee Voting\n",
      "Accuracy: 0.594306049822064\n",
      "F1-macro: 0.45029351526912714\n",
      "F1-score SDQC: 0.45029351526912714\n",
      "--------TESTING SET--------\n",
      "\n",
      "Committee Best Fit\n",
      "Accuracy: 0.5994123408423114\n",
      "F1-macro: 0.37945697608088513\n",
      "F1-score SDQC: 0.37945697608088513\n",
      "\n",
      "Committee Norm\n",
      "Accuracy: 0.672869735553379\n",
      "F1-macro: 0.41056099844856175\n",
      "F1-score SDQC: 0.41056099844856175\n",
      "\n",
      "Committee Voting\n",
      "Accuracy: 0.67384916748286\n",
      "F1-macro: 0.4115929690890653\n",
      "F1-score SDQC: 0.4115929690890653\n",
      "\n",
      "---------------- EJECUCIÓN #4----------------\n",
      "Agregando predicciones Val set\n",
      "Agregando predicciones Test set rnn3\n",
      "--------VALIDATION SET--------\n",
      "\n",
      "Committee Best Fit\n",
      "Accuracy: 0.6014234875444839\n",
      "F1-macro: 0.4765559795603603\n",
      "F1-score SDQC: 0.4765559795603603\n",
      "\n",
      "Committee Norm\n",
      "Accuracy: 0.6690391459074733\n",
      "F1-macro: 0.5159662333874933\n",
      "F1-score SDQC: 0.5159662333874933\n",
      "\n",
      "Committee Voting\n",
      "Accuracy: 0.6868327402135231\n",
      "F1-macro: 0.5349414519906324\n",
      "F1-score SDQC: 0.5349414519906324\n",
      "--------TESTING SET--------\n",
      "\n",
      "Committee Best Fit\n",
      "Accuracy: 0.6474045053868757\n",
      "F1-macro: 0.39211744596017895\n",
      "F1-score SDQC: 0.39211744596017895\n",
      "\n",
      "Committee Norm\n",
      "Accuracy: 0.7198824681684622\n",
      "F1-macro: 0.41234774061833585\n",
      "F1-score SDQC: 0.41234774061833585\n",
      "\n",
      "Committee Voting\n",
      "Accuracy: 0.7286973555337904\n",
      "F1-macro: 0.40335897549658517\n",
      "F1-score SDQC: 0.40335897549658517\n",
      "\n",
      "---------------- EJECUCIÓN #5----------------\n",
      "Agregando predicciones Val set\n",
      "Agregando predicciones Test set rnn3\n",
      "--------VALIDATION SET--------\n",
      "\n",
      "Committee Best Fit\n",
      "Accuracy: 0.608540925266904\n",
      "F1-macro: 0.47957065217391304\n",
      "F1-score SDQC: 0.47957065217391304\n",
      "\n",
      "Committee Norm\n",
      "Accuracy: 0.6619217081850534\n",
      "F1-macro: 0.5154716671364868\n",
      "F1-score SDQC: 0.5154716671364868\n",
      "\n",
      "Committee Voting\n",
      "Accuracy: 0.6619217081850534\n",
      "F1-macro: 0.5146402372946995\n",
      "F1-score SDQC: 0.5146402372946995\n",
      "--------TESTING SET--------\n",
      "\n",
      "Committee Best Fit\n",
      "Accuracy: 0.6601371204701273\n",
      "F1-macro: 0.38708079684912594\n",
      "F1-score SDQC: 0.38708079684912594\n",
      "\n",
      "Committee Norm\n",
      "Accuracy: 0.7012732615083251\n",
      "F1-macro: 0.4060162182267323\n",
      "F1-score SDQC: 0.4060162182267323\n",
      "\n",
      "Committee Voting\n",
      "Accuracy: 0.6914789422135161\n",
      "F1-macro: 0.41591264681407714\n",
      "F1-score SDQC: 0.41591264681407714\n"
     ]
    }
   ],
   "source": [
    "prob=0.85\n",
    "path='../new_glove_augmented/stance_baselines/'\n",
    "bs=32\n",
    "    \n",
    "fs_macro_val={'cnn1':[], 'cnn2':[], 'rnn1':[],'rnn2':[],'rnn3':[]}\n",
    "fs_none_val={'cnn1':[], 'cnn2':[], 'rnn1':[],'rnn2':[],'rnn3':[]}\n",
    "accs_val={'cnn1':[], 'cnn2':[], 'rnn1':[],'rnn2':[],'rnn3':[]}\n",
    "confusions_val={'cnn1':[], 'cnn2':[], 'rnn1':[],'rnn2':[],'rnn3':[]}\n",
    "\n",
    "fs_macro_test={'cnn1':[], 'cnn2':[], 'rnn1':[],'rnn2':[],'rnn3':[]}\n",
    "fs_none_test={'cnn1':[], 'cnn2':[], 'rnn1':[],'rnn2':[],'rnn3':[]}\n",
    "accs_test={'cnn1':[], 'cnn2':[], 'rnn1':[],'rnn2':[],'rnn3':[]}\n",
    "confusions_test={'cnn1':[], 'cnn2':[], 'rnn1':[],'rnn2':[],'rnn3':[]}\n",
    "\n",
    "best={'acc':[], 'none':[], 'macro':[]}\n",
    "norm={'acc':[], 'none':[], 'macro':[]}\n",
    "voting={'acc':[], 'none':[], 'macro':[]}\n",
    "best_test={'acc':[], 'none':[], 'macro':[]}\n",
    "norm_test={'acc':[], 'none':[], 'macro':[]}\n",
    "voting_test={'acc':[], 'none':[], 'macro':[]}\n",
    "\n",
    "for j in range(1,6):\n",
    "    print (\"\")\n",
    "    print (\"---------------- EJECUCIÓN #\"+str(j)+'----------------')\n",
    "    cnn1= load_model(path+str(prob)+'/cnn1_'+str(j)+'-exec_app1_Top_1.h5', custom_objects={'FocalLoss': focal_loss, 'focal_loss_fixed': focal_loss()})  \n",
    "    cnn2= load_model(path+str(prob)+'/cnn2_'+str(j)+'-exec_app1_Top_1.h5', custom_objects={'FocalLoss': focal_loss, 'focal_loss_fixed': focal_loss()}) \n",
    "    rnn1= load_model(path+str(prob)+'/rnn1_'+str(j)+'-exec_app1_Top_1.h5', custom_objects={'FocalLoss': focal_loss, 'focal_loss_fixed': focal_loss()}) \n",
    "    rnn2= load_model(path+str(prob)+'/rnn2_'+str(j)+'-exec_app1_Top_1.h5', custom_objects={'FocalLoss': focal_loss, 'focal_loss_fixed': focal_loss()}) \n",
    "    rnn3= load_model(path+str(prob)+'/rnn3_'+str(j)+'-exec_app1_Top_1.h5') \n",
    "    \n",
    "    list_models=['cnn1', 'cnn2', 'rnn1', 'rnn2', 'rnn3']\n",
    "    index_models=np.arange(5)\n",
    "    dict_models=dict((key, value) for (key, value) in zip(index_models,list_models))\n",
    "    modelos=[cnn1, cnn2, rnn1, rnn2, rnn3]\n",
    "    ind=np.arange(5)\n",
    "    dict_trainedModel=dict((key, value) for (key, value) in zip(ind,modelos))\n",
    "    \n",
    "    print (\"Agregando predicciones Val set\")\n",
    "    for i in dict_trainedModel.keys(): \n",
    "        a_evaluar=dict_models[i]\n",
    "        predicho=dict_trainedModel[i].predict(x_val, batch_size=bs)\n",
    "        predicho=[np.argmax(pred) for pred in predicho]\n",
    "        acc= accuracy_score(etiq_v, predicho)  \n",
    "        f1_ma=f1_score(etiq_v, predicho, average='macro')\n",
    "        f1_no=f1_score(etiq_v, predicho, average=None)\n",
    "        matriz=normalize(confusion_matrix(etiq_v, predicho))\n",
    "        fs_macro_val[dict_models[i]].append(f1_ma)\n",
    "        fs_none_val[dict_models[i]].append(f1_no)\n",
    "        accs_val[dict_models[i]].append(acc)\n",
    "        confusions_val[dict_models[i]].append(matriz)\n",
    "        \n",
    "    predicciones_all_val=[]\n",
    "    for i in dict_trainedModel.keys(): \n",
    "        a_evaluar=dict_models[i]\n",
    "        predicciones_all_val.append(dict_trainedModel[i].predict(x_val, batch_size=bs))\n",
    "    predichos_all_val=np.concatenate(np.asarray(predicciones_all_val),axis=-1)\n",
    "    \n",
    "    print (\"Agregando predicciones Test set\", dict_models[i])    \n",
    "    for i in dict_trainedModel.keys(): \n",
    "        a_evaluar=dict_models[i]\n",
    "        predicho=dict_trainedModel[i].predict(x_test, batch_size=bs)\n",
    "        predicho=[np.argmax(pred) for pred in predicho]\n",
    "        acc= accuracy_score(etiq_t, predicho)  \n",
    "        f1_ma=f1_score(etiq_t, predicho, average='macro')\n",
    "        f1_no=f1_score(etiq_t, predicho, average=None)\n",
    "        matriz=normalize(confusion_matrix(etiq_t, predicho))\n",
    "        fs_macro_test[dict_models[i]].append(f1_ma)\n",
    "        fs_none_test[dict_models[i]].append(f1_no)\n",
    "        accs_test[dict_models[i]].append(acc)\n",
    "        confusions_test[dict_models[i]].append(matriz)\n",
    "\n",
    "    predicciones_all_test=[]\n",
    "    bs=32\n",
    "    for i in dict_trainedModel.keys(): \n",
    "        a_evaluar=dict_models[i]\n",
    "        predicciones_all_test.append(dict_trainedModel[i].predict(x_test, batch_size=bs))\n",
    "    predichos_all_test=np.concatenate(np.asarray(predicciones_all_test),axis=-1)\n",
    "    \n",
    "    print (\"--------VALIDATION SET--------\")\n",
    "    \n",
    "    print (\"\\nCommittee Best Fit\")\n",
    "    trainPredicho, maxPredicho, confianzas = predecir_modelos(bs,predichos_all_val)\n",
    "    acc_comite= accuracy_score(etiq_v, trainPredicho)  \n",
    "    print(\"Accuracy:\",acc_comite)\n",
    "    f1=f1_score(etiq_v, trainPredicho, average='macro') \n",
    "    print(\"F1-macro:\",f1)\n",
    "    f1_no=f1_score(etiq_v, trainPredicho, average=None)  \n",
    "    print(\"F1-score SDQC:\",f1)\n",
    "    best['acc'].append(acc_comite)\n",
    "    best['macro'].append(f1)\n",
    "    best['none'].append(f1_no)\n",
    "    \n",
    "    print (\"\\nCommittee Norm\")\n",
    "    trainPredicho, maxPredicho, confianzas = predecir_modelos_norm(bs,predichos_all_val)\n",
    "    acc_comite= accuracy_score(etiq_v, trainPredicho)  \n",
    "    print(\"Accuracy:\",acc_comite)\n",
    "    f1=f1_score(etiq_v, trainPredicho, average='macro') \n",
    "    print(\"F1-macro:\",f1)\n",
    "    f1_no=f1_score(etiq_v, trainPredicho, average=None) \n",
    "    print(\"F1-score SDQC:\",f1)\n",
    "    norm['acc'].append(acc_comite)\n",
    "    norm['macro'].append(f1)\n",
    "    norm['none'].append(f1_no)\n",
    "\n",
    "    print (\"\\nCommittee Voting\")\n",
    "    trainPredicho, maxPredicho, confianzas = predecir_modelos_average(bs,predichos_all_val)\n",
    "    acc_comite= accuracy_score(etiq_v, trainPredicho)  \n",
    "    print(\"Accuracy:\",acc_comite)\n",
    "    f1=f1_score(etiq_v, trainPredicho, average='macro') \n",
    "    print(\"F1-macro:\",f1)\n",
    "    f1_no=f1_score(etiq_v, trainPredicho, average=None)  \n",
    "    print(\"F1-score SDQC:\",f1)\n",
    "    voting['acc'].append(acc_comite)\n",
    "    voting['macro'].append(f1)\n",
    "    voting['none'].append(f1_no)\n",
    "    \n",
    "    print (\"--------TESTING SET--------\")\n",
    "    \n",
    "    print (\"\\nCommittee Best Fit\")\n",
    "    trainPredicho, maxPredicho, confianzas = predecir_modelos(bs,predichos_all_test)\n",
    "    acc_comite= accuracy_score(etiq_t, trainPredicho)  \n",
    "    print(\"Accuracy:\",acc_comite)\n",
    "    f1=f1_score(etiq_t, trainPredicho, average='macro') \n",
    "    print(\"F1-macro:\",f1)\n",
    "    f1_no=f1_score(etiq_t, trainPredicho, average=None)  \n",
    "    print(\"F1-score SDQC:\",f1)\n",
    "    best_test['acc'].append(acc_comite)\n",
    "    best_test['macro'].append(f1)\n",
    "    best_test['none'].append(f1_no)\n",
    "\n",
    "    print (\"\\nCommittee Norm\")\n",
    "    trainPredicho, maxPredicho, confianzas = predecir_modelos_norm(bs,predichos_all_test)\n",
    "    acc_comite= accuracy_score(etiq_t, trainPredicho)  \n",
    "    print(\"Accuracy:\",acc_comite)\n",
    "    f1=f1_score(etiq_t, trainPredicho, average='macro') \n",
    "    print(\"F1-macro:\",f1)\n",
    "    f1_no=f1_score(etiq_t, trainPredicho, average=None) \n",
    "    print(\"F1-score SDQC:\",f1)\n",
    "    norm_test['acc'].append(acc_comite)\n",
    "    norm_test['macro'].append(f1)\n",
    "    norm_test['none'].append(f1_no)\n",
    "\n",
    "    print (\"\\nCommittee Voting\")\n",
    "    trainPredicho, maxPredicho, confianzas = predecir_modelos_average(bs,predichos_all_test)\n",
    "    acc_comite= accuracy_score(etiq_t, trainPredicho)  \n",
    "    print(\"Accuracy:\",acc_comite)\n",
    "    f1=f1_score(etiq_t, trainPredicho, average='macro') \n",
    "    print(\"F1-macro:\",f1)\n",
    "    f1_no=f1_score(etiq_t, trainPredicho, average=None)  \n",
    "    print(\"F1-score SDQC:\",f1)\n",
    "    voting_test['acc'].append(acc_comite)\n",
    "    voting_test['macro'].append(f1)\n",
    "    voting_test['none'].append(f1_no)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0.10884354, 0.16071429, 0.48648649, 0.82728411]),\n",
       " array([0.08474576, 0.16216216, 0.53038674, 0.83823529]),\n",
       " array([0.06802721, 0.23602484, 0.54368932, 0.79450262]),\n",
       " array([0.08474576, 0.17054264, 0.56410256, 0.83      ]),\n",
       " array([0.09722222, 0.20472441, 0.5026738 , 0.81944444])]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_test['none']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TEST] Acc Best comité: 0.6575905974534769\n",
      "[TEST] F1-score SDQC Best comité: [0.0937964  0.18084757 0.49464667 0.78703429]\n",
      "[TEST] F1 macro Best comité: 0.3890812326513179\n",
      "-----------\n",
      "[TEST] Acc Norm comité: 0.7065621939275221\n",
      "[TEST] F1-score SDQC Norm comité: [0.0887169  0.18683367 0.52546778 0.82189329]\n",
      "[TEST] F1 macro Norm comité: 0.40572791016224186\n",
      "-----------\n",
      "[TEST] Acc Voting comité: 0.706366307541626\n",
      "[TEST] F1-score SDQC Voting comité: [0.09054231 0.19308136 0.53721733 0.82107484]\n",
      "[TEST] F1 macro Voting comité: 0.4104789599427998\n"
     ]
    }
   ],
   "source": [
    "m=5.0\n",
    "\n",
    "print (\"[TEST] Acc Best comité:\",np.sum(np.asarray(best_test['acc'])/m))\n",
    "temp=np.zeros(4)\n",
    "for result in best_test['none']:\n",
    "    temp+=result\n",
    "print (\"[TEST] F1-score SDQC Best comité:\" ,temp/m)\n",
    "print (\"[TEST] F1 macro Best comité:\",np.sum(np.asarray(best_test['macro'])/m))\n",
    "print (\"-----------\")\n",
    "print (\"[TEST] Acc Norm comité:\",np.sum(np.asarray(norm_test['acc'])/m))\n",
    "temp=np.zeros(4)\n",
    "for result in norm_test['none']:\n",
    "    temp+=result\n",
    "print (\"[TEST] F1-score SDQC Norm comité:\" ,temp/m)\n",
    "print (\"[TEST] F1 macro Norm comité:\",np.sum(np.asarray(norm_test['macro'])/m))\n",
    "print (\"-----------\")\n",
    "print (\"[TEST] Acc Voting comité:\",np.sum(np.asarray(voting_test['acc'])/m))\n",
    "temp=np.zeros(4)\n",
    "for result in voting_test['none']:\n",
    "    temp+=result\n",
    "print (\"[TEST] F1-score SDQC Voting comité:\" ,temp/m)\n",
    "print (\"[TEST] F1 macro Voting comité:\",np.sum(np.asarray(voting_test['macro'])/m))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:newpy3_tf1]",
   "language": "python",
   "name": "conda-env-newpy3_tf1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
